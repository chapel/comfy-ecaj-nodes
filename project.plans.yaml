kynetic_plans: "1.0"
plans:
  - _ulid: 01KH4H1VQ6QVKYZ61M38YV672C
    slugs:
      - plan-foundation-project-infrastructure
    title: Foundation — Project Infrastructure
    content: |
      # Foundation — Project Infrastructure

      ## Specs

      ```yaml
      - title: ComfyUI Packaging
        slug: comfyui-packaging
        type: feature
        description: |
          Node registration, Python packaging, and ComfyUI integration infrastructure.
          Covers __init__.py NODE_CLASS_MAPPINGS, pyproject.toml with [tool.comfy],
          requirements.txt, and CATEGORY namespace (ecaj/*).
        acceptance_criteria:
          - id: ac-1
            given: comfy-ecaj-nodes is installed in ComfyUI custom_nodes
            when: ComfyUI starts
            then: all registered nodes appear under the ecaj category in the node menu
          - id: ac-2
            given: a new node class is added to a nodes/ module
            when: it is added to NODE_CLASS_MAPPINGS in __init__.py
            then: ComfyUI discovers it without changes to other files
          - id: ac-3
            given: pyproject.toml defines [tool.comfy] metadata
            when: the package is published to the ComfyUI registry
            then: it displays as ECAJ Nodes with publisher ecaj
          - id: ac-4
            given: the CATEGORY namespace uses ecaj/merge for WIDEN nodes
            when: future non-merge nodes are added
            then: they use ecaj/<domain> following the established lowercase pattern
        implementation_notes: |
          Skeleton exists in __init__.py (5 WIDEN mappings) and pyproject.toml.
          Validate NODE_CLASS_MAPPINGS pattern works with ComfyUI node discovery.
          CATEGORY must use lowercase ecaj/ prefix with sub-categories per capability
          area (e.g., ecaj/merge for WIDEN nodes). Existing stubs use ECAJ/merge --
          update all to lowercase ecaj/merge. For the task: review ComfyUI's
          nodes_core.py loading path, verify pyproject.toml [tool.comfy] fields match
          registry requirements (PublisherId, DisplayName, Icon), ensure requirements.txt
          lists only dependencies ComfyUI doesn't already provide (torch and safetensors
          are provided by ComfyUI -- check if we need them at all), update CATEGORY on
          all 5 existing node stubs from ECAJ/merge to ecaj/merge.
          Files: __init__.py, pyproject.toml, requirements.txt, nodes/entry.py,
          nodes/lora.py, nodes/compose.py, nodes/merge.py, nodes/exit.py.

      - title: Testing Infrastructure
        slug: testing-infrastructure
        type: feature
        description: |
          pytest configuration, ComfyUI mocking strategy, and test fixtures for
          recipe tree construction. Must support testing nodes without a running
          ComfyUI instance.
        acceptance_criteria:
          - id: ac-1
            given: a developer runs pytest from the project root
            when: tests execute
            then: all tests pass without requiring a running ComfyUI instance
          - id: ac-2
            given: a test needs a ModelPatcher-like object
            when: it uses the mock fixture
            then: |
              the mock provides model_state_dict(), clone(), add_patches(),
              get_key_patches(), patches_uuid, and model.diffusion_model state
              dict access
          - id: ac-3
            given: a test needs a recipe tree
            when: it uses recipe fixtures
            then: |
              pre-built recipe trees for single-LoRA, multi-LoRA set, compose,
              and chain patterns are available
          - id: ac-4
            given: tests for nodes that use ComfyUI APIs like folder_paths
            when: they run without ComfyUI installed
            then: the APIs are mocked via conftest.py fixtures
        implementation_notes: |
          Create tests/conftest.py with: (1) MockModelPatcher class -- needs
          model_state_dict(filter_prefix) returning a dict of fake tensors keyed like
          diffusion_model.input_blocks.0.0.weight, clone() returning a new
          MockModelPatcher, add_patches(patches, strength_patch, strength_model)
          storing patches, get_key_patches(filter_prefix) returning patch data,
          patches_uuid property. Use small tensors (e.g., 4x4 float32) for speed.
          (2) Recipe tree fixtures -- recipe_base() returning RecipeBase with
          MockModelPatcher and arch=sdxl, recipe_single_lora(), recipe_lora_set()
          (2 LoRAs chained), recipe_compose() (2 branches), recipe_chain() (2
          sequential merges), recipe_full() (compose + chain like hyphoria example
          from design doc section 6.5). (3) Mock folder_paths module --
          get_filename_list(loras) returning [test.safetensors]. (4) pytest config
          in pyproject.toml [tool.pytest.ini_options] with testpaths=[tests].
          Files: tests/conftest.py, pyproject.toml (pytest config section).
      ```

      ## Tasks

      derive_from_specs: true

      ## Implementation Notes

      Foundation specs are project-level infrastructure. They should be completed
      before WIDEN feature work begins, as they establish patterns all nodes follow.
    status: active
    derived_tasks:
      - "@implement-comfyui-packaging"
      - "@implement-testing-infrastructure"
    derived_specs:
      - "@comfyui-packaging"
      - "@testing-infrastructure"
    source_path: /tmp/foundation-plan.md
    created_at: 2026-02-10T19:39:32.454Z
    approved_at: null
    completed_at: null
    notes:
      - _ulid: 01KH4H1VQ7PHVBZ7BHBJMKYDJE
        created_at: 2026-02-10T19:39:32.455Z
        author: "@claude"
        content: |-
          Implementation notes:

          Foundation specs are project-level infrastructure. They should be completed
          before WIDEN feature work begins, as they establish patterns all nodes follow.
  - _ulid: 01KH4HA42F7WBQQC8KXVFKN5WT
    slugs:
      - plan-widen-merge-feature-specs
    title: WIDEN Merge — Feature Specs
    content: >
      # WIDEN Merge — Feature Specs


      ## Specs


      ```yaml

      - title: Recipe Type System
        slug: recipe-system
        type: feature
        description: |
          The WIDEN custom ComfyUI type and its recipe dataclasses. All recipe
          objects are frozen (immutable) to prevent aliasing bugs with ComfyUI
          caching and graph fan-out. Fields use tuples, not lists. Recipe objects
          hold no GPU tensors -- they are pure recipe descriptions.
        acceptance_criteria:
          - id: ac-1
            given: any recipe dataclass instance
            when: a field is assigned after construction
            then: a FrozenInstanceError is raised
          - id: ac-2
            given: a RecipeCompose with existing branches
            when: a new branch is appended
            then: a new RecipeCompose is returned with a new tuple (persistent tree semantics)
          - id: ac-3
            given: any recipe object
            when: inspected for GPU tensors
            then: no torch.Tensor objects are found (only references and metadata)
          - id: ac-4
            given: RecipeBase, RecipeLoRA, RecipeCompose, and RecipeMerge classes
            when: imported from lib.recipe
            then: all four are available and constructible with documented fields
          - id: ac-5
            given: nodes that output WIDEN type and nodes that accept WIDEN input
            when: ComfyUI loads the node pack
            then: WIDEN wire connections are valid between nodes in the graph editor
        implementation_notes: |
          Partially implemented in lib/recipe.py -- has all 4 dataclasses but
          missing BlockConfig (added later in per-block-control). The WIDEN custom
          type is registered implicitly by ComfyUI when a node declares
          RETURN_TYPES = ("WIDEN",) -- no explicit registration needed, but verify
          this works by checking that ComfyUI type system allows connections between
          nodes sharing the custom type name. AC-5 can be tested by constructing a
          mock workflow JSON with WIDEN connections and validating against ComfyUI
          graph validation, or by testing in a running ComfyUI instance. For the
          task: verify existing dataclasses match design doc section 6.6, ensure all
          fields use tuples (not lists), verify frozen=True on all, add __all__
          export list. Consider adding RecipeNode = Union[RecipeBase, RecipeLoRA,
          RecipeCompose, RecipeMerge] type alias for type checking.
          Files: lib/recipe.py.

      - title: Entry Node
        slug: entry-node
        type: feature
        description: |
          Boundary from ComfyUI MODEL world to WIDEN recipe world. Wraps a
          ModelPatcher reference in a RecipeBase. Auto-detects architecture
          (SDXL, Z-Image, Flux, Qwen) by inspecting state dict key patterns.
          Zero GPU work -- just stores reference and arch tag.
        acceptance_criteria:
          - id: ac-1
            given: a ComfyUI MODEL (ModelPatcher) input
            when: Entry node executes
            then: it returns a RecipeBase wrapping the ModelPatcher reference
          - id: ac-2
            given: an SDXL model with diffusion_model.input_blocks keys
            when: architecture detection runs
            then: arch field is set to sdxl
          - id: ac-3
            given: a Z-Image model with diffusion_model.layers keys and noise_refiner
            when: architecture detection runs
            then: arch field is set to zimage
          - id: ac-4
            given: Entry node executes
            when: output is inspected
            then: no GPU memory is allocated and no tensors are copied
          - id: ac-5
            given: a model whose state dict keys match none of the known architecture patterns
            when: architecture detection runs
            then: it raises a clear error listing supported architectures
        implementation_notes: |
          Stub in nodes/entry.py. Architecture detection function should live in
          lib/arch_detect.py (or inline if small) -- inspect model.model_state_dict()
          keys for patterns: SDXL has input_blocks, middle_block, output_blocks;
          Z-Image has layers.N. plus noise_refiner; Flux has double_blocks; Qwen
          has transformer_blocks at depth 60. Check patterns in order of specificity
          (Z-Image before generic layers). For unsupported arch error, include first
          5 state dict key prefixes in the error message for debugging. ModelPatcher
          is stored as-is in RecipeBase (reference only, no clone, no tensor ops).
          Test by constructing MockModelPatcher instances with different key patterns
          and asserting detected arch.
          Files: nodes/entry.py, optionally lib/arch_detect.py.

      - title: LoRA Node
        slug: lora-node
        type: feature
        description: |
          Declares a LoRA to be applied as part of the recipe. Uses our own
          loader (not ComfyUI built-in) for deferred loading enabling batched
          bmm apply at Exit time. Chains via optional prev input to form sets
          (multiple LoRAs applied to the same base).
        acceptance_criteria:
          - id: ac-1
            given: a LoRA file path and strength value
            when: LoRA node executes
            then: it returns a RecipeLoRA with the file path and strength in its loras tuple
          - id: ac-2
            given: two LoRA nodes chained via prev connection
            when: the second node executes
            then: the output RecipeLoRA contains both LoRAs in its loras tuple forming a set
          - id: ac-3
            given: the LoRA node in ComfyUI UI
            when: the node renders
            then: lora_name shows a dropdown via folder_paths.get_filename_list loras
          - id: ac-4
            given: a LoRA node with no prev connection
            when: it executes
            then: the output RecipeLoRA has a single-element loras tuple
          - id: ac-5
            given: a LoRA node with strength 0.0
            when: it executes
            then: |
              the LoRA still appears in the recipe (strength=0 is the executor
              concern, not the recipe builder concern)
        implementation_notes: |
          Stub in nodes/lora.py. The lora_name input must change from STRING type
          to use folder_paths.get_filename_list(loras) -- the ComfyUI pattern is:
          import folder_paths then lora_name: (folder_paths.get_filename_list(loras),)
          as a combo input. Full file path resolved at Exit time via
          folder_paths.get_full_path(loras, lora_name). The prev input receives a
          RecipeLoRA from a previous LoRA node -- extract its .loras tuple and
          concatenate: new_loras = prev.loras + (dict(path=lora_name, strength=strength),).
          If prev is None, create single-element tuple. For the task: update
          INPUT_TYPES to use folder_paths combo, implement add_lora() method,
          handle prev chaining with tuple concatenation. Edge case: prev might
          be None (optional input).
          Files: nodes/lora.py.

      - title: Compose Node
        slug: compose-node
        type: feature
        description: |
          Accumulates branches for simultaneous WIDEN merging. Pure recipe building
          with zero computation. Chain multiple Compose nodes to accumulate any
          number of branches for the Merge node.
        acceptance_criteria:
          - id: ac-1
            given: a branch WIDEN input and no compose input
            when: Compose node executes
            then: it returns RecipeCompose with a single-element branches tuple
          - id: ac-2
            given: a branch input and a compose chain from a previous Compose
            when: Compose node executes
            then: it returns RecipeCompose with the new branch appended to existing branches
          - id: ac-3
            given: three Compose nodes chained together
            when: the final output is inspected
            then: all three branches are present in order
          - id: ac-4
            given: a RecipeBase wired directly to the branch input
            when: Compose node executes
            then: |
              it raises an error because branch must be a LoRA spec, compose group,
              or merge result -- a raw base without LoRAs is not a valid compose branch
        implementation_notes: |
          Stub in nodes/compose.py. Implementation: if compose is provided and is a
          RecipeCompose, extract its .branches tuple and append the new branch:
          RecipeCompose(branches=compose.branches + (branch,)). If compose is None,
          create RecipeCompose(branches=(branch,)). For AC-4 validation: check
          isinstance(branch, RecipeBase) and raise ValueError with message about
          needing to apply LoRAs first or use as a Merge base input. Valid branch
          types: RecipeLoRA, RecipeCompose, RecipeMerge. Also validate that compose
          input when provided is a RecipeCompose (not some other recipe type).
          Files: nodes/compose.py.

      - title: Merge Node
        slug: merge-node
        type: feature
        description: |
          The recipe builder for WIDEN merge operations. Produces RecipeMerge
          which the Exit node evaluates. Compose target triggers merge_weights,
          single target triggers filter_delta. Optional backbone override for
          explicit WIDEN importance reference.
        acceptance_criteria:
          - id: ac-1
            given: base and target WIDEN inputs with a t_factor value
            when: Merge node executes
            then: it returns a RecipeMerge with base, target, and t_factor stored
          - id: ac-2
            given: no backbone input connected
            when: Merge node executes
            then: backbone field is None in the RecipeMerge
          - id: ac-3
            given: an explicit backbone input connected
            when: Merge node executes
            then: the backbone reference is stored in RecipeMerge
          - id: ac-4
            given: a Merge output
            when: wired to another Merge base input
            then: it forms a valid chain for sequential merging
          - id: ac-5
            given: a RecipeLoRA or RecipeCompose wired to the base input
            when: Merge node executes
            then: |
              it raises an error because base must be RecipeBase or RecipeMerge
          - id: ac-6
            given: t_factor is -1.0
            when: stored in RecipeMerge
            then: the value is preserved (Exit interprets -1.0 as passthrough with no WIDEN)
        implementation_notes: |
          Stub in nodes/merge.py. Implementation: validate base is RecipeBase or
          RecipeMerge (raise ValueError with specific message if not). Validate
          target is RecipeLoRA, RecipeCompose, or RecipeMerge (not RecipeBase).
          Then construct RecipeMerge(base=base, target=target, backbone=backbone,
          t_factor=t_factor). The t_factor slider range is -1.0 to 5.0, step 0.05
          (already correct in stub). -1.0 means passthrough -- interpreted by Exit
          node, not Merge node. Backbone defaults to None when not connected
          (optional input). For the task: add isinstance validation checks at top
          of merge(), construct and return RecipeMerge.
          Files: nodes/merge.py.

      - title: Exit Node
        slug: exit-node
        type: feature
        description: |
          The only node that performs GPU computation. Receives the complete recipe
          tree, validates it, executes the full batched GPU pipeline, and returns
          a ComfyUI MODEL with merged weights as set patches. Handles IS_CHANGED
          for LoRA file monitoring and reports progress.
        acceptance_criteria:
          - id: ac-1
            given: a valid recipe tree ending in RecipeMerge
            when: Exit node executes
            then: it returns a ComfyUI MODEL (ModelPatcher clone) with set patches
          - id: ac-2
            given: an invalid recipe tree with type mismatches
            when: Exit node validates
            then: it raises ValueError naming the invalid type and its position in the tree
          - id: ac-3
            given: a recipe tree with compose target containing multiple branches
            when: Exit evaluates the merge step
            then: it calls merge_weights for simultaneous parameter routing
          - id: ac-4
            given: a recipe tree with single LoRA target
            when: Exit evaluates the merge step
            then: it calls filter_delta for importance filtering
          - id: ac-5
            given: a chain where RecipeMerge base is another RecipeMerge
            when: Exit evaluates
            then: inner merge evaluates first and its result becomes the base for outer merge
          - id: ac-6
            given: a RecipeCompose with a single branch
            when: Exit evaluates the merge step
            then: it treats it as filter_delta not merge_weights (single-branch passthrough)
          - id: ac-7
            given: the Exit node output MODEL
            when: a downstream ComfyUI LoRA node applies additional patches
            then: the additional LoRA patches apply additively on top of the set patches
          - id: ac-8
            given: the base ModelPatcher uses bf16 weights
            when: set patches are installed
            then: patch tensors match the base model storage dtype
        implementation_notes: |
          Stub in nodes/exit.py. This is the most complex node -- it orchestrates
          everything. The execute() method: (1) Validate recipe tree structure by
          walking recursively and checking types at each node. (2) Call batched
          executor from lib/executor.py which handles phases 1-3. (3) Install
          results as set patches on a ModelPatcher clone. The IS_CHANGED classmethod
          must walk recipe tree to find all RecipeLoRA nodes, resolve file paths via
          folder_paths.get_full_path(loras, name), and return hash of (mtime, size)
          tuples. Use os.path.getmtime() and os.path.getsize(). If any file missing,
          return float(NaN) to force re-execution. Progress reporting via
          comfy.utils.ProgressBar(total_steps) -- get total from executor. Downstream
          LoRA compat: set patches work because ComfyUI calculate_weight() processes
          patches in list order -- set replaces first, then subsequent LoRA patches
          add on top. Depends on lib/executor.py, lib/recipe.py, all node implementations.
          Files: nodes/exit.py.

      - title: Exit Recipe Analysis
        slug: exit-recipe-analysis
        type: requirement
        parent: "@exit-node"
        acceptance_criteria:
          - id: ac-1
            given: a recipe tree
            when: Exit walks to the root
            then: it finds the RecipeBase and extracts model_patcher and arch tag
          - id: ac-2
            given: multiple RecipeLoRA nodes in the tree
            when: synthetic set IDs are assigned
            then: |
              each unique RecipeLoRA group gets a distinct set ID and two LoRAs
              chained via prev share the same set ID
          - id: ac-3
            given: a recipe with LoRA references
            when: LoRA files are loaded
            then: the architecture-appropriate loader is selected based on the arch tag
          - id: ac-4
            given: loaded LoRA files
            when: the affected-key map is built
            then: each set ID maps to the set of base model parameter keys that set modifies
          - id: ac-5
            given: keys not affected by any LoRA set
            when: the executor processes keys
            then: those keys are skipped entirely with no work performed
          - id: ac-6
            given: a recipe referencing a LoRA file that does not exist
            when: Exit loads LoRAs
            then: it raises FileNotFoundError naming the missing file and which LoRA node referenced it
        implementation_notes: |
          This phase happens at start of execute() in nodes/exit.py or in
          lib/executor.py entry point. Tree walk: recursive function following
          RecipeMerge.base links until hitting RecipeBase. Collect all RecipeLoRA
          nodes by walking .target and .base recursively. Set ID assignment:
          identity-based -- with frozen dataclasses, chained LoRAs produce a single
          RecipeLoRA with a multi-element tuple, so each unique RecipeLoRA instance
          equals one set. LoRA loading: select loader from lib/lora/{arch}.py based
          on RecipeBase.arch. Resolve file paths with folder_paths.get_full_path(loras,
          name). Build affected-key map by calling loader.affected_keys for each set.
          The tree walk and set ID assignment is the most critical piece -- get the
          identity semantics right.
          Files: lib/executor.py, references lib/lora/base.py interface.

      - title: Exit Batched Evaluation
        slug: exit-batched-eval
        type: requirement
        parent: "@exit-node"
        acceptance_criteria:
          - id: ac-1
            given: a recipe tree with a compose target
            when: batched evaluation runs
            then: merge_weights_batched is called with all branch results and the backbone
          - id: ac-2
            given: a recipe tree with a single LoRA target
            when: batched evaluation runs
            then: filter_delta_batched is called with the applied LoRA delta and backbone
          - id: ac-3
            given: a chain of RecipeMerge nodes
            when: evaluation recurses
            then: inner merges evaluate first and results become the base for outer merges
          - id: ac-4
            given: the WIDEN algorithm produces results
            when: they are returned from evaluation
            then: all result tensors are on GPU (transferred to CPU in patch installation phase)
        implementation_notes: |
          This is the inner evaluation loop in lib/executor.py. For each OpSignature
          group, for each chunk of B keys: stack base tensors to GPU, walk recipe
          tree recursively. Tree walker dispatches on recipe node type: RecipeMerge
          with RecipeCompose target -> evaluate each branch then call
          widen.merge_weights_batched() (or filter_delta_batched if single branch).
          RecipeMerge with RecipeLoRA target -> call _apply_lora_set_batched_gpu()
          to get delta then widen.filter_delta_batched(). Chain: if RecipeMerge.base
          is another RecipeMerge, recurse on inner merge first. Backbone: use
          RecipeMerge.backbone if not None, else use base tensor. Port recursive
          evaluation from merge-router evaluate_node_batched() (~lines 850-950 of
          scripts/lora_chain_merge.py), adapting from config dict traversal to
          recipe dataclass traversal.
          Files: lib/executor.py.

      - title: Exit Patch Installation
        slug: exit-patch-install
        type: requirement
        parent: "@exit-node"
        acceptance_criteria:
          - id: ac-1
            given: merged tensors from batched evaluation
            when: patch installation runs
            then: original ModelPatcher is cloned via clone() and merged weights added as set patches
          - id: ac-2
            given: set patches to install
            when: add_patches is called
            then: |
              each key is prefixed with diffusion_model. to match ModelPatcher namespace
          - id: ac-3
            given: merged tensors
            when: stored as patches
            then: all tensors are on CPU (transferred from GPU during this phase)
          - id: ac-4
            given: the base model uses bf16 storage dtype
            when: set patch tensors are created
            then: they match the base model storage dtype
          - id: ac-5
            given: IS_CHANGED is called twice with no LoRA file changes
            when: hashes are compared
            then: they are identical enabling a cache hit
          - id: ac-6
            given: IS_CHANGED is called and a LoRA file has been modified
            when: the hash is computed
            then: it differs from the previous call triggering a cache miss
        implementation_notes: |
          After batched evaluation produces dict of {key: merged_tensor_on_gpu},
          transfer each to CPU with .cpu(), cast to base model storage dtype with
          .to(base_dtype). Get base dtype from first value in
          model_patcher.model_state_dict(). Clone model: merged = model_patcher.clone().
          Build patch dict: {f"diffusion_model.{k}": ("set", tensor) for k, tensor in
          merged_state.items()}. Install: merged.add_patches(patches, strength_patch=1.0).
          Note: the set patch format for add_patches is a tuple (strength, ("set", tensor),
          strength_model, None, None) -- check that add_patches handles the format or if
          raw tuple is needed. Verify against ComfyUI comfy/model_patcher.py add_patches
          and comfy/lora.py calculate_weight for exact format. IS_CHANGED: implement as
          @classmethod on WIDENExitNode -- receives same args as execute(). Walk recipe
          to collect all LoRA file paths, compute hashlib.sha256 of (path, mtime, size)
          tuples sorted by path. Return hex digest.
          Files: nodes/exit.py.

      - title: WIDEN Core Algorithm
        slug: widen-core
        type: feature
        description: |
          Port of the WIDEN algorithm from merge-router src/core/. Includes
          filter_delta (single-model importance filtering), merge_weights
          (multi-model parameter routing), ranking mechanisms, divergence
          metrics, and batched variants for GPU-vectorized operation.
          Pure algorithm code with no ComfyUI imports.
        acceptance_criteria:
          - id: ac-1
            given: base and delta tensors
            when: filter_delta is called with a t_factor
            then: importance-filtered delta is returned with low-importance parameters zeroed
          - id: ac-2
            given: multiple model tensors and a backbone
            when: merge_weights is called
            then: each parameter is routed to the most-important contributor via calibrated softmax
          - id: ac-3
            given: batched inputs of shape [B, *param_shape]
            when: filter_delta_batched or merge_weights_batched is called
            then: results match per-key variants applied individually
          - id: ac-4
            given: lib/widen.py
            when: imported
            then: no ComfyUI modules are imported (pure torch and stdlib)
          - id: ac-5
            given: the WIDEN implementation
            when: compared against merge-router src/core/widen.py
            then: algorithm behavior is equivalent for identical inputs within float tolerance
          - id: ac-6
            given: bf16 or fp16 input tensors
            when: WIDEN computation runs
            then: internal computation uses fp32 for numerical stability
          - id: ac-7
            given: no advanced configuration
            when: WIDEN is initialized
            then: |
              it uses default WIDENConfig values with ranking_strategy=percentile,
              sparsity_method=softmax, s_calibration=1.0
          - id: ac-8
            given: a non-OOM error during filter_delta_batched
            when: the error is caught
            then: unfiltered delta is used as passthrough and a warning is logged
          - id: ac-9
            given: a non-OOM error during merge_weights_batched
            when: the error is caught
            then: simple averaging is used as fallback and a warning is logged
        implementation_notes: |
          Port from ~/Projects/merge-router/src/core/widen.py. Key classes/functions
          to port: WIDEN class with filter_delta(), merge_weights(),
          filter_delta_batched(), merge_weights_batched(), _disentangle(),
          _rank_importance(), _calibrate(). Also port WIDENConfig dataclass with
          fields: n_models, t_factor, s_calibration, ranking_strategy,
          sparsity_method, calibration_mode, dtype. Port supporting modules:
          lib/divergence.py from src/core/divergence.py (divergence metrics),
          lib/ranking.py from src/core/ranking.py (ranking mechanisms),
          lib/numerical_config.py from src/core/numerical_config.py (eps values
          per dtype). Strip: any CLI imports, config file parsing, logging setup
          (use stdlib logging). Keep: all torch operations, numerical stability
          handling (upcast to fp32 for computation, downcast result back), batched
          variants operating on [B, *shape] tensors. Fallback behavior (AC-8, AC-9)
          from merge-router lines ~905-928 in scripts/lora_chain_merge.py -- wrap
          batched WIDEN calls in try/except, on non-OOM error fall back to passthrough
          (filter_delta) or averaging (merge_weights), log warning via logging.warning().
          Test by creating small synthetic tensors (e.g., 8x8 float32) and verifying
          filter_delta zeros low-importance entries, merge_weights routes correctly.
          Compare against merge-router by running both on same input and checking allclose.
          Files: lib/widen.py, lib/divergence.py, lib/ranking.py, lib/numerical_config.py.

      - title: Batched Pipeline Executor
        slug: batched-executor
        type: feature
        description: |
          OpSignature-based parameter grouping and batched GPU evaluation.
          Groups parameters by (affecting_sets, shape, ndim), computes optimal
          batch sizes based on free VRAM, applies LoRAs via torch.bmm, and
          handles OOM backoff. Ported from merge-router lora_chain_merge.py.
        acceptance_criteria:
          - id: ac-1
            given: a set of parameter keys with varying shapes and affecting sets
            when: grouped by OpSignature
            then: keys with identical shape and affecting sets are in the same group
          - id: ac-2
            given: a batch of parameters and LoRA DeltaSpecs
            when: bmm LoRA apply runs
            then: torch.bmm produces correct deltas matching per-key application
          - id: ac-3
            given: available VRAM and parameter shapes
            when: compute_batch_size is called
            then: it returns a batch size targeting 70 percent of free VRAM
          - id: ac-4
            given: a torch.cuda.OutOfMemoryError during batch evaluation
            when: OOM backoff triggers
            then: the failed chunk retries at batch size 1 while other chunks continue normally
          - id: ac-5
            given: the executor completes evaluation
            when: merged tensors are produced
            then: all result tensors are on CPU ready for set patch installation
          - id: ac-6
            given: a base model with bf16 storage dtype
            when: batched evaluation produces merged results
            then: output tensors match the base model storage dtype
          - id: ac-7
            given: a LoRA with LoKr weights
            when: batched apply runs
            then: LoKr weights use per-key torch.kron on GPU instead of bmm
        implementation_notes: |
          Port from ~/Projects/merge-router/scripts/lora_chain_merge.py. Key pieces:
          OpSignature frozen dataclass with affecting_sets (frozenset), shape (tuple),
          ndim (int). DeltaSpec dataclass with fields for LoRA factors (up, down,
          scale, alpha, kind, rank, key_index). compute_batch_size(shape, n_models,
          dtype, free_vram) formula: B = floor(free_vram * 0.7 / (numel(shape) *
          dtype_bytes * (3 + 3 * n_models))). _apply_lora_set_batched_gpu(base_batch,
          delta_specs, ...) -- partition specs by (kind, rank), stack up/down matrices,
          torch.bmm(down, up) for standard LoRA, torch.kron per-key for LoKr, scatter
          deltas back by key_index. OOM backoff: wrap chunk evaluation in try/except
          torch.cuda.OutOfMemoryError, on catch call torch.cuda.empty_cache() and retry
          with B=1. Define DeltaSpec in lib/types.py or lib/executor.py and import from
          lib/lora/ loaders. The executor is the main integration point -- it calls into
          lib/widen.py for WIDEN ops, lib/lora/*.py for LoRA loading, and walks the
          recipe tree from lib/recipe.py.
          Files: lib/executor.py, lib/types.py (for DeltaSpec/OpSignature if shared).

      - title: Architecture-Specific LoRA Loaders
        slug: lora-loaders
        type: feature
        description: |
          LoRA loading with architecture-specific key mapping. Each architecture
          has distinct key naming conventions and special handling requirements.
          Loaders produce DeltaSpec objects for the batched executor pipeline.
        acceptance_criteria:
          - id: ac-1
            given: a LoRA file and a detected architecture tag
            when: the appropriate loader is selected
            then: the correct architecture-specific loader handles key mapping
          - id: ac-2
            given: any architecture loader
            when: it processes a LoRA file
            then: it produces DeltaSpec objects compatible with the batched executor
          - id: ac-3
            given: a new architecture needs LoRA support
            when: a loader module is added to lib/lora/
            then: it integrates without modifying existing loaders (pluggable design)
          - id: ac-4
            given: any loader
            when: it implements the loader interface
            then: |
              it provides load(path) for loading, affected_keys property for the
              key set, get_delta_specs(keys) returning DeltaSpecs, and cleanup()
              for resource release
        implementation_notes: |
          Define loader interface in lib/lora/base.py as an abstract base class or
          protocol: class LoRALoader(ABC) with @abstractmethod load(self, path, strength),
          @property affected_keys -> set[str], get_delta_specs(self, keys) -> list[DeltaSpec],
          cleanup(self). Each architecture implements in its own module. DeltaSpec
          dataclass (in lib/types.py or lib/executor.py) needs: key, key_index, kind
          (standard/lokr/qkv), rank, up (Tensor), down (Tensor), scale, alpha, offset
          (optional tuple for QKV). Loader selection: simple dict lookup in executor
          like {"sdxl": SDXLLoader, "zimage": ZImageLoader}. For AC-3 pluggable design:
          use registry pattern or just the dict -- adding new arch means adding one entry.
          Files: lib/lora/base.py, lib/lora/__init__.py (registry).

      - title: SDXL LoRA Loader
        slug: sdxl-loader
        type: requirement
        parent: "@lora-loaders"
        acceptance_criteria:
          - id: ac-1
            given: an SDXL LoRA safetensors file
            when: loaded with the SDXL loader
            then: |
              LoRA keys are mapped to diffusion_model input_blocks, middle_block,
              and output_blocks keys
          - id: ac-2
            given: SDXL LoRA factors (up, down, alpha)
            when: DeltaSpecs are produced
            then: each spec contains correct rank, kind, and factor tensors
        implementation_notes: |
          Port from merge-router or implement fresh. SDXL LoRA key mapping: keys follow
          patterns like lora_unet_input_blocks_0_0_op.lora_down.weight -> base key
          input_blocks.0.0.weight. ComfyUI own comfy/lora.py has model_lora_keys_unet()
          that builds this mapping. Options: (1) Use ComfyUI key mapping function and
          wrap in our loader interface, (2) Implement standalone for consistency.
          Recommend option (1) for SDXL since ComfyUI handles all edge cases (attention,
          proj_in/out, time_embed). Load safetensors with safetensors.torch.load_file(),
          map keys, extract up/down/alpha per key, construct DeltaSpec objects. Standard
          LoRA: kind=standard, up=lora_up.weight, down=lora_down.weight, alpha from lora
          key or default to rank.
          Files: lib/lora/sdxl.py.

      - title: Z-Image LoRA Loader
        slug: zimage-loader
        type: requirement
        parent: "@lora-loaders"
        acceptance_criteria:
          - id: ac-1
            given: a Z-Image LoRA file with separate to_q, to_k, to_v keys
            when: loaded with Z-Image loader
            then: QKV keys are fused into the base model attention.qkv.weight layout
          - id: ac-2
            given: a Z-Image LoRA with Diffusers-style key names
            when: key mapping runs
            then: keys are correctly mapped to S3-DiT parameter names
          - id: ac-3
            given: Z-Image LoRA factors
            when: DeltaSpecs are produced
            then: |
              QKV-fused specs have correct offset indexing for the fused weight
              where q occupies 0 to 3840, k occupies 3840 to 7680, and v occupies
              7680 to 11520
        implementation_notes: |
          Port from ~/Projects/merge-router/scripts/zimage_lora_merge.py. Key function:
          _parse_lora_key(key) which maps Diffusers LoRA key names to S3-DiT base model
          keys and identifies QKV components. Z-Image base model uses fused
          attention.qkv.weight (11520x3840 = 3x3840) but LoRAs have separate
          to_q/to_k/to_v. The loader must: (1) Parse each LoRA key to identify
          target parameter and QKV component. (2) For QKV keys, create DeltaSpecs
          with kind=qkv and offset=(0, q_start, q_len) indicating which third of
          the fused weight this LoRA targets. The offset tuple is (dimension=0,
          start, length) where start is 0/3840/7680 for q/k/v respectively and
          length is 3840. (3) Handle non-QKV keys (FFN, norm, etc.) as standard
          LoRA. Also handle LoKr weights if present -- these have lokr_w1, lokr_w2
          instead of lora_up/lora_down, use kind=lokr. The Diffusers key mapping
          handles patterns like transformer_blocks.0.attn.to_q -> layers.0.attention.qkv
          (with offset for q portion).
          Files: lib/lora/zimage.py.

      - title: Memory Management
        slug: memory-management
        type: feature
        description: |
          GPU memory lifecycle during and after merge execution. Covers per-chunk
          tensor cleanup, between-group GC cycles, loader resource teardown, and
          ensuring all final patches are CPU-only.
        acceptance_criteria:
          - id: ac-1
            given: batched evaluation processes a chunk of parameters
            when: the chunk completes and results transfer to CPU
            then: all GPU tensors for that chunk are deleted and freed
          - id: ac-2
            given: an OpSignature group completes all chunks
            when: transitioning to the next group
            then: gc.collect() and torch.cuda.empty_cache() are called
          - id: ac-3
            given: all LoRA files have been loaded and evaluation is complete
            when: cleanup runs
            then: all loader resources are freed including delta caches and file handles
          - id: ac-4
            given: the complete merge execution
            when: final merged patches are produced
            then: all patch tensors are on CPU with no GPU tensor references remaining
          - id: ac-5
            given: peak GPU usage during a chunk
            when: compared to compute_batch_size estimate
            then: actual usage does not exceed the estimate by more than 20 percent
        implementation_notes: |
          Memory management is woven throughout lib/executor.py. Key patterns to port
          from merge-router scripts/lora_chain_merge.py: (1) Per-chunk cleanup: after
          transferring results to CPU, explicitly del base_batch and gpu intermediates
          then gc.collect() and torch.cuda.empty_cache(). (2) Per-group cleanup: between
          OpSignature groups, call gc.collect() + torch.cuda.empty_cache(). (3) Loader
          cleanup: after evaluation completes, call loader.cleanup() for each loader
          (which calls clear_delta_cache() and drops cached LoRA state). (4) Final
          cleanup: ensure returned merged_state dict contains only CPU tensors. The
          merge-router source has ~8 explicit gc.collect/empty_cache calls -- identify
          each and port the pattern. For AC-5 testing: use torch.cuda.max_memory_allocated()
          before/after chunk and compare to compute_batch_size prediction. This is a
          cross-cutting concern -- its ACs affect implementation in lib/executor.py
          primarily, also nodes/exit.py (loader cleanup after execute completes) and
          lib/lora/base.py (cleanup interface).
          Files: lib/executor.py (primary), nodes/exit.py (loader teardown),
          lib/lora/base.py (cleanup interface).

      - title: Per-Block Control
        slug: per-block-control
        type: feature
        description: |
          BLOCK_CONFIG custom type enabling per-block t_factor and LoRA strength
          overrides. Architecture-specific config nodes expose block group sliders.
          Uses Option B design with explicit config type wired to consuming nodes.
        acceptance_criteria:
          - id: ac-1
            given: no BLOCK_CONFIG inputs connected to any node
            when: the workflow executes
            then: all nodes behave identically to pre-block-control behavior
          - id: ac-2
            given: architecture-specific block config nodes exist
            when: a user creates a block config for their model architecture
            then: block group sliders are available with float range 0.0 to 2.0
          - id: ac-3
            given: a single BLOCK_CONFIG output
            when: connected to multiple consuming nodes
            then: it fans out correctly to each consumer
        implementation_notes: |
          Adds BLOCK_CONFIG custom ComfyUI type. Architecture-specific config nodes
          go in nodes/block_config_sdxl.py, nodes/block_config_zimage.py. Each
          exposes sliders for its architecture block groups: SDXL has input_blocks
          (groups of 3: IN00-02, IN03-05, IN06-08), middle_block, output_blocks
          (groups of 3). Z-Image has layers (groups of 5: L00-04, L05-09, ... L25-29),
          noise_refiner, context_refiner. Each slider FLOAT range 0.0-2.0, step 0.05.
          ComfyUI allows typing values outside slider range so -1.0 is accessible.
          The node produces a BlockConfig dataclass (in lib/recipe.py). Backwards
          compatibility: when block_config fields on RecipeMerge/RecipeLoRA are None,
          executor uses global t_factor/strength -- no special casing needed.
          Files: nodes/block_config_sdxl.py, nodes/block_config_zimage.py,
          lib/recipe.py (BlockConfig dataclass).

      - title: Block Config Type
        slug: block-config-type
        type: requirement
        parent: "@per-block-control"
        acceptance_criteria:
          - id: ac-1
            given: a BlockConfig dataclass
            when: constructed with arch and block_overrides
            then: it is frozen and stores per-block float values as a tuple of pairs
          - id: ac-2
            given: RecipeLoRA and RecipeMerge dataclasses
            when: block_config field is present
            then: it accepts BlockConfig or None
        implementation_notes: |
          Add to lib/recipe.py: @dataclass(frozen=True) class BlockConfig with fields
          arch (str), block_overrides (tuple), layer_type_overrides (tuple). The
          block_overrides is a tuple of (block_pattern, value) pairs e.g.,
          (("IN00-02", 0.5), ("MID", 1.0), ...). The layer_type_overrides is a tuple
          of (layer_type, value) pairs for cross-cutting layer type control (attention,
          feed_forward, norm, etc.). Add block_config: object = None field to both
          RecipeLoRA and RecipeMerge -- since frozen, this means defining new versions
          with the additional field. Field defaults to None for backwards compat. The
          arch field must match RecipeBase.arch -- validated at Exit time.
          Files: lib/recipe.py.

      - title: Merge Per-Block T-Factor
        slug: merge-block-config
        type: requirement
        parent: "@per-block-control"
        acceptance_criteria:
          - id: ac-1
            given: a BLOCK_CONFIG connected to Merge block_t_factor input
            when: Exit evaluates the merge step
            then: per-block t_factor overrides are applied instead of global t_factor
          - id: ac-2
            given: no BLOCK_CONFIG connected to Merge
            when: Exit evaluates
            then: global t_factor applies to all blocks (backwards compatible)
        implementation_notes: |
          Merge node gains optional block_t_factor input of type BLOCK_CONFIG in
          INPUT_TYPES. When present, stored in RecipeMerge.block_config. At Exit
          evaluation time, for each parameter key, executor: (1) classifies key into
          block group using architecture-specific patterns (e.g., for SDXL
          input_blocks.3. -> IN03-05), (2) looks up block group in
          BlockConfig.block_overrides, (3) uses override value as t_factor instead
          of global RecipeMerge.t_factor. If key block group not in overrides, falls
          back to global t_factor. Block classification function should live in
          lib/block_classify.py with one function per arch.
          Files: nodes/merge.py (add input), lib/executor.py (use block config
          during eval), lib/block_classify.py (key-to-block mapping).

      - title: LoRA Per-Block Strength
        slug: lora-block-config
        type: requirement
        parent: "@per-block-control"
        acceptance_criteria:
          - id: ac-1
            given: a BLOCK_CONFIG connected to LoRA node block_strength input
            when: Exit applies LoRA deltas
            then: per-block strength scaling is applied to LoRA deltas
          - id: ac-2
            given: no BLOCK_CONFIG connected to LoRA node
            when: Exit applies LoRA deltas
            then: global strength applies uniformly (backwards compatible)
        implementation_notes: |
          LoRA node gains optional block_strength input of type BLOCK_CONFIG in
          INPUT_TYPES. When present, stored in RecipeLoRA.block_config. At Exit time
          during batched LoRA apply phase (_apply_lora_set_batched_gpu), for each
          parameter key in batch: (1) classify key into block group, (2) look up
          override strength in BlockConfig, (3) multiply LoRA delta by per-block
          strength instead of global strength. This scales LoRA contribution before
          WIDEN sees it. During DeltaSpec processing, scale each spec effective
          strength by per-block override. Since batching groups keys by OpSignature
          (same shape + affecting sets), and per-block strength varies by key, need
          to apply scaling per-key within batch -- either as diagonal scaling matrix
          or by splitting batch by block group.
          Files: nodes/lora.py (add input), lib/executor.py (per-block scaling
          in LoRA apply).
      ```


      ## Tasks


      derive_from_specs: true


      ## Implementation Notes


      Architecture scope: SDXL and Z-Image are concrete requirements with

      implementation detail. Flux and Qwen support is planned but not specced

      as requirements -- loaders can be added as new requirement specs under

      lora-loaders when ready.


      The dependency chain for implementation:

      1. recipe-system (standalone, pure Python)

      2. widen-core (standalone, pure torch)

      3. entry-node, lora-node, compose-node, merge-node (depend on recipe-system)

      4. lora-loaders (depend on arch knowledge, produce DeltaSpecs)

      5. batched-executor (depends on widen-core + lora-loaders + recipe-system)

      6. memory-management (depends on batched-executor + lora-loaders)

      7. exit-node + sub-reqs (depends on batched-executor + recipe-system + memory-management)

      8. per-block-control (layers on top of everything)
    status: active
    derived_tasks:
      - "@implement-recipe-type-system"
      - "@implement-entry-node"
      - "@implement-lora-node"
      - "@implement-compose-node"
      - "@implement-merge-node"
      - "@implement-exit-node"
      - "@implement-exit-recipe-analysis"
      - "@implement-exit-batched-evaluation"
      - "@implement-exit-patch-installation"
      - "@implement-widen-core-algorithm"
      - "@implement-batched-pipeline-executor"
      - "@implement-architecture-specific-lora-loaders"
      - "@implement-sdxl-lora-loader"
      - "@implement-z-image-lora-loader"
      - "@implement-memory-management"
      - "@implement-per-block-control"
      - "@implement-block-config-type"
      - "@implement-merge-per-block-t-factor"
      - "@implement-lora-per-block-strength"
    derived_specs:
      - "@recipe-system"
      - "@entry-node"
      - "@lora-node"
      - "@compose-node"
      - "@merge-node"
      - "@exit-node"
      - "@exit-recipe-analysis"
      - "@exit-batched-eval"
      - "@exit-patch-install"
      - "@widen-core"
      - "@batched-executor"
      - "@lora-loaders"
      - "@sdxl-loader"
      - "@zimage-loader"
      - "@memory-management"
      - "@per-block-control"
      - "@block-config-type"
      - "@merge-block-config"
      - "@lora-block-config"
    source_path: /tmp/widen-plan.md
    created_at: 2026-02-10T19:44:03.151Z
    approved_at: null
    completed_at: null
    notes:
      - _ulid: 01KH4HA42FBM7ZS0H0EKPMES8T
        created_at: 2026-02-10T19:44:03.151Z
        author: "@claude"
        content: |-
          Implementation notes:

          Architecture scope: SDXL and Z-Image are concrete requirements with
          implementation detail. Flux and Qwen support is planned but not specced
          as requirements -- loaders can be added as new requirement specs under
          lora-loaders when ready.

          The dependency chain for implementation:
          1. recipe-system (standalone, pure Python)
          2. widen-core (standalone, pure torch)
          3. entry-node, lora-node, compose-node, merge-node (depend on recipe-system)
          4. lora-loaders (depend on arch knowledge, produce DeltaSpecs)
          5. batched-executor (depends on widen-core + lora-loaders + recipe-system)
          6. memory-management (depends on batched-executor + lora-loaders)
          7. exit-node + sub-reqs (depends on batched-executor + recipe-system + memory-management)
          8. per-block-control (layers on top of everything)
  - _ulid: 01KH508VCV991R0CS6GX3Q40ZW
    slugs:
      - plan-testing-strategy
    title: Testing Strategy
    content: |
      # Testing Strategy

      Restructure @testing-infrastructure into a parent feature with three sub-requirements,
      add a CI spec, and establish a project convention requiring AC-annotated test coverage
      for every implementation task.

      ## Specs

      ```yaml
      - title: Testing Infrastructure
        slug: testing-infrastructure
        type: feature
        parent: "@foundation"
        description: |
          Comprehensive testing strategy for comfy-ecaj-nodes. Covers pytest
          configuration, ComfyUI mocking, recipe fixtures, node graph validation
          via mock entry/exit, and CI pipeline. Parent feature for all testing
          sub-requirements.
        acceptance_criteria:
          - id: ac-1
            given: a developer runs pytest from the project root
            when: tests execute
            then: all tests pass without requiring a running ComfyUI instance
          - id: ac-2
            given: any implementation task is completed
            when: its tests are inspected
            then: each spec AC has a corresponding test annotated with AC @spec-ref ac-N

      - title: ComfyUI Mocking and Fixtures
        slug: comfyui-mocking
        type: requirement
        parent: "@testing-infrastructure"
        description: |
          pytest conftest.py with sys.modules mocking for ComfyUI imports,
          MockModelPatcher class, and recipe tree fixtures. The base layer that
          all other tests depend on.
        acceptance_criteria:
          - id: ac-1
            given: a test needs a ModelPatcher-like object
            when: it uses the mock_model_patcher fixture
            then: |
              the mock provides model_state_dict(filter_prefix) returning a dict
              of small fake tensors keyed like diffusion_model.input_blocks.0.0.weight,
              clone() returning a new MockModelPatcher, add_patches() storing patches,
              get_key_patches() returning patch data, and patches_uuid property
          - id: ac-2
            given: a test needs a recipe tree
            when: it uses recipe fixtures
            then: |
              pre-built recipe trees are available for single-LoRA, multi-LoRA set,
              compose (2 branches), chain (2 sequential merges), and full
              (compose + chain) patterns
          - id: ac-3
            given: tests for nodes that import ComfyUI modules like folder_paths
            when: they run without ComfyUI installed
            then: |
              ComfyUI modules are mocked via sys.modules patching in conftest.py
              before any node module is imported
          - id: ac-4
            given: a test needs fake SDXL or Z-Image state dict keys
            when: it uses arch-specific fixtures
            then: |
              fixture provides a dict with representative key patterns for each
              supported architecture (input_blocks for SDXL, layers + noise_refiner
              for Z-Image)
        implementation_notes: |
          Use ComfyUI's own pattern from tests-unit/ and ComfyUI_Selectors:
          sys.modules patching in conftest.py before node imports. MockModelPatcher
          should use small tensors (4x4 float32) for speed. Recipe fixtures build
          on lib/recipe.py dataclasses. Arch fixtures provide representative state
          dict key sets for detection testing.
          Files: tests/conftest.py, tests/mocks/__init__.py, tests/mocks/mock_comfy.py

      - title: Node Graph Testing
        slug: node-graph-testing
        type: requirement
        parent: "@testing-infrastructure"
        description: |
          Integration tests that validate the recipe graph building pipeline.
          Uses mock entry node to feed RecipeBase into the node chain, validates
          recipe tree structure through LoRA/Compose/Merge, and uses a mock
          executor path in Exit to verify the tree would produce correct operation
          sequences (filter_delta vs merge_weights) without GPU execution.
        acceptance_criteria:
          - id: ac-1
            given: a mock Entry node producing a RecipeBase with arch sdxl
            when: wired to LoRA node then to Merge node
            then: |
              the resulting RecipeMerge contains the correct base (RecipeBase)
              and target (RecipeLoRA with the specified LoRA) and t_factor
          - id: ac-2
            given: a recipe graph with compose target containing 3 branches
            when: the mock executor analyzes the tree
            then: it identifies this as a merge_weights operation (not filter_delta)
          - id: ac-3
            given: a recipe graph with single LoRA target
            when: the mock executor analyzes the tree
            then: it identifies this as a filter_delta operation
          - id: ac-4
            given: a chain of two Merge nodes (inner merge feeds outer base)
            when: the mock executor walks the tree
            then: it identifies inner merge must evaluate first and feeds into outer
          - id: ac-5
            given: an invalid recipe graph (e.g. RecipeBase wired to compose branch)
            when: validation runs
            then: a clear error is raised naming the invalid type and position
          - id: ac-6
            given: a complete graph matching the hyphoria workflow from design doc 6.5
            when: built and validated through the node chain
            then: the recipe tree structure matches the expected compose-merge-chain pattern
        implementation_notes: |
          Create tests/test_graph.py with helper functions that instantiate node
          classes and call their FUNCTION methods directly to build recipe trees.
          The mock executor is a lightweight tree walker (separate from the real
          executor) that returns an operation plan (list of {op: filter_delta|merge_weights,
          keys: ...}) without touching GPU. This validates the Exit node's recipe
          analysis logic independently.
          Files: tests/test_graph.py, tests/helpers/graph_builder.py

      - title: CI Pipeline
        slug: ci-pipeline
        type: requirement
        parent: "@testing-infrastructure"
        description: |
          GitHub Actions workflow for automated testing. Initial scope is
          unit tests with CPU-only torch and ruff linting on push/PR. Designed
          to be extended later with comfy-test registration and workflow smoke tests.
        acceptance_criteria:
          - id: ac-1
            given: a push to any branch or a PR is opened
            when: GitHub Actions runs
            then: |
              pytest executes with CPU-only PyTorch on ubuntu-latest and all
              tests pass
          - id: ac-2
            given: the CI workflow
            when: linting step runs
            then: ruff check passes with no errors
          - id: ac-3
            given: CI completes
            when: results are reported
            then: PR shows green check for both test and lint jobs
        implementation_notes: |
          Follow ComfyUI's test-unit.yml pattern: install CPU-only torch via
          --index-url https://download.pytorch.org/whl/cpu, install project deps,
          run pytest. Add ruff for linting. Single ubuntu-latest runner to start
          (extend to matrix later). Add pyproject.toml [tool.ruff] config.
          Files: .github/workflows/test.yml, pyproject.toml (ruff + pytest config)
      ```

      ## Tasks

      derive_from_specs: true

      ## Implementation Notes

      The existing @testing-infrastructure spec and task need to be restructured:
      the current spec becomes the parent feature, its ACs are redistributed into
      the new sub-requirements (comfyui-mocking absorbs the original ACs), and the
      existing task is updated to reflect the new scope.

      Additionally, a project convention should be established requiring AC-annotated
      test coverage (# AC: @spec ac-N) for every implementation task. This is enforced
      by the local-review workflow which checks for AC annotations in test files.
    status: active
    derived_tasks:
      - "@implement-comfyui-mocking-and-fixtures"
      - "@implement-node-graph-testing"
      - "@implement-ci-pipeline"
    derived_specs:
      - "@comfyui-mocking"
      - "@node-graph-testing"
      - "@ci-pipeline"
    source_path: /home/chapel/Projects/comfy-ecaj-nodes/.claude/plans/testing-strategy.md
    created_at: 2026-02-11T00:05:30.139Z
    approved_at: null
    completed_at: null
    notes:
      - _ulid: 01KH508VCVC1M4ZJX4J0KRM024
        created_at: 2026-02-11T00:05:30.139Z
        author: "@claude"
        content: |-
          Implementation notes:

          The existing @testing-infrastructure spec and task need to be restructured:
          the current spec becomes the parent feature, its ACs are redistributed into
          the new sub-requirements (comfyui-mocking absorbs the original ACs), and the
          existing task is updated to reflect the new scope.

          Additionally, a project convention should be established requiring AC-annotated
          test coverage (# AC: @spec ac-N) for every implementation task. This is enforced
          by the local-review workflow which checks for AC annotations in test files.
  - _ulid: 01KHCJ41CGR9M0YXT5FZTH76F7
    slugs:
      - plan-full-model-merging-widen-on-full-checkpoints
    title: Full Model Merging — WIDEN on Full Checkpoints
    content: |
      # Full Model Merging — WIDEN on Full Checkpoints

      Extends the WIDEN merge pipeline to support full model checkpoint merging
      alongside existing LoRA merging. Models are loaded from disk out-of-band
      from ComfyUI using safetensors streaming (safe_open), matching the deferred
      loading pattern used for LoRAs. This avoids requiring ComfyUI to load
      additional models into memory and enables per-batch streaming for large
      checkpoints.

      merge-router (sibling project at ../merge-router, used for offline merge
      experiments) already proves WIDEN works on full model state dicts. The gap
      is plumbing full model weights through the ComfyUI recipe tree and exit
      node execution pipeline.

      Architecture scope: SDXL and Z-Image (current supported set). Flux/Qwen
      architecture support is tracked separately.

      Key terminology: "WIDEN" is the custom ComfyUI type string used for all
      recipe nodes (RecipeBase, RecipeLoRA, RecipeCompose, RecipeMerge). Nodes
      with RETURN_TYPES = ("WIDEN",) can connect to any input accepting WIDEN.

      ## Specs

      ```yaml
      - title: Full Model Recipe Type
        slug: full-model-recipe
        type: feature
        description: |
          Frozen recipe dataclass representing a full model checkpoint to merge.
          Stores a file path and strength (like RecipeLoRA stores LoRA paths),
          not a ComfyUI MODEL reference. This enables deferred disk-based loading
          at Exit time via safetensors streaming. Follows all recipe conventions:
          frozen, tuples not lists, no GPU tensors.
        acceptance_criteria:
          - id: ac-1
            given: a RecipeModel instance
            when: a field is assigned after construction
            then: a FrozenInstanceError is raised
          - id: ac-2
            given: a RecipeModel
            when: inspecting its fields
            then: |
              it has path (str), strength (float, default 1.0), and
              block_config (BlockConfig or None, default None)
          - id: ac-3
            given: a RecipeModel instance
            when: passed to RecipeCompose.with_branch()
            then: a new RecipeCompose is returned containing it as a branch
          - id: ac-4
            given: a RecipeMerge constructed with target=RecipeModel
            when: the tree is inspected
            then: construction succeeds and target is the RecipeModel
          - id: ac-5
            given: the RecipeNode type alias
            when: inspected
            then: RecipeModel is included in the union
          - id: ac-6
            given: a RecipeModel
            when: inspected for GPU tensors
            then: no torch.Tensor objects are found (path and strength only)
        implementation_notes: |
          Add to lib/recipe.py alongside existing dataclasses. Pattern follows
          RecipeLoRA but simpler -- single path+strength instead of tuple of
          LoRA dicts. No MappingProxyType needed (only scalar fields). Update
          __all__ and RecipeNode type alias to include RecipeModel.

          The path field stores the checkpoint filename (resolved to full path
          at Exit time via folder_paths, same as LoRA path resolution).
          block_config enables per-block strength control, reusing existing
          BlockConfig type.

          Files: lib/recipe.py.

      - title: Model Input Node
        slug: model-input-node
        type: feature
        description: |
          ComfyUI node that produces a RecipeModel from a checkpoint file picker.
          Loads from disk out-of-band from ComfyUI -- the model is NOT loaded at
          node execution time. Like the LoRA node, this is pure recipe building
          with zero GPU work and zero file I/O. The file path is stored in the
          recipe and resolved at Exit time for deferred streaming access.
        acceptance_criteria:
          - id: ac-1
            given: the node's INPUT_TYPES
            when: inspected
            then: |
              it has model_name (checkpoint file combo via folder_paths) and
              strength (FLOAT, default 1.0, range 0.0-2.0)
          - id: ac-2
            given: the node executes with a valid checkpoint name
            when: output is inspected
            then: it returns a RecipeModel with the filename and strength stored
          - id: ac-3
            given: the node executes
            when: checking GPU memory and disk I/O
            then: no GPU memory is allocated and no file is opened (deferred to Exit)
          - id: ac-4
            given: the node class
            when: inspecting CATEGORY
            then: it is ecaj/merge
          - id: ac-5
            given: the node's RETURN_TYPES
            when: inspected
            then: it returns WIDEN type (compatible with Compose and Merge inputs)
          - id: ac-6
            given: an optional BLOCK_CONFIG input
            when: connected
            then: the BlockConfig is stored in RecipeModel.block_config
        implementation_notes: |
          New file nodes/model_input.py. Pattern follows nodes/lora.py closely:
          checkpoint file combo via folder_paths.get_filename_list("checkpoints"),
          strength slider, optional BLOCK_CONFIG input. No chaining (unlike LoRA
          node) -- each RecipeModel represents one model. To merge multiple models,
          use Compose node.

          Register in __init__.py NODE_CLASS_MAPPINGS as
          "WIDENModelInput": WIDENModelInputNode with display name
          "WIDEN Model Input". The file path is stored as-is (the filename,
          not full path) -- Exit resolves via
          folder_paths.get_full_path("checkpoints", name) at execution time.

          Files: nodes/model_input.py, __init__.py.

      - title: Full Model Loader
        slug: full-model-loader
        type: feature
        description: |
          Streaming model loader using safetensors.safe_open() for memory-efficient
          per-batch access to full checkpoint weights. Matches the LoRALoader
          interface pattern but provides full weight tensors instead of low-rank
          factors. Handles key normalization between checkpoint file format and
          base model state dict format. Architecture-specific key mapping for
          SDXL and Z-Image. Only supports safetensors format (non-safetensors
          checkpoints raise a clear error).
        acceptance_criteria:
          - id: ac-1
            given: a safetensors checkpoint path
            when: the loader opens it
            then: |
              it uses safe_open() for memory-mapped access without loading the
              full file into memory
          - id: ac-2
            given: a list of base model parameter keys
            when: get_weights(keys) is called
            then: |
              it returns the corresponding weight tensors from the checkpoint
              file, correctly mapped from file key format to base model key format
          - id: ac-3
            given: an SDXL checkpoint file with model.diffusion_model prefix
            when: key mapping runs
            then: |
              file keys are normalized to match base model state dict keys
              (e.g., model.diffusion_model.input_blocks.0 maps to input_blocks.0)
          - id: ac-4
            given: a Z-Image checkpoint file
            when: key mapping runs
            then: |
              file keys are normalized to match base model state dict keys,
              handling the diffusion_model or transformer prefix variants
          - id: ac-5
            given: the loader
            when: affected_keys is accessed
            then: |
              it returns the set of base model keys that have corresponding
              diffusion model weights in the checkpoint file, excluding
              VAE and text encoder keys
          - id: ac-6
            given: the loader is no longer needed
            when: cleanup() is called
            then: the safe_open file handle is closed and resources freed
          - id: ac-7
            given: a checkpoint file with keys that don't match the base model
            when: the mismatch is detected
            then: a clear error is raised listing unmatched keys
          - id: ac-8
            given: the loader
            when: detecting architecture from file keys
            then: |
              it can determine architecture without loading any tensor data
              by inspecting normalized keys against architecture patterns
          - id: ac-9
            given: a non-safetensors checkpoint file (e.g., .ckpt, .pt)
            when: the loader attempts to open it
            then: |
              a clear error is raised explaining that only safetensors
              format is supported for model merging
        implementation_notes: |
          New file lib/model_loader.py. Uses safetensors.safe_open(path,
          framework="pt", device="cpu") context manager.

          Key normalization pipeline (runs once at open time):
          1. Read all keys from safe_open metadata (no tensor loading).
          2. Normalize: strip architecture-specific prefixes to canonical form.
             - SDXL files: strip "model.diffusion_model." prefix.
             - Z-Image files: strip "model.diffusion_model." or similar prefix.
             - Reference merge-router src/models/ for format-specific patterns.
          3. Filter: keep only diffusion model keys (drop VAE "first_stage_model."
             and text encoder "conditioner." / "cond_stage_model." keys).
          4. Build forward map: file_key -> base_model_key (normalized).
          5. Build reverse map: base_model_key -> file_key (for lookups).

          Architecture detection: run architecture patterns on NORMALIZED keys
          (post prefix-stripping), so the same _ARCH_PATTERNS from nodes/entry.py
          work. Detection must happen AFTER normalization, not before, since
          patterns expect state_dict-format keys (e.g., "diffusion_model.input_blocks"
          not "model.diffusion_model.input_blocks").

          get_weights(keys) calls f.get_tensor(reverse_map[key]) per key, returns
          list of tensors. Streaming means per-batch disk I/O but avoids full model
          in memory. The safe_open handle is kept open for the duration of execution
          (closed in cleanup()).

          Note: safetensors safe_open uses memory-mapping on supported platforms,
          but actual behavior may vary. The key guarantee is that tensors are not
          allocated in Python memory until get_tensor() is called.

          Files: lib/model_loader.py, reference lib/lora/base.py for interface
          pattern, reference merge-router src/models/ for key normalization.

      - title: Full Model Execution
        slug: full-model-execution
        type: feature
        description: |
          Exit node extension for executing WIDEN merge on full model checkpoints.
          Adds OpApplyModel operation to the recipe evaluation engine. During
          recipe analysis, detects RecipeModel nodes and opens streaming loaders.
          During batched evaluation, OpApplyModel loads model weights per-batch
          into registers, then existing OpFilterDelta/OpMergeWeights apply WIDEN
          importance routing unchanged. Validates architecture consistency between
          base model and merge models. Adding RecipeModel as a 5th recipe type
          requires updating all isinstance dispatch points across the codebase.
        acceptance_criteria:
          - id: ac-1
            given: a recipe tree containing RecipeModel nodes
            when: recipe analysis runs (lib/analysis.py)
            then: |
              it detects RecipeModel nodes, opens FullModelLoader instances for
              each unique path, and builds affected-key maps per model
          - id: ac-2
            given: a RecipeModel in a recipe tree
            when: compile_plan processes it
            then: an OpApplyModel op is emitted referencing the model's loader ID
          - id: ac-3
            given: execute_plan encounters OpApplyModel
            when: executing a batch of keys
            then: |
              it loads the model weight tensors for those keys from the streaming
              loader into a register (the raw weights, no arithmetic)
          - id: ac-4
            given: OpApplyModel result is in a register
            when: OpFilterDelta or OpMergeWeights uses it with a backbone register
            then: |
              WIDEN computes delta (model_weights - backbone) internally and
              routes by importance -- existing algorithm, unchanged
          - id: ac-5
            given: a recipe that mixes RecipeModel and RecipeLoRA nodes
            when: the exit node processes it
            then: |
              both paths execute correctly -- LoRA via existing DeltaSpec path,
              models via OpApplyModel streaming path
          - id: ac-6
            given: a checkpoint file whose detected architecture
            when: it differs from the base model architecture
            then: a clear error is raised naming both architectures and both file paths
          - id: ac-7
            given: full model weights loaded per-batch via streaming
            when: GPU evaluation completes for a batch
            then: |
              loaded weights are freed after use, not held resident
              (streaming loader re-reads from disk as needed)
          - id: ac-8
            given: GPU runs out of memory during full model evaluation
            when: OOM is caught
            then: |
              existing chunked_evaluation backoff retries at batch_size=1
              (compatible with streaming loader -- just re-reads fewer keys)
          - id: ac-9
            given: a RecipeModel with block_config
            when: per-block control is applied during execution
            then: |
              block-level strength scaling is applied to full model deltas
              the same way it applies to LoRA deltas
          - id: ac-10
            given: the checkpoint file does not exist or is not a valid safetensors file
            when: the exit node attempts to open it
            then: |
              a clear error is raised naming the missing file and which
              Model Input node referenced it
          - id: ac-11
            given: IS_CHANGED is called
            when: the recipe tree contains RecipeModel nodes
            then: |
              checkpoint file (mtime, size) is included in the hash alongside
              any LoRA file hashes
          - id: ac-12
            given: a recipe with only RecipeModel targets (no LoRAs)
            when: affected keys are computed
            then: |
              all diffusion model keys present in both base and merge model
              are processed (not just LoRA-affected subset)
          - id: ac-13
            given: a recipe composing 3 full models for merge
            when: execution runs
            then: |
              only one batch of model weights per loader is on GPU at a time
              (streaming loaders are read sequentially, not all at once)
        implementation_notes: |
          This is the largest spec -- it touches every isinstance dispatch
          point that currently hardcodes the 4 recipe types. All changes needed:

          RECIPE TYPE SYSTEM (lib/recipe.py):
          - Add RecipeModel to RecipeNode type alias (line 86).
          - Add RecipeModel to __all__ exports.

          VALIDATION (nodes/exit.py _validate_recipe_tree):
          - Add RecipeModel as valid Compose branch type (line 62).
          - Add RecipeModel as valid Merge target type (line 81).
          - Add RecipeModel leaf case (no children to validate).

          RECIPE ANALYSIS (lib/analysis.py):
          - _walk_to_base(): RecipeModel cannot be tree root -- add case
            that raises ValueError like RecipeLoRA (line 71-75).
          - _collect_lora_sets(): Add RecipeModel case that skips (no LoRAs
            to collect) instead of raising ValueError (line 124).
          - New function _collect_model_refs(): Walk tree, collect unique
            RecipeModel nodes with synthetic model IDs (parallel to
            _collect_lora_sets pattern).
          - Extend analyze_recipe() or create analyze_recipe_models() to
            open FullModelLoader per unique path, validate arch match,
            build model affected-key maps. Return extended AnalysisResult
            with model_loaders and model_affected fields.
          - Extend get_keys_to_process(): Union of LoRA affected keys AND
            model affected keys determines which keys need processing.

          EXIT NODE (nodes/exit.py):
          - _collect_lora_paths(): Add RecipeModel skip case so tree walk
            doesn't raise on model nodes.
          - New _collect_model_paths(): Parallel function that collects
            checkpoint file paths from RecipeModel nodes.
          - _compute_recipe_hash(): Include model file (mtime, size) in hash.
          - execute(): Call model analysis, pass model_loaders to executor.
          - Cleanup: Close model loaders after execution completes.

          PLAN COMPILER (lib/recipe_eval.py):
          - New OpApplyModel frozen dataclass: model_id (str), block_config,
            input_reg (int), out_reg (int).
          - Add OpApplyModel to _Op type alias (line 78).
          - _input_regs(): Add OpApplyModel case returning (input_reg,).
          - _PlanCompiler.compile_node(): Add RecipeModel dispatch that
            emits OpApplyModel (parallel to _compile_lora pattern).
          - _PlanCompiler._compile_merge(): Add RecipeModel to valid
            target isinstance check (line 283).

          PLAN EXECUTOR (lib/recipe_eval.py):
          - execute_plan() gains new parameter:
            model_loaders: dict[str, FullModelLoader] | None = None
            This keeps backward compatibility (None = no models).
          - OpApplyModel handler: look up loader by model_id, call
            loader.get_weights(keys), stack into [B, *shape] tensor,
            move to device/dtype, store in register.
          - evaluate_recipe() wrapper passes model_loaders through.

          COMPOSE NODE (nodes/compose.py):
          - Add RecipeModel to valid branch types (line 48).

          MERGE NODE (nodes/merge.py):
          - Add RecipeModel to valid target types (line 77-81).

          KEY INSIGHT: OpApplyModel just loads weights into a register.
          It does NOT compute deltas. OpFilterDelta internally computes
          delta = input_reg - backbone_reg via WIDEN's filter_delta_batched.
          So full model merge reuses 100% of the WIDEN algorithm unchanged.

          MIXED RECIPES: compile_plan handles both node types independently.
          A chained merge like Merge(base=Merge(base=Entry, target=LoRA),
          target=Model) evaluates inner merge (LoRA path) first, then outer
          merge (model path) with inner result as base. The only naming
          confusion: execute_plan's variable "lora_applied" (line 397) is
          actually "the weights in input_reg" which may be raw model weights
          when from OpApplyModel. Consider renaming to "applied" for clarity.

          PERFORMANCE NOTE: For recipes with only RecipeModel targets (no
          LoRAs), affected_keys is the full diffusion model key set. This
          means ALL keys are processed, not a small LoRA subset. Batch sizing
          via compute_batch_size() handles this, but total processing time is
          proportionally higher. For SDXL: ~1500 keys vs ~200-400 for typical
          LoRA merges.

          MEMORY: Streaming loaders mean only 1 batch of model weights on GPU
          at a time. For SDXL with batch_size=64, this is ~100-500MB GPU per
          batch vs 4.2GB for full model. With 3 models in a Compose, each
          batch loads 3 × batch_size weights sequentially.

          Files: lib/recipe.py, lib/recipe_eval.py, lib/analysis.py,
          nodes/exit.py, nodes/merge.py, nodes/compose.py.
      ```

      ## Tasks

      derive_from_specs: true

      ## Implementation Notes

      The dependency chain for implementation:
      1. full-model-recipe (standalone, extends lib/recipe.py)
      2. model-input-node (depends on full-model-recipe)
      3. full-model-loader (standalone, new lib/model_loader.py)
      4. full-model-execution (depends on all above + existing exit-node + batched-executor)

      Key architecture decisions:
      - Disk-based loading, not ComfyUI MODEL: checkpoint files are opened
        directly via safetensors.safe_open() at Exit time, bypassing ComfyUI's
        model loading. This enables streaming per-batch access without full
        model in memory. Same pattern as LoRA deferred loading.
      - OpApplyModel is a "load" not "compute": it loads model weights into a
        register. WIDEN's filter_delta/merge_weights handles the delta internally.
      - Mixed recipes work naturally: LoRA and model nodes coexist in the same
        recipe tree via chained merges.
      - Per-block control reuses existing BlockConfig and block_classify
        infrastructure. No new classifiers needed for SDXL/Z-Image.
      - Only safetensors format supported. Non-safetensors checkpoints get a
        clear error message.
      - VAE/CLIP/text encoder keys in checkpoint files are filtered out --
        only diffusion model weights are merged.
      - Architecture detection runs on normalized keys (post prefix-stripping)
        so existing _ARCH_PATTERNS work unchanged.

      Reference implementations:
      - merge-router/scripts/sdxl_merge.py: SDXL full model merge pattern
      - merge-router/scripts/qwen_merge.py: Streaming safe_open() pattern
      - merge-router/src/models/qwen_key_mapper.py: Key normalization pattern
    status: active
    derived_tasks:
      - "@implement-full-model-recipe-type"
      - "@implement-model-input-node"
      - "@implement-full-model-loader"
      - "@implement-full-model-execution"
    derived_specs:
      - "@full-model-recipe"
      - "@model-input-node"
      - "@full-model-loader"
      - "@full-model-execution"
    source_path: /home/chapel/.claude/plans/cheeky-squishing-kahan.md
    created_at: 2026-02-13T22:32:07.824Z
    approved_at: null
    completed_at: null
    notes:
      - _ulid: 01KHCJ41CG4R637R23MYDGSNT8
        created_at: 2026-02-13T22:32:07.824Z
        author: "@claude"
        content: |-
          Implementation notes:

          The dependency chain for implementation:
          1. full-model-recipe (standalone, extends lib/recipe.py)
          2. model-input-node (depends on full-model-recipe)
          3. full-model-loader (standalone, new lib/model_loader.py)
          4. full-model-execution (depends on all above + existing exit-node + batched-executor)

          Key architecture decisions:
          - Disk-based loading, not ComfyUI MODEL: checkpoint files are opened
            directly via safetensors.safe_open() at Exit time, bypassing ComfyUI's
            model loading. This enables streaming per-batch access without full
            model in memory. Same pattern as LoRA deferred loading.
          - OpApplyModel is a "load" not "compute": it loads model weights into a
            register. WIDEN's filter_delta/merge_weights handles the delta internally.
          - Mixed recipes work naturally: LoRA and model nodes coexist in the same
            recipe tree via chained merges.
          - Per-block control reuses existing BlockConfig and block_classify
            infrastructure. No new classifiers needed for SDXL/Z-Image.
          - Only safetensors format supported. Non-safetensors checkpoints get a
            clear error message.
          - VAE/CLIP/text encoder keys in checkpoint files are filtered out --
            only diffusion model weights are merged.
          - Architecture detection runs on normalized keys (post prefix-stripping)
            so existing _ARCH_PATTERNS work unchanged.

          Reference implementations:
          - merge-router/scripts/sdxl_merge.py: SDXL full model merge pattern
          - merge-router/scripts/qwen_merge.py: Streaming safe_open() pattern
          - merge-router/src/models/qwen_key_mapper.py: Key normalization pattern
  - _ulid: 01KHDHEGJP4RMVVFZTC0GY96B5
    slugs:
      - plan-qwen-and-flux-2-klein-architecture-support
    title: Qwen and Flux 2 Klein Architecture Support
    content: |
      # Qwen and Flux 2 Klein Architecture Support

      Add WIDEN merge support for Qwen-Image (20B MMDiT) and Flux 2 Klein (4B/9B) architectures.

      ## Specs

      ```yaml
      - title: Qwen Architecture Support
        slug: qwen-support
        type: feature
        description: |
          Full WIDEN merge support for Qwen-Image (20B MMDiT, 60 dual-stream
          transformer blocks). Dual-stream (img/txt) architecture with separate
          Q/K/V projections -- no QKV fusion needed. Standard diffusers LoRA
          format plus A1111/kohya and LyCORIS variants.
          Reference implementation in ../merge-router (qwen_key_mapper.py,
          qwen_splitter.py, qwen_merge.py).
          Key files to modify: nodes/entry.py, lib/block_classify.py,
          lib/lora/ (new qwen.py), lib/model_loader.py,
          nodes/block_config_qwen.py (new), __init__.py.
        acceptance_criteria:
          - id: ac-1
            given: a ComfyUI model whose state dict contains transformer_blocks keys
            when: detect_architecture() runs
            then: |
              it returns "qwen" and "qwen" is in _SUPPORTED_ARCHITECTURES.
              Must not false-match Flux which uses double_blocks not transformer_blocks.
          - id: ac-2
            given: a parameter key from a Qwen model
            when: classify_key_qwen() is called
            then: |
              transformer_blocks.N maps to TBNN (zero-padded two digits, e.g.
              TB00-TB59). Block indices discovered dynamically via regex (not
              hardcoded to 60). Non-block keys (proj_out, norm_out, pos_embed,
              x_embedder, context_embedder, t_embedder, time_text_embed)
              return None.
          - id: ac-3
            given: a parameter key from a Qwen model
            when: classify_layer_type(key, "qwen") is called
            then: |
              returns "attention" for attn.to_q, attn.to_k, attn.to_v,
              attn.add_q_proj, attn.add_k_proj, attn.add_v_proj,
              attn.to_out, attn.to_add_out, attn.norm_q, attn.norm_k,
              attn.norm_added_q, attn.norm_added_k.
              Returns "feed_forward" for img_mlp, txt_mlp.
              Returns "norm" for img_norm1, img_norm2, txt_norm1, txt_norm2,
              img_mod, txt_mod.
          - id: ac-4
            given: a Qwen LoRA file in diffusers format
            when: QwenLoader parses it
            then: |
              keys like transformer.transformer_blocks.N.attn.to_q.lora_A.weight
              are mapped to base model key transformer_blocks.N.attn.to_q.weight
              and produce DeltaSpec with kind="standard", offset=None, and
              correct up/down tensor pairs.
          - id: ac-5
            given: a Qwen LoRA file in A1111/kohya format
            when: QwenLoader parses it
            then: |
              keys like lora_unet_transformer_blocks_N_attn_to_q.lora_down.weight
              are normalized with compound name preservation (transformer_blocks,
              add_k_proj, add_q_proj, add_v_proj, to_add_out, img_mod, txt_mod,
              img_mlp, txt_mlp, img_norm, txt_norm, time_text_embed) and
              produce DeltaSpec with kind="standard" and correct base model key.
          - id: ac-6
            given: a Qwen LoRA file in LyCORIS format
            when: QwenLoader parses it
            then: |
              keys like lycoris_transformer_blocks_N_attn_to_q.lokr_w1 are
              normalized using underscore-to-dot conversion with compound name
              handling and produce DeltaSpec with kind="lokr" and correct
              w1/w2 tensor pairs.
          - id: ac-7
            given: a safetensors checkpoint with Qwen architecture keys
            when: model_loader.py opens it
            then: |
              architecture is detected as "qwen" from transformer_blocks pattern.
              Key normalization strips diffusion_model. and transformer. prefixes.
              VAE and text encoder keys are excluded.
          - id: ac-8
            given: a WIDENBlockConfigQwen node in ComfyUI
            when: rendered
            then: |
              shows 60 individual block sliders (TB00-TB59, FLOAT 0.0-2.0,
              default 1.0) plus 3 layer-type sliders (attention, feed_forward,
              norm). Generated via make_block_config_node() factory.
          - id: ac-9
            given: QwenLoader, classify_key_qwen, classify_layer_type for qwen,
              model_loader qwen detection, and block config node all implemented
            when: all registries are checked
            then: |
              _SUPPORTED_ARCHITECTURES includes "qwen", _CLASSIFIERS has "qwen"
              entry, _LAYER_TYPE_PATTERNS has "qwen" patterns, LOADER_REGISTRY
              has "qwen" entry, _ARCH_PATTERNS has qwen detection,
              NODE_CLASS_MAPPINGS has WIDENBlockConfigQwen.

      - title: Flux 2 Klein Architecture Support
        slug: flux-klein-support
        type: feature
        description: |
          Full WIDEN merge support for Flux 2 Klein (4B and 9B variants).
          Dual block types: double_blocks (joint img/txt attention with fused
          QKV per stream) and single_blocks (fused linear1 = QKV + MLP
          projection). Requires offset-based DeltaSpec for QKV fusing,
          following Z-Image's pattern in lib/lora/zimage.py.
          Klein 9B has 8 double + 24 single = 32 blocks.
          Klein 4B has 5 double + 20 single = 25 blocks.
          Block config sized for Klein 9B max. Flux.1 out of scope for now.
          Key files to modify: nodes/entry.py, lib/block_classify.py,
          lib/lora/ (new flux.py), lib/model_loader.py,
          nodes/block_config_flux.py (new), __init__.py.
        acceptance_criteria:
          - id: ac-1
            given: a ComfyUI model whose state dict contains double_blocks keys
            when: detect_architecture() runs
            then: it returns "flux" and "flux" is in _SUPPORTED_ARCHITECTURES
          - id: ac-2
            given: a parameter key from a Flux Klein model
            when: classify_key_flux() is called
            then: |
              double_blocks.N maps to DB00-DB07 (9B) or DB00-DB04 (4B).
              single_blocks.N maps to SB00-SB23 (9B) or SB00-SB19 (4B).
              Block indices are discovered dynamically from keys (not hardcoded).
              Non-block keys (guidance_in, time_in, vector_in, img_in, txt_in,
              final_layer) return None.
          - id: ac-3
            given: a parameter key from a Flux Klein model
            when: classify_layer_type(key, "flux") is called
            then: |
              returns "attention" for img_attn, txt_attn, qkv, proj,
              norm.query_norm, norm.key_norm.
              Returns "feed_forward" for img_mlp, txt_mlp, linear2.
              Returns "norm" for img_mod, txt_mod, modulation
              (excluding attention-specific norms already captured above).
          - id: ac-4
            given: a Flux LoRA targeting double_block attention (to_q, to_k, to_v)
            when: FluxLoader parses it
            then: |
              separate LoRA to_q/to_k/to_v components map to fused
              img_attn.qkv and txt_attn.qkv base model keys with
              DeltaSpec kind=qkv_q/qkv_k/qkv_v and offset=(start, length)
              tuples. Two QKV fusions per double_block (one per stream).
          - id: ac-5
            given: a Flux LoRA targeting single_block components
            when: FluxLoader parses it
            then: |
              LoRA to_q, to_k, to_v map to slices of fused linear1 weight
              with DeltaSpec kind=qkv_q/qkv_k/qkv_v and offset tuples.
              proj_mlp maps with a new offset-aware kind (e.g. offset_mlp)
              and offset tuple. The gpu_ops.py offset-slice branch (line 200)
              is extended to include the new kind.
          - id: ac-6
            given: a Flux LoRA in BFL/kohya format
            when: FluxLoader parses it
            then: |
              keys like lora_unet_double_blocks_N_img_attn_qkv.lora_down.weight
              are correctly normalized and mapped to base model keys.
          - id: ac-7
            given: a Flux LoRA in diffusers format
            when: FluxLoader parses it
            then: |
              keys like transformer.double_blocks.N.img_attn.to_q.lora_A.weight
              are correctly normalized and mapped to base model keys.
          - id: ac-8
            given: a safetensors checkpoint with Flux architecture keys
            when: model_loader.py opens it
            then: |
              architecture is detected as "flux" from double_blocks pattern.
              Key normalization strips diffusion_model. and transformer. prefixes.
          - id: ac-9
            given: a WIDENBlockConfigFlux node in ComfyUI
            when: rendered
            then: |
              shows 32 block sliders for Klein 9B max (DB00-DB07 + SB00-SB23,
              FLOAT 0.0-2.0, default 1.0) plus 3 layer-type sliders.
              Klein 4B models have unused block sliders default to 1.0.
              Generated via make_block_config_node() factory.
          - id: ac-10
            given: a Klein 4B model (5 double + 20 single blocks) and a Klein 9B
              model (8 double + 24 single blocks)
            when: both are processed with the same "flux" arch tag
            then: |
              the classifier returns block names matching the actual blocks
              present in each model (e.g. DB00-DB04 for 4B, DB00-DB07 for 9B).
              No error is raised for either variant.
          - id: ac-11
            given: all Flux Klein components implemented
            when: all registries are checked
            then: |
              _SUPPORTED_ARCHITECTURES includes "flux", _CLASSIFIERS has "flux"
              entry, _LAYER_TYPE_PATTERNS has "flux" patterns, LOADER_REGISTRY
              has "flux" entry, _ARCH_PATTERNS has flux detection,
              NODE_CLASS_MAPPINGS has WIDENBlockConfigFlux.
      ```

      ## Tasks

      ```yaml
      - title: Implement Qwen detection and block classification
        slug: qwen-detect-classify
        spec_ref: "@qwen-support"
        priority: 2
        description: |
          Enable Qwen detection in nodes/entry.py (add "qwen" to
          _SUPPORTED_ARCHITECTURES). Add classify_key_qwen() in
          lib/block_classify.py mapping transformer_blocks.N to TB00-TB59
          with dynamic index discovery (regex, not hardcoded 60).
          Add Qwen layer type patterns (attention/feed_forward/norm).
          Register in _CLASSIFIERS and _LAYER_TYPE_PATTERNS.
          Update __all__ in block_classify.py to export new functions.

          BREAKING TESTS to update:
          - tests/test_entry.py: test_qwen_detected_but_unsupported (line 189)
            must become test_qwen_detected_and_supported.
          - tests/test_layer_type_classify.py: assertions that qwen returns
            None must become positive classification tests.
          - tests/test_merge_block_config.py: assertion that
            get_block_classifier("qwen") is None must test real classifier.
          - tests/test_lora_loaders.py: assertion that get_loader("qwen")
            raises ValueError must test real loader (covered by lora task).

          New tests in: tests/test_entry.py, tests/test_layer_type_classify.py,
          tests/test_merge_block_config.py.
          Covers ac-1, ac-2, ac-3.
        tags:
          - qwen
          - classification

      - title: Implement Qwen LoRA loader
        slug: qwen-lora-loader
        spec_ref: "@qwen-support"
        priority: 2
        depends_on:
          - "@qwen-detect-classify"
        description: |
          Create lib/lora/qwen.py implementing QwenLoader (subclass LoRALoader).
          Handle 3 LoRA formats -- diffusers, A1111/kohya, LyCORIS -- with
          compound name preservation from merge-router reference
          (qwen_merge.py lines 254-283). No QKV fusion needed (separate
          to_q/to_k/to_v). Standard up/down DeltaSpec production.
          Register in LOADER_REGISTRY in lib/lora/__init__.py.

          Update tests/test_lora_loaders.py: replace ValueError assertion for
          get_loader("qwen") with real loader tests. Add new tests with
          synthetic safetensors LoRA files for each format.
          Covers ac-4, ac-5, ac-6.
        tags:
          - qwen
          - lora

      - title: Implement Qwen model loader support
        slug: qwen-model-loader
        spec_ref: "@qwen-support"
        priority: 2
        description: |
          Add Qwen architecture detection pattern to _ARCH_PATTERNS in
          lib/model_loader.py (match transformer_blocks, distinguish from
          Flux double_blocks). Add Qwen-specific entries to _FILE_KEY_PREFIXES
          (e.g. model.transformer.) and _EXCLUDED_PREFIXES if needed.
          Update _normalize_key() for any Qwen-specific prefix stripping.

          Tests in tests/test_model_loader.py: add Qwen detection and
          key normalization tests.
          Covers ac-7.
        tags:
          - qwen
          - model-loader

      - title: Implement Qwen block config node and registration
        slug: qwen-block-config
        spec_ref: "@qwen-support"
        priority: 2
        depends_on:
          - "@qwen-detect-classify"
          - "@qwen-lora-loader"
          - "@qwen-model-loader"
        description: |
          Create nodes/block_config_qwen.py using make_block_config_node()
          factory with 60 block definitions (TB00-TB59) and 3 layer-type
          sliders. Register WIDENBlockConfigQwen in NODE_CLASS_MAPPINGS and
          NODE_DISPLAY_NAME_MAPPINGS in __init__.py. Verify all registries
          are wired (ac-9). Add integration test exercising mock Qwen recipe
          through the full pipeline.

          Tests in: tests/test_per_block_control.py (new Qwen node tests).
          Covers ac-8, ac-9.
        tags:
          - qwen
          - node

      - title: Implement Flux Klein detection and block classification
        slug: flux-detect-classify
        spec_ref: "@flux-klein-support"
        priority: 2
        description: |
          Enable Flux detection in nodes/entry.py (add "flux" to
          _SUPPORTED_ARCHITECTURES). Add classify_key_flux() in
          lib/block_classify.py mapping double_blocks.N to DB0N and
          single_blocks.N to SB0N with dynamic index discovery.
          Add Flux layer type patterns. Register in _CLASSIFIERS and
          _LAYER_TYPE_PATTERNS. Update __all__ exports.

          BREAKING TESTS to update:
          - tests/test_entry.py: test_flux_detected_but_unsupported (line 174)
            must become test_flux_detected_and_supported.
          - tests/test_layer_type_classify.py: assertions that flux returns
            None must become positive classification tests.
          - tests/test_merge_block_config.py: assertion that
            get_block_classifier("flux") is None must test real classifier.

          New tests in: tests/test_entry.py, tests/test_layer_type_classify.py,
          tests/test_merge_block_config.py.
          Covers ac-1, ac-2, ac-3.
        tags:
          - flux
          - classification

      - title: Implement Flux Klein LoRA loader
        slug: flux-lora-loader
        spec_ref: "@flux-klein-support"
        priority: 2
        depends_on:
          - "@flux-detect-classify"
        description: |
          Create lib/lora/flux.py implementing FluxLoader (subclass LoRALoader).
          Handle double_block QKV fusing (img_attn.qkv and txt_attn.qkv per
          block, two streams) using existing qkv_q/qkv_k/qkv_v DeltaSpec kinds
          with offset=(start, length).

          Handle single_block linear1 fusing (4-way offset split for
          to_q/to_k/to_v/proj_mlp). Q/K/V components reuse qkv_q/qkv_k/qkv_v
          kinds. The MLP projection slice needs a new DeltaSpec kind (e.g.
          "offset_mlp") and a minor executor update in lib/gpu_ops.py to add
          it to the offset-aware kind set at line 200. This is a one-line
          change to extend the tuple.

          Follow Z-Image offset-based DeltaSpec pattern from lib/lora/zimage.py.
          Support both BFL/kohya and diffusers LoRA formats.
          Register in LOADER_REGISTRY.

          Update tests/test_lora_loaders.py: replace ValueError assertion for
          get_loader("flux") with real loader tests. Add synthetic safetensors
          LoRA files testing both double_block QKV and single_block linear1.
          Covers ac-4, ac-5, ac-6, ac-7.
        tags:
          - flux
          - lora

      - title: Implement Flux Klein model loader support
        slug: flux-model-loader
        spec_ref: "@flux-klein-support"
        priority: 2
        description: |
          Add Flux architecture detection pattern to _ARCH_PATTERNS in
          lib/model_loader.py (match double_blocks). Add Flux-specific entries
          to _FILE_KEY_PREFIXES and _EXCLUDED_PREFIXES. Update _normalize_key()
          for Flux-specific prefix stripping.

          Tests in tests/test_model_loader.py: add Flux detection and
          key normalization tests.
          Covers ac-8.
        tags:
          - flux
          - model-loader

      - title: Implement Flux Klein block config node and registration
        slug: flux-block-config
        spec_ref: "@flux-klein-support"
        priority: 2
        depends_on:
          - "@flux-detect-classify"
          - "@flux-lora-loader"
          - "@flux-model-loader"
        description: |
          Create nodes/block_config_flux.py using make_block_config_node()
          factory with 32 block definitions for Klein 9B max (DB00-DB07 +
          SB00-SB23) and 3 layer-type sliders. Register WIDENBlockConfigFlux
          in NODE_CLASS_MAPPINGS and NODE_DISPLAY_NAME_MAPPINGS in __init__.py.
          Verify both 4B and 9B variants work with same "flux" arch tag (ac-10).
          Verify all registries wired (ac-11). Add integration test.

          Tests in: tests/test_per_block_control.py (new Flux node tests).
          Covers ac-9, ac-10, ac-11.
        tags:
          - flux
          - node
      ```

      ## Implementation Notes

      Implementation order: Qwen first (simpler, no QKV fusion, has reference impl in
      ../merge-router), then Flux Klein (complex QKV fusing in both block types).

      Within each architecture, 3 tasks can run in parallel (detect-classify, model-loader,
      and then lora-loader + block-config after classify completes).

      Key reference files:
      - Qwen reference: ../merge-router/src/models/qwen_key_mapper.py (compound names)
      - Qwen reference: ../merge-router/scripts/qwen_merge.py (LoRA formats, lines 90-308)
      - QKV fusion pattern: lib/lora/zimage.py (lines 34-44, _QKV_OFFSETS)
      - Block config factory: nodes/block_config.py (make_block_config_node)
      - LoRA base class: lib/lora/base.py (LoRALoader abstract)
      - Current detection: nodes/entry.py (detect_architecture, _SUPPORTED_ARCHITECTURES)

      Existing architecture patterns (SDXL + Z-Image) serve as templates. Each new
      architecture adds: one classifier function, one set of layer type patterns, one
      LoRA loader module, one block config node module, and registry entries.

      Executor change for Flux linear1: lib/gpu_ops.py line 200 currently handles
      qkv_q/qkv_k/qkv_v kinds with offset-based slicing. Flux single_block linear1
      4-way split reuses qkv_* for Q/K/V and adds one new kind (e.g. "offset_mlp")
      to the same elif branch. This is a one-line tuple extension, not a new code
      path. The offset=(start, length) mechanism is already general-purpose.

      No changes needed to: lib/analysis.py (dispatches via registry), lib/per_block.py
      (dispatches via classify_key/classify_layer_type registries), nodes/exit.py
      (architecture-agnostic), lib/persistence.py (recipe serialization is
      architecture-agnostic).
    status: active
    derived_tasks:
      - "@qwen-detect-classify"
      - "@qwen-lora-loader"
      - "@qwen-model-loader"
      - "@qwen-block-config"
      - "@flux-detect-classify"
      - "@flux-lora-loader"
      - "@flux-model-loader"
      - "@flux-block-config"
    derived_specs:
      - "@qwen-support"
      - "@flux-klein-support"
    source_path: /home/chapel/.claude/plans/functional-yawning-clover.md
    created_at: 2026-02-14T07:39:36.918Z
    approved_at: null
    completed_at: null
    notes:
      - _ulid: 01KHDHEGJP2B4YRB9JRG8KC4CF
        created_at: 2026-02-14T07:39:36.918Z
        author: "@claude"
        content: |-
          Implementation notes:

          Implementation order: Qwen first (simpler, no QKV fusion, has reference impl in
          ../merge-router), then Flux Klein (complex QKV fusing in both block types).

          Within each architecture, 3 tasks can run in parallel (detect-classify, model-loader,
          and then lora-loader + block-config after classify completes).

          Key reference files:
          - Qwen reference: ../merge-router/src/models/qwen_key_mapper.py (compound names)
          - Qwen reference: ../merge-router/scripts/qwen_merge.py (LoRA formats, lines 90-308)
          - QKV fusion pattern: lib/lora/zimage.py (lines 34-44, _QKV_OFFSETS)
          - Block config factory: nodes/block_config.py (make_block_config_node)
          - LoRA base class: lib/lora/base.py (LoRALoader abstract)
          - Current detection: nodes/entry.py (detect_architecture, _SUPPORTED_ARCHITECTURES)

          Existing architecture patterns (SDXL + Z-Image) serve as templates. Each new
          architecture adds: one classifier function, one set of layer type patterns, one
          LoRA loader module, one block config node module, and registry entries.

          Executor change for Flux linear1: lib/gpu_ops.py line 200 currently handles
          qkv_q/qkv_k/qkv_v kinds with offset-based slicing. Flux single_block linear1
          4-way split reuses qkv_* for Q/K/V and adds one new kind (e.g. "offset_mlp")
          to the same elif branch. This is a one-line tuple extension, not a new code
          path. The offset=(start, length) mechanism is already general-purpose.

          No changes needed to: lib/analysis.py (dispatches via registry), lib/per_block.py
          (dispatches via classify_key/classify_layer_type registries), nodes/exit.py
          (architecture-agnostic), lib/persistence.py (recipe serialization is
          architecture-agnostic).
