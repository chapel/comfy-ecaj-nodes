{
  "generated_at": "2026-02-15T21:33:09.806Z",
  "branch": "feat/implement-diffusion-model-input-node",
  "context": {
    "focus": null,
    "threads": [],
    "open_questions": [],
    "updated_at": "2026-02-15T21:33:09.806Z"
  },
  "active_tasks": [],
  "pending_review_tasks": [
    {
      "ref": "01KHHE6K1",
      "title": "Implement Diffusion Model Input Node",
      "started_at": "2026-02-15T21:22:19.223Z",
      "priority": 2,
      "spec_ref": "@diffusion-model-input-node",
      "note_count": 2,
      "last_note_at": "2026-02-15T21:31:29.263Z",
      "todo_count": 0,
      "incomplete_todos": 0
    }
  ],
  "recent_notes": [
    {
      "task_ref": "01KHHE6K1",
      "task_title": "Implement Diffusion Model Input Node",
      "task_status": "pending_review",
      "note_ulid": "01KHHKEE",
      "created_at": "2026-02-15T21:31:29.263Z",
      "author": "@claude",
      "content": "Implemented: (1) Added source_dir field to RecipeModel with default 'checkpoints', (2) Created WIDENDiffusionModelInputNode in nodes/diffusion_model_input.py with source_dir='diffusion_models' and unet fallback, (3) Updated analyze_recipe_models and _compute_recipe_hash to use source_dir for path resolution, (4) Updated _build_model_resolver to accept (name, source_dir) tuple and handle diffusion_models/unet fallback, (5) 18 tests covering all 8 ACs. 890 tests pass, ruff clean."
    },
    {
      "task_ref": "01KHHE6KF",
      "task_title": "Investigate ComfyUI CLIP clone and patch API",
      "task_status": "completed",
      "note_ulid": "01KHHJ2F",
      "created_at": "2026-02-15T21:07:28.723Z",
      "author": "@claude",
      "content": "## Spike Findings: ComfyUI CLIP clone and patch API\n\n### Q1: Does ComfyUI CLIP use ModelPatcher internally?\n\n**YES.** The CLIP class (comfy/sd.py:103-163) wraps its cond_stage_model in a ModelPatcher at line 131:\n\n    self.patcher = comfy.model_patcher.ModelPatcher(\n        self.cond_stage_model, load_device=load_device, offload_device=offload_device)\n\nThe patcher gets is_clip=True and hook_mode=MinVram.\n\n### Q2: Can add_patches('set', ...) apply CLIP weight patches?\n\n**YES.** CLIP.add_patches() delegates directly to self.patcher.add_patches() (sd.py:179-180). The same ('set', (tensor,)) patch format used for diffusion models works identically for CLIP. ComfyUI's own load_lora_for_models() at sd.py:72-100 demonstrates this — it calls clip.clone() then clip.add_patches(loaded, strength_clip) with exactly the same pattern as model patching.\n\n### Q3: What is the CLIP clone/patch API?\n\nCLIP.clone() (sd.py:165-174):\n- Creates a new CLIP(no_init=True)\n- Clones the patcher: n.patcher = self.patcher.clone()\n- SHARES cond_stage_model: n.cond_stage_model = self.cond_stage_model\n- Copies tokenizer, layer_idx, tokenizer_options\n\nThis is the SAME clone-and-patch pattern as the diffusion model pipeline. Our install_merged_patches() in exit.py should work on CLIP objects with minimal adaptation:\n1. Call clip.clone() instead of model_patcher.clone()\n2. Call clip.add_patches(patches, strength_patch=1.0) instead of model_patcher.add_patches(...)\n3. Or equivalently, work at the patcher level: clip.patcher.clone() + patcher.add_patches()\n\n### Q4: How to access CLIP state dict keys without loading weights to GPU?\n\nUse clip.patcher.model_state_dict() (model_patcher.py:604-612). This calls self.model.state_dict() under use_ejected() context, returning CPU tensors without GPU load.\n\nFor SDXL, the cond_stage_model is SDXLClipModel (sdxl_clip.py:41-68) which has:\n- self.clip_l (SD1 CLIP, 12 layers): keys like clip_l.transformer.text_model.encoder.layers.{N}.{component}.weight\n- self.clip_g (CLIP-G, 32 layers): keys like clip_g.transformer.text_model.encoder.layers.{N}.{component}.weight\n\nThe state_dict() returns ALL these keys together with their correct prefixes.\n\n### Key Architecture Details\n\n**State dict key format (SDXL CLIP):**\n- clip_l.transformer.text_model.encoder.layers.{0-11}.self_attn.{q,k,v,out}_proj.{weight,bias}\n- clip_l.transformer.text_model.encoder.layers.{0-11}.mlp.fc{1,2}.{weight,bias}\n- clip_l.transformer.text_model.encoder.layers.{0-11}.layer_norm{1,2}.{weight,bias}\n- clip_l.transformer.text_model.embeddings.token_embedding.weight\n- clip_l.transformer.text_model.embeddings.position_embedding.weight\n- clip_l.transformer.text_model.final_layer_norm.{weight,bias}\n- (same pattern for clip_g with 32 layers instead of 12)\n\n**LoRA key mapping (from comfy/lora.py:97-156):**\n- lora_te1_text_model_encoder_layers_{N}_{component} → clip_l.transformer.text_model.encoder.layers.{N}.{component}.weight\n- lora_te2_text_model_encoder_layers_{N}_{component} → clip_g.transformer.text_model.encoder.layers.{N}.{component}.weight\n- Also supports generic format: text_encoders.{full_key_without_.weight} → {full_key}\n\n**_unpatch_loaded_clones concern:**\nThe existing _unpatch_loaded_clones() in exit.py uses is_clone() which works across ALL ModelPatcher instances. A CLIP patcher clone would be detected correctly. However, the CLIP exit node should implement its own unpatch using the CLIP's patcher, not the diffusion model patcher.\n\n### Impact on Implementation\n\n1. **CLIP Entry node**: Access keys via clip.patcher.model_state_dict().keys() — zero GPU cost\n2. **CLIP Exit node**: Clone clip, install merged patches via add_patches('set', ...), return CLIP — same pattern as diffusion Exit\n3. **CLIP LoRA loader**: Map lora_te1_ → clip_l, lora_te2_ → clip_g (confirmed by comfy/lora.py)\n4. **CLIP Model loader**: Include conditioner.embedders.* keys, normalize to clip_l/clip_g format\n5. **Arch detection**: Check for both clip_l and clip_g prefixes in state dict → SDXL"
    },
    {
      "task_ref": "01KHHE6KF",
      "task_title": "Investigate ComfyUI CLIP clone and patch API",
      "task_status": "completed",
      "note_ulid": "01KHHH74",
      "created_at": "2026-02-15T20:52:33.107Z",
      "author": "@claude",
      "content": "Automation status set to manual_only: Spike - output is knowledge (documented in task notes), not code"
    },
    {
      "task_ref": "01KHHE6KF",
      "task_title": "Investigate ComfyUI CLIP clone and patch API",
      "task_status": "completed",
      "note_ulid": "01KHHGFH",
      "created_at": "2026-02-15T20:39:39.314Z",
      "author": "@claude",
      "content": "Output: document findings as task notes. Key questions: (1) Does ComfyUI CLIP use ModelPatcher internally? (2) Can add_patches('set', ...) apply CLIP weight patches? (3) If not, what is the CLIP clone/patch API? (4) How to access CLIP state dict keys without GPU load? Findings feed into @implement-clip-entry-node and @implement-clip-exit-node."
    },
    {
      "task_ref": "01KHHE6KD",
      "task_title": "Rename existing Model Input node display name",
      "task_status": "completed",
      "note_ulid": "01KHHGFF",
      "created_at": "2026-02-15T20:39:37.281Z",
      "author": "@claude",
      "content": "Change NODE_DISPLAY_NAME_MAPPINGS in __init__.py from 'WIDEN Model Input' to 'WIDEN Checkpoint Input'. This distinguishes it from the new Diffusion Model Input node. Only __init__.py needs to change."
    },
    {
      "task_ref": "01KHHE6K1",
      "task_title": "Implement Diffusion Model Input Node",
      "task_status": "pending_review",
      "note_ulid": "01KHHE6K",
      "created_at": "2026-02-15T19:59:49.284Z",
      "author": "@claude",
      "content": "Implementation notes:\n\nCreate nodes/diffusion_model_input.py mirroring model_input.py.\nUse folder_paths.get_filename_list(\"diffusion_models\") with fallback\nto \"unet\" for older ComfyUI versions (try/except at runtime).\nSet source_dir=\"diffusion_models\" on the RecipeModel.\nRegister in __init__.py with display name \"WIDEN Diffusion Model Input\".\n"
    }
  ],
  "active_todos": [],
  "ready_tasks": [
    {
      "ref": "01KHHE6K3",
      "title": "Implement Recipe Domain Field",
      "priority": 2,
      "spec_ref": "@recipe-domain-field",
      "tags": []
    }
  ],
  "blocked_tasks": [],
  "recently_completed": [
    {
      "ref": "01KHHE6KD",
      "title": "Rename existing Model Input node display name",
      "completed_at": "2026-02-15T21:21:26.171Z",
      "closed_reason": "Merged in PR #68. Renamed Model Input node display name from 'WIDEN Model Input' to 'WIDEN Checkpoint Input' in NODE_DISPLAY_NAME_MAPPINGS. This preparatory change clarifies the existing node reads from checkpoints/ directory, distinguishing it from the upcoming Diffusion Model Input node."
    },
    {
      "ref": "01KHHE6KF",
      "title": "Investigate ComfyUI CLIP clone and patch API",
      "completed_at": "2026-02-15T21:07:37.385Z",
      "closed_reason": null
    },
    {
      "ref": "01KHGYM2",
      "title": "Rework t_factor semantics: 0=base-only, remove negative values",
      "completed_at": "2026-02-15T15:29:51.070Z",
      "closed_reason": "Implemented t_factor rework: 0=base-only, removed negative values. 4 spec ACs updated/added, code in widen.py + merge.py, tests updated. 870 tests pass, ruff clean."
    },
    {
      "ref": "01KHFZ61",
      "title": "Implement: Incremental Block Recomputation",
      "completed_at": "2026-02-15T06:29:30.940Z",
      "closed_reason": "Implemented incremental block recomputation: structural fingerprint in persistence.py, change detection in block_classify.py, LRU-1 cache in exit.py. 41 tests covering all 16 ACs, 862 total tests pass, ruff clean."
    },
    {
      "ref": "01KHDRCK",
      "title": "Apply per-block strength scaling to model weights in OpApplyModel",
      "completed_at": "2026-02-14T09:43:29.104Z",
      "closed_reason": "Implemented per-block strength scaling for model weights in OpApplyModel. Added _apply_per_block_lora_strength call mirroring LoRA pattern. AC-15 covered with 2 tests. PR #64 created, awaiting CI/merge."
    },
    {
      "ref": "01KHDHEGX",
      "title": "Implement Flux Klein block config node and registration",
      "completed_at": "2026-02-14T08:38:56.263Z",
      "closed_reason": "Merged in PR #62. Implemented WIDENBlockConfigFlux node with 32 block sliders (DB00-DB07 + SB00-SB23) plus 3 layer-type sliders (attention, feed_forward, norm). Registered in NODE_CLASS_MAPPINGS. Klein 4B/9B variants handled with same 'flux' arch tag. All ACs covered: ac-9 (block sliders), ac-10 (variant handling), ac-11 (registry wiring). 18 tests added, all 815 tests pass."
    },
    {
      "ref": "01KHDHEGW",
      "title": "Implement Flux Klein model loader support",
      "completed_at": "2026-02-14T08:31:39.765Z",
      "closed_reason": "Merged in PR #61. Implemented Flux Klein model loader support with architecture detection from double_blocks pattern and key normalization (transformer. → diffusion_model.). 9 tests covering ac-8. All CI checks passed."
    },
    {
      "ref": "01KHDHEGV",
      "title": "Implement Flux Klein LoRA loader",
      "completed_at": "2026-02-14T08:26:22.584Z",
      "closed_reason": "Merged in PR #60. Implemented FluxLoader for Flux Klein architecture (4B/9B) with: double_block img_attn/txt_attn QKV fusing using qkv_q/qkv_k/qkv_v kinds with offsets; single_block linear1 4-way fusing (to_q/to_k/to_v/proj_mlp) with offset_mlp kind; support for BFL/kohya and diffusers LoRA formats; registered in LOADER_REGISTRY. AC coverage verified: ac-4 (double_block QKV fusing), ac-5 (single_block linear1 fusing), ac-6 (BFL/kohya format), ac-7 (diffusers format). 12 tests added, all 789 tests pass."
    },
    {
      "ref": "01KHDHEGT",
      "title": "Implement Flux Klein detection and block classification",
      "completed_at": "2026-02-14T08:16:03.308Z",
      "closed_reason": "Merged in PR #59. Added Flux Klein detection (ac-1) and block classification (ac-2, ac-10) with classify_key_flux() mapping double_blocks.N to DB0N and single_blocks.N to SB0N via dynamic index discovery. Added layer type patterns (ac-3) for attention, feed_forward, and norm. Registered in _CLASSIFIERS, _LAYER_TYPE_PATTERNS, and _SUPPORTED_ARCHITECTURES. 17 new tests with AC annotations, all 777 tests passing."
    },
    {
      "ref": "01KHDHEGS",
      "title": "Implement Qwen block config node and registration",
      "completed_at": "2026-02-14T08:09:56.960Z",
      "closed_reason": "Merged in PR #58. Implemented WIDENBlockConfigQwen node with 60 transformer block sliders (TB00-TB59) and 3 layer-type sliders (attention, feed_forward, norm). Registered in NODE_CLASS_MAPPINGS and NODE_DISPLAY_NAME_MAPPINGS. 9 tests covering AC-8 (60 individual blocks + layer type sliders) and AC-9 (registry wiring). All CI passed."
    }
  ],
  "recent_commits": [
    {
      "hash": "d7e9515",
      "full_hash": "d7e951549fdb78eb1c75ca1740f39979d49a6cb5",
      "date": "2026-02-15T21:31:53.000Z",
      "message": "feat: implement WIDEN Diffusion Model Input node",
      "author": "Jacob Chapel"
    },
    {
      "hash": "014594b",
      "full_hash": "014594bb6200ef80d940d3b7f52a234c2352ca03",
      "date": "2026-02-15T21:21:17.000Z",
      "message": "Merge pull request #68 from chapel/fix/rename-model-input-display",
      "author": "Jacob Chapel"
    },
    {
      "hash": "ec61944",
      "full_hash": "ec6194465cacc9fcb4c63a82dbb3fe484c1c9640",
      "date": "2026-02-15T21:19:25.000Z",
      "message": "fix: rename Model Input display name to Checkpoint Input",
      "author": "Jacob Chapel"
    },
    {
      "hash": "2603994",
      "full_hash": "260399441bdc57b1552f83b132da59ad6c2ec897",
      "date": "2026-02-15T17:07:53.000Z",
      "message": "Merge pull request #67 from chapel/feat/rework-t-factor-semantics",
      "author": "Jacob Chapel"
    },
    {
      "hash": "62c5825",
      "full_hash": "62c58250a34a835902597f69989d90d504b7f48c",
      "date": "2026-02-15T17:06:44.000Z",
      "message": "fix: address review findings — dead guard and docstring",
      "author": "Jacob Chapel"
    },
    {
      "hash": "1028605",
      "full_hash": "1028605761409d753c838f69404c31f1f65d23e9",
      "date": "2026-02-15T15:29:39.000Z",
      "message": "feat: rework t_factor semantics — 0 = base only, remove negative values",
      "author": "Jacob Chapel"
    },
    {
      "hash": "db13683",
      "full_hash": "db136834a815c412ffffc8db22b56761340679ab",
      "date": "2026-02-15T07:31:42.000Z",
      "message": "Merge pull request #66 from chapel/fix/classify-structural-keys",
      "author": "Jacob Chapel"
    },
    {
      "hash": "69356b7",
      "full_hash": "69356b772250fcba9ef0f403f3fb13c3ec820ba8",
      "date": "2026-02-15T07:23:26.000Z",
      "message": "fix: address review findings — lint E501 and stale docstring",
      "author": "Jacob Chapel"
    },
    {
      "hash": "bd5f4f5",
      "full_hash": "bd5f4f5adebbe8f6bbd665919c7d387b925d66e1",
      "date": "2026-02-15T07:16:39.000Z",
      "message": "fix: classify structural keys so per-block strength applies to all weights",
      "author": "Jacob Chapel"
    },
    {
      "hash": "ec88b3e",
      "full_hash": "ec88b3e833ee281b66ec49e3db7f243f38849bc5",
      "date": "2026-02-15T06:39:14.000Z",
      "message": "Merge pull request #65 from chapel/feat/incremental-block-recompute",
      "author": "Jacob Chapel"
    }
  ],
  "working_tree": {
    "clean": true,
    "staged": [],
    "unstaged": [],
    "untracked": []
  },
  "inbox_items": [
    {
      "ref": "01KHCXS4",
      "text": "Recipe serialization as a trait/protocol — serialize_recipe currently uses isinstance checks for each recipe type. Should be a protocol method on RecipeNode so new recipe types implement their own serialization. Prevents silent skips and keeps persistence.py decoupled from recipe type enumeration.",
      "created_at": "2026-02-14T01:55:53.531Z",
      "tags": [],
      "added_by": "@claude"
    },
    {
      "ref": "01KHCXS7",
      "text": "compute_lora_stats._walk() silently ignores unknown recipe node types — should raise ValueError like serialize_recipe does. Related to serialization-as-trait refactor.",
      "created_at": "2026-02-14T01:55:56.494Z",
      "tags": [],
      "added_by": "@claude"
    },
    {
      "ref": "01KHCXS9",
      "text": "load_affected_keys should wrap safetensors errors with helpful message pointing to cached file corruption — tells users to delete and re-run.",
      "created_at": "2026-02-14T01:55:58.446Z",
      "tags": [],
      "added_by": "@claude"
    },
    {
      "ref": "01KHDNHH",
      "text": "kspec plan import should wire depends_on from task YAML — currently ignores the field, requiring manual kspec batch to set dependencies after import. Encountered when importing Qwen/Flux plan with 4 dependent tasks.",
      "created_at": "2026-02-14T08:51:10.255Z",
      "tags": [
        "reflection",
        "kspec"
      ],
      "added_by": "@claude"
    },
    {
      "ref": "01KHDX4R",
      "text": "Add test for composed strength + block_config on OpApplyModel (strength != 1.0 AND block_config both active)",
      "created_at": "2026-02-14T11:04:00.499Z",
      "tags": [
        "test"
      ],
      "added_by": "@claude"
    },
    {
      "ref": "01KHFVFM",
      "text": "v2: Auto per-block LoRA importance analysis for targeted merging — compute Frobenius norm of B@A per block at recipe build time to auto-populate per-block strengths. Could extend to SVD spectral analysis, TIES/DARE pruning during merge execution, and TSV interference detection for multi-LoRA conflict prediction. Natural hook point: existing per_block.py infrastructure. See FreeFuse (spatial segmentation, different problem), LoRA Inspector, resize_lora, LoRA Power-Merger, Task Singular Vectors paper for prior art.",
      "created_at": "2026-02-15T05:13:28.800Z",
      "tags": [],
      "added_by": "@claude"
    },
    {
      "ref": "01KHG2ZK",
      "text": "Missing AC-10 test for incremental-block-recompute — no test covers save_model=True partial-recompute path with saved-state + metadata behavior. Flagged by codex review on PR #66.",
      "created_at": "2026-02-15T07:24:31.884Z",
      "tags": [],
      "added_by": "@claude"
    },
    {
      "ref": "01KHG2ZP",
      "text": "Incremental recompute cache-hit tests lack bit-identical output assertions — AC-2 requires bit-identical output vs full recompute, but tests only check that chunked_evaluation is not called, never comparing output tensors. Flagged by codex review on PR #66.",
      "created_at": "2026-02-15T07:24:35.005Z",
      "tags": [],
      "added_by": "@claude"
    },
    {
      "ref": "01KHG3A2",
      "text": "Add test helper fixture for block config node kwargs — generate full kwargs from node.INPUT_TYPES() with defaults, so tests only need to override specific values. Would prevent the 15+ manual edits needed when adding new sliders (as happened with structural keys fix).",
      "created_at": "2026-02-15T07:30:15.101Z",
      "tags": [
        "reflection",
        "test"
      ],
      "added_by": "@claude"
    },
    {
      "ref": "01KHHGS4",
      "text": "kspec plan import puts manual task description field into notes instead of task description — task shows no Description section, only notes. Means manual tasks from plan import are less self-documenting than spec-derived tasks. Related to 01KHDNHH (depends_on gap) — both are plan import fidelity issues.",
      "created_at": "2026-02-15T20:44:54.504Z",
      "tags": [
        "reflection",
        "kspec"
      ],
      "added_by": "@claude"
    }
  ],
  "stats": {
    "total_tasks": 90,
    "in_progress": 0,
    "pending_review": 1,
    "ready": 2,
    "blocked": 0,
    "completed": 78,
    "inbox_items": 10
  }
}