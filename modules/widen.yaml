_ulid: 01KH4GZW6PAQEQR1GSK88TE64W
slugs:
  - widen
title: WIDEN Merge
type: module
status:
  maturity: draft
  implementation: not_started
description: WIDEN-based model merging â€” weight disentanglement for intelligent parameter-level composition
tags: []
depends_on: []
implements: []
relates_to: []
tests: []
traits: []
notes: []
features:
  - _ulid: 01KH4HA42FZ8MM2XN4FZS75SVZ
    slugs:
      - recipe-system
    title: Recipe Type System
    type: feature
    tags: []
    description: |
      The WIDEN custom ComfyUI type and its recipe dataclasses. All recipe
      objects are frozen (immutable) to prevent aliasing bugs with ComfyUI
      caching and graph fan-out. Fields use tuples, not lists. Recipe objects
      hold no GPU tensors -- they are pure recipe descriptions.
    acceptance_criteria:
      - id: ac-1
        given: any recipe dataclass instance
        when: a field is assigned after construction
        then: a FrozenInstanceError is raised
      - id: ac-2
        given: a RecipeCompose with existing branches
        when: a new branch is appended
        then: a new RecipeCompose is returned with a new tuple (persistent tree semantics)
      - id: ac-3
        given: any recipe object
        when: inspected for GPU tensors
        then: no torch.Tensor objects are found (only references and metadata)
      - id: ac-4
        given: RecipeBase, RecipeLoRA, RecipeCompose, and RecipeMerge classes
        when: imported from lib.recipe
        then: all four are available and constructible with documented fields
      - id: ac-5
        given: nodes that output WIDEN type and nodes that accept WIDEN input
        when: ComfyUI loads the node pack
        then: WIDEN wire connections are valid between nodes in the graph editor
    depends_on: []
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-10T19:44:03.152Z
    status:
      maturity: draft
      implementation: implemented
  - _ulid: 01KH4HA42KCZR0ME6AH4G0WXHY
    slugs:
      - entry-node
    title: Entry Node
    type: feature
    tags: []
    description: |
      Boundary from ComfyUI MODEL world to WIDEN recipe world. Wraps a
      ModelPatcher reference in a RecipeBase. Auto-detects architecture
      (SDXL, Z-Image, Flux, Qwen) by inspecting state dict key patterns.
      Zero GPU work -- just stores reference and arch tag.
    acceptance_criteria:
      - id: ac-1
        given: a ComfyUI MODEL (ModelPatcher) input
        when: Entry node executes
        then: it returns a RecipeBase wrapping the ModelPatcher reference
      - id: ac-2
        given: an SDXL model with diffusion_model.input_blocks keys
        when: architecture detection runs
        then: arch field is set to sdxl
      - id: ac-3
        given: a Z-Image model with diffusion_model.layers keys and noise_refiner
        when: architecture detection runs
        then: arch field is set to zimage
      - id: ac-4
        given: Entry node executes
        when: output is inspected
        then: no GPU memory is allocated and no tensors are copied
      - id: ac-5
        given: a model whose state dict keys match none of the known architecture patterns
        when: architecture detection runs
        then: it raises a clear error listing supported architectures
    depends_on: []
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-10T19:44:03.155Z
    status:
      maturity: draft
      implementation: implemented
  - _ulid: 01KH4HA42PAEFBHY5GT1D1YZVP
    slugs:
      - lora-node
    title: LoRA Node
    type: feature
    tags: []
    description: |
      Declares a LoRA to be applied as part of the recipe. Uses our own
      loader (not ComfyUI built-in) for deferred loading enabling batched
      bmm apply at Exit time. Chains via optional prev input to form sets
      (multiple LoRAs applied to the same base).
    acceptance_criteria:
      - id: ac-1
        given: a LoRA file path and strength value
        when: LoRA node executes
        then: it returns a RecipeLoRA with the file path and strength in its loras tuple
      - id: ac-2
        given: two LoRA nodes chained via prev connection
        when: the second node executes
        then: the output RecipeLoRA contains both LoRAs in its loras tuple forming a set
      - id: ac-3
        given: the LoRA node in ComfyUI UI
        when: the node renders
        then: lora_name shows a dropdown via folder_paths.get_filename_list loras
      - id: ac-4
        given: a LoRA node with no prev connection
        when: it executes
        then: the output RecipeLoRA has a single-element loras tuple
      - id: ac-5
        given: a LoRA node with strength 0.0
        when: it executes
        then: |
          the LoRA still appears in the recipe (strength=0 is the executor
          concern, not the recipe builder concern)
    depends_on: []
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-10T19:44:03.158Z
    status:
      maturity: draft
      implementation: implemented
  - _ulid: 01KH4HA42SFZP4GCSWT6SS2TZ4
    slugs:
      - compose-node
    title: Compose Node
    type: feature
    tags: []
    description: |
      Accumulates branches for simultaneous WIDEN merging. Pure recipe building
      with zero computation. Chain multiple Compose nodes to accumulate any
      number of branches for the Merge node.
    acceptance_criteria:
      - id: ac-1
        given: a branch WIDEN input and no compose input
        when: Compose node executes
        then: it returns RecipeCompose with a single-element branches tuple
      - id: ac-2
        given: a branch input and a compose chain from a previous Compose
        when: Compose node executes
        then: it returns RecipeCompose with the new branch appended to existing branches
      - id: ac-3
        given: three Compose nodes chained together
        when: the final output is inspected
        then: all three branches are present in order
      - id: ac-4
        given: a RecipeBase wired directly to the branch input
        when: Compose node executes
        then: |
          it raises an error because branch must be a LoRA spec, compose group,
          or merge result -- a raw base without LoRAs is not a valid compose branch
    depends_on: []
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-10T19:44:03.162Z
    status:
      maturity: draft
      implementation: implemented
  - _ulid: 01KH4HA42XHMGZN46F7TF54G2B
    slugs:
      - merge-node
    title: Merge Node
    type: feature
    tags: []
    description: |
      The recipe builder for WIDEN merge operations. Produces RecipeMerge
      which the Exit node evaluates. Compose target triggers merge_weights,
      single target triggers filter_delta. Optional backbone override for
      explicit WIDEN importance reference.
    acceptance_criteria:
      - id: ac-1
        given: base and target WIDEN inputs with a t_factor value
        when: Merge node executes
        then: it returns a RecipeMerge with base, target, and t_factor stored
      - id: ac-2
        given: no backbone input connected
        when: Merge node executes
        then: backbone field is None in the RecipeMerge
      - id: ac-3
        given: an explicit backbone input connected
        when: Merge node executes
        then: the backbone reference is stored in RecipeMerge
      - id: ac-4
        given: a Merge output
        when: wired to another Merge base input
        then: it forms a valid chain for sequential merging
      - id: ac-5
        given: a RecipeLoRA or RecipeCompose wired to the base input
        when: Merge node executes
        then: |
          it raises an error because base must be RecipeBase or RecipeMerge
      - id: ac-6
        given: t_factor is -1.0
        when: stored in RecipeMerge
        then: the value is preserved (Exit interprets -1.0 as passthrough with no WIDEN)
    depends_on: []
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-10T19:44:03.165Z
    status:
      maturity: draft
      implementation: implemented
  - _ulid: 01KH4HA4303B95N2GJDWJTD4GZ
    slugs:
      - exit-node
    title: Exit Node
    type: feature
    tags: []
    description: |
      The only node that performs GPU computation. Receives the complete recipe
      tree, validates it, executes the full batched GPU pipeline, and returns
      a ComfyUI MODEL with merged weights as set patches. Handles IS_CHANGED
      for LoRA file monitoring and reports progress.
    acceptance_criteria:
      - id: ac-1
        given: a valid recipe tree ending in RecipeMerge
        when: Exit node executes
        then: it returns a ComfyUI MODEL (ModelPatcher clone) with set patches
      - id: ac-2
        given: an invalid recipe tree with type mismatches
        when: Exit node validates
        then: it raises ValueError naming the invalid type and its position in the tree
      - id: ac-3
        given: a recipe tree with compose target containing multiple branches
        when: Exit evaluates the merge step
        then: it calls merge_weights for simultaneous parameter routing
      - id: ac-4
        given: a recipe tree with single LoRA target
        when: Exit evaluates the merge step
        then: it calls filter_delta for importance filtering
      - id: ac-5
        given: a chain where RecipeMerge base is another RecipeMerge
        when: Exit evaluates
        then: inner merge evaluates first and its result becomes the base for outer merge
      - id: ac-6
        given: a RecipeCompose with a single branch
        when: Exit evaluates the merge step
        then: it treats it as filter_delta not merge_weights (single-branch passthrough)
      - id: ac-7
        given: the Exit node output MODEL
        when: a downstream ComfyUI LoRA node applies additional patches
        then: the additional LoRA patches apply additively on top of the set patches
      - id: ac-8
        given: the base ModelPatcher uses bf16 weights
        when: set patches are installed
        then: patch tensors match the base model storage dtype
      - id: ac-9
        given: the exit node processes multiple batch groups
        when: each batch group completes
        then: progress is reported via ComfyUI ProgressBar with value incrementing and max equal to total
          groups
    depends_on: []
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-10T19:44:03.168Z
    requirements:
      - _ulid: 01KH4HA434GERCYP2X4V28802Z
        slugs:
          - exit-recipe-analysis
        title: Exit Recipe Analysis
        type: requirement
        tags: []
        acceptance_criteria:
          - id: ac-1
            given: a recipe tree
            when: Exit walks to the root
            then: it finds the RecipeBase and extracts model_patcher and arch tag
          - id: ac-2
            given: multiple RecipeLoRA nodes in the tree
            when: synthetic set IDs are assigned
            then: |
              each unique RecipeLoRA group gets a distinct set ID and two LoRAs
              chained via prev share the same set ID
          - id: ac-3
            given: a recipe with LoRA references
            when: LoRA files are loaded
            then: the architecture-appropriate loader is selected based on the arch tag
          - id: ac-4
            given: loaded LoRA files
            when: the affected-key map is built
            then: each set ID maps to the set of base model parameter keys that set modifies
          - id: ac-5
            given: keys not affected by any LoRA set
            when: the executor processes keys
            then: those keys are skipped entirely with no work performed
          - id: ac-6
            given: a recipe referencing a LoRA file that does not exist
            when: Exit loads LoRAs
            then: it raises FileNotFoundError naming the missing file and which LoRA node referenced it
        depends_on: []
        implements: []
        relates_to: []
        tests: []
        traits: []
        notes: []
        created: 2026-02-10T19:44:03.172Z
        status:
          maturity: draft
          implementation: implemented
      - _ulid: 01KH4HA438KQPGVN0FZHHDFSYF
        slugs:
          - exit-batched-eval
        title: Exit Batched Evaluation
        type: requirement
        tags: []
        acceptance_criteria:
          - id: ac-1
            given: a recipe tree with a compose target
            when: batched evaluation runs
            then: merge_weights_batched is called with all branch results and the backbone
          - id: ac-2
            given: a recipe tree with a single LoRA target
            when: batched evaluation runs
            then: filter_delta_batched is called with the applied LoRA delta and backbone
          - id: ac-3
            given: a chain of RecipeMerge nodes
            when: evaluation recurses
            then: inner merges evaluate first and results become the base for outer merges
          - id: ac-4
            given: the WIDEN algorithm produces results
            when: they are returned from evaluation
            then: all result tensors are on GPU (transferred to CPU in patch installation phase)
          - id: ac-5
            given: a RecipeMerge with an explicit backbone reference that differs from the base
            when: batched evaluation runs the WIDEN step
            then: the backbone model (not the base) is used as the importance reference for WIDEN analysis
        depends_on: []
        implements: []
        relates_to: []
        tests: []
        traits: []
        notes: []
        created: 2026-02-10T19:44:03.177Z
        status:
          maturity: draft
          implementation: implemented
      - _ulid: 01KH4HA43E6W8V2Z0EFAWE0TQ5
        slugs:
          - exit-patch-install
        title: Exit Patch Installation
        type: requirement
        tags: []
        acceptance_criteria:
          - id: ac-1
            given: merged tensors from batched evaluation
            when: patch installation runs
            then: original ModelPatcher is cloned via clone() and merged weights added as set patches
          - id: ac-2
            given: set patches to install
            when: add_patches is called
            then: |
              each key is prefixed with diffusion_model. to match ModelPatcher namespace
          - id: ac-3
            given: merged tensors
            when: stored as patches
            then: all tensors are on CPU (transferred from GPU during this phase)
          - id: ac-4
            given: the base model uses bf16 storage dtype
            when: set patch tensors are created
            then: they match the base model storage dtype
          - id: ac-5
            given: IS_CHANGED is called twice with no LoRA file changes
            when: hashes are compared
            then: they are identical enabling a cache hit
          - id: ac-6
            given: IS_CHANGED is called and a LoRA file has been modified
            when: the hash is computed
            then: it differs from the previous call triggering a cache miss
          - id: ac-7
            given: an exit node executing after a previous generation left set patches on a loaded clone
            when: patch installation begins
            then: loaded clones sharing the base model are fully unloaded first so model_state_dict() returns
              original unpatched weights
        depends_on: []
        implements: []
        relates_to: []
        tests: []
        traits: []
        notes:
          - _ulid: 01KH7QYW4E5MNYR0Y9GTK6H583
            created_at: 2026-02-12T01:37:57.902Z
            author: "@claude"
            content: "Implementation note: install_merged_patches uses (\"set\", (tensor,)) tuple format to
              match ComfyUI convention for add_patches. ac-1 'added as set patches' is realized via
              this specific tuple structure."
            supersedes: null
        created: 2026-02-10T19:44:03.182Z
        status:
          maturity: draft
          implementation: implemented
    status:
      maturity: draft
      implementation: implemented
  - _ulid: 01KH4HA43J7SSD59Y6GR7BKN1G
    slugs:
      - widen-core
    title: WIDEN Core Algorithm
    type: feature
    tags: []
    description: |
      Port of the WIDEN algorithm from merge-router src/core/. Includes
      filter_delta (single-model importance filtering), merge_weights
      (multi-model parameter routing), ranking mechanisms, divergence
      metrics, and batched variants for GPU-vectorized operation.
      Pure algorithm code with no ComfyUI imports.
    acceptance_criteria:
      - id: ac-1
        given: base and delta tensors
        when: filter_delta is called with a t_factor
        then: importance-filtered delta is returned with low-importance parameters zeroed
      - id: ac-2
        given: multiple model tensors and a backbone
        when: merge_weights is called
        then: each parameter is routed to the most-important contributor via calibrated softmax
      - id: ac-3
        given: batched inputs of shape [B, *param_shape]
        when: filter_delta_batched or merge_weights_batched is called
        then: results match per-key variants applied individually
      - id: ac-4
        given: lib/widen.py
        when: imported
        then: no ComfyUI modules are imported (pure torch and stdlib)
      - id: ac-5
        given: the WIDEN implementation
        when: compared against merge-router src/core/widen.py
        then: algorithm behavior is equivalent for identical inputs within float tolerance
      - id: ac-6
        given: bf16 or fp16 input tensors
        when: WIDEN computation runs
        then: internal computation uses fp32 for numerical stability
      - id: ac-7
        given: no advanced configuration
        when: WIDEN is initialized
        then: |
          it uses default WIDENConfig values with ranking_strategy=percentile,
          sparsity_method=softmax, s_calibration=1.0
      - id: ac-8
        given: a non-OOM error during filter_delta_batched
        when: the error is caught
        then: unfiltered delta is used as passthrough and a warning is logged
      - id: ac-9
        given: a non-OOM error during merge_weights_batched
        when: the error is caught
        then: simple averaging is used as fallback and a warning is logged
    depends_on: []
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-10T19:44:03.186Z
    status:
      maturity: draft
      implementation: implemented
  - _ulid: 01KH4HA43RMA4PGDRWR8FFRMND
    slugs:
      - batched-executor
    title: Batched Pipeline Executor
    type: feature
    tags: []
    description: OpSignature-based parameter grouping and batched GPU evaluation. Groups parameters by
      (shape, ndim) for maximum batch utilization. Per-set LoRA filtering is handled by the loader
      at evaluation time, not at the grouping stage. Computes optimal batch sizes based on free
      VRAM, applies LoRAs via torch.bmm, and handles OOM backoff. Ported from merge-router
      lora_chain_merge.py.
    acceptance_criteria:
      - id: ac-1
        given: a set of parameter keys with varying shapes
        when: grouped by OpSignature
        then: keys with identical shape are in the same group; per-set LoRA filtering is handled by the
          loader at evaluation time
      - id: ac-2
        given: a batch of parameters and LoRA DeltaSpecs
        when: bmm LoRA apply runs
        then: torch.bmm produces correct deltas matching per-key application
      - id: ac-3
        given: available VRAM and parameter shapes
        when: compute_batch_size is called
        then: it returns a batch size targeting 70 percent of free VRAM
      - id: ac-4
        given: a torch.cuda.OutOfMemoryError during batch evaluation
        when: OOM backoff triggers
        then: the failed chunk retries at batch size 1 while other chunks continue normally
      - id: ac-5
        given: the executor completes evaluation
        when: merged tensors are produced
        then: all result tensors are on CPU ready for set patch installation
      - id: ac-6
        given: a base model with bf16 storage dtype
        when: batched evaluation produces merged results
        then: output tensors match the base model storage dtype
      - id: ac-7
        given: a LoRA with LoKr weights
        when: batched apply runs
        then: LoKr weights use per-key torch.kron on GPU instead of bmm
    depends_on: []
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-10T19:44:03.192Z
    status:
      maturity: draft
      implementation: implemented
  - _ulid: 01KH4HA43YH91JRARAHVJE40AZ
    slugs:
      - lora-loaders
    title: Architecture-Specific LoRA Loaders
    type: feature
    tags: []
    description: |
      LoRA loading with architecture-specific key mapping. Each architecture
      has distinct key naming conventions and special handling requirements.
      Loaders produce DeltaSpec objects for the batched executor pipeline.
    acceptance_criteria:
      - id: ac-1
        given: a LoRA file and a detected architecture tag
        when: the appropriate loader is selected
        then: the correct architecture-specific loader handles key mapping
      - id: ac-2
        given: any architecture loader
        when: it processes a LoRA file
        then: it produces DeltaSpec objects compatible with the batched executor
      - id: ac-3
        given: a new architecture needs LoRA support
        when: a loader module is added to lib/lora/
        then: it integrates without modifying existing loaders (pluggable design)
      - id: ac-4
        given: any loader
        when: it implements the loader interface
        then: |
          it provides load(path) for loading, affected_keys property for the
          key set, get_delta_specs(keys) returning DeltaSpecs, and cleanup()
          for resource release
    depends_on: []
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-10T19:44:03.198Z
    requirements:
      - _ulid: 01KH4HA445WEG2RT7V1XGK45D3
        slugs:
          - sdxl-loader
        title: SDXL LoRA Loader
        type: requirement
        tags: []
        acceptance_criteria:
          - id: ac-1
            given: an SDXL LoRA safetensors file
            when: loaded with the SDXL loader
            then: |
              LoRA keys are mapped to diffusion_model input_blocks, middle_block,
              and output_blocks keys
          - id: ac-2
            given: SDXL LoRA factors (up, down, alpha)
            when: DeltaSpecs are produced
            then: each spec contains correct rank, kind, and factor tensors
          - id: ac-3
            given: an SDXL LoRA with attention keys containing attn patterns
            when: key mapping runs
            then: attention keys including proj_in, proj_out, and to_q/to_k/to_v are correctly mapped to their
              base model counterparts
        depends_on: []
        implements: []
        relates_to: []
        tests: []
        traits: []
        notes: []
        created: 2026-02-10T19:44:03.205Z
        status:
          maturity: draft
          implementation: implemented
      - _ulid: 01KH4HA44C16BTA406DSC3V4HZ
        slugs:
          - zimage-loader
        title: Z-Image LoRA Loader
        type: requirement
        tags: []
        acceptance_criteria:
          - id: ac-1
            given: a Z-Image LoRA file with separate to_q, to_k, to_v keys
            when: loaded with Z-Image loader
            then: QKV keys are fused into the base model attention.qkv.weight layout
          - id: ac-2
            given: a Z-Image LoRA with Diffusers-style key names
            when: key mapping runs
            then: keys are correctly mapped to S3-DiT parameter names
          - id: ac-3
            given: Z-Image LoRA factors
            when: DeltaSpecs are produced
            then: |
              QKV-fused specs have correct offset indexing for the fused weight
              where q occupies 0 to 3840, k occupies 3840 to 7680, and v occupies
              7680 to 11520
        depends_on: []
        implements: []
        relates_to: []
        tests: []
        traits: []
        notes: []
        created: 2026-02-10T19:44:03.212Z
        status:
          maturity: draft
          implementation: implemented
    status:
      maturity: draft
      implementation: implemented
  - _ulid: 01KH4HA44KN101QT0AHV6JVW5J
    slugs:
      - memory-management
    title: Memory Management
    type: feature
    tags: []
    description: |
      GPU memory lifecycle during and after merge execution. Covers per-chunk
      tensor cleanup, between-group GC cycles, loader resource teardown, and
      ensuring all final patches are CPU-only.
    acceptance_criteria:
      - id: ac-1
        given: batched evaluation processes a chunk of parameters
        when: the chunk completes and results transfer to CPU
        then: all GPU tensors for that chunk are deleted and freed
      - id: ac-2
        given: an OpSignature group completes all chunks
        when: transitioning to the next group
        then: gc.collect() and torch.cuda.empty_cache() are called once after all groups complete, not
          between individual groups (OOM backoff handles per-group memory pressure)
      - id: ac-3
        given: all LoRA files have been loaded and evaluation is complete
        when: cleanup runs
        then: all loader resources are freed including delta caches and file handles
      - id: ac-4
        given: the complete merge execution
        when: final merged patches are produced
        then: all patch tensors are on CPU with no GPU tensor references remaining
      - id: ac-5
        given: peak GPU usage during a chunk
        when: compared to compute_batch_size estimate
        then: actual usage does not exceed the estimate by more than 20 percent
    depends_on: []
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-10T19:44:03.219Z
    status:
      maturity: draft
      implementation: implemented
  - _ulid: 01KH4HA44TE1N5S7YYJJZK082G
    slugs:
      - per-block-control
    title: Per-Block Control
    type: feature
    tags: []
    description: |
      BLOCK_CONFIG custom type enabling per-block t_factor and LoRA strength
      overrides. Architecture-specific config nodes expose block group sliders.
      Uses Option B design with explicit config type wired to consuming nodes.
    acceptance_criteria:
      - id: ac-1
        given: no BLOCK_CONFIG inputs connected to any node
        when: the workflow executes
        then: all nodes behave identically to pre-block-control behavior
      - id: ac-2
        given: architecture-specific block config nodes exist
        when: a user creates a block config for their model architecture
        then: block group sliders are available with float range 0.0 to 2.0
      - id: ac-3
        given: a single BLOCK_CONFIG output
        when: connected to multiple consuming nodes
        then: it fans out correctly to each consumer
      - id: ac-4
        given: SDXL architecture
        when: block config node is used
        then: each of the 19 individual blocks (IN00-IN08, MID, OUT00-OUT08) has its own slider
      - id: ac-5
        given: Z-Image architecture
        when: block config node is used
        then: each of the 34 individual blocks (L00-L29, NOISE_REF0, NOISE_REF1, CTX_REF0, CTX_REF1) has its
          own slider
    depends_on: []
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-10T19:44:03.226Z
    requirements:
      - _ulid: 01KH4HA4529G5BGA25WXD1VJWQ
        slugs:
          - block-config-type
        title: Block Config Type
        type: requirement
        tags: []
        acceptance_criteria:
          - id: ac-1
            given: a BlockConfig dataclass
            when: constructed with arch and block_overrides
            then: it is frozen and stores per-block float values as a tuple of pairs
          - id: ac-2
            given: RecipeLoRA and RecipeMerge dataclasses
            when: block_config field is present
            then: it accepts BlockConfig or None
        depends_on: []
        implements: []
        relates_to: []
        tests: []
        traits: []
        notes: []
        created: 2026-02-10T19:44:03.234Z
        status:
          maturity: draft
          implementation: implemented
      - _ulid: 01KH4HA45A5T7Y0CDAT62T796Y
        slugs:
          - merge-block-config
        title: Merge Per-Block T-Factor
        type: requirement
        tags: []
        acceptance_criteria:
          - id: ac-1
            given: BLOCK_CONFIG connected to Merge node
            when: per-block t_factor overrides are applied
            then: block classification returns individual block names (e.g. IN00, IN01, L05, NOISE_REF0), not
              grouped ranges
          - id: ac-2
            given: no BLOCK_CONFIG connected to Merge
            when: Exit evaluates
            then: global t_factor applies to all blocks (backwards compatible)
        depends_on: []
        implements: []
        relates_to: []
        tests: []
        traits: []
        notes: []
        created: 2026-02-10T19:44:03.242Z
        status:
          maturity: draft
          implementation: implemented
      - _ulid: 01KH4HA45N4GVEC484HN367MX4
        slugs:
          - lora-block-config
        title: LoRA Per-Block Strength
        type: requirement
        tags: []
        acceptance_criteria:
          - id: ac-1
            given: BLOCK_CONFIG connected to LoRA node
            when: per-block strength scaling is applied to LoRA deltas
            then: block classification returns individual block names (e.g. IN00, IN01, L05, NOISE_REF0), not
              grouped ranges
          - id: ac-2
            given: no BLOCK_CONFIG connected to LoRA node
            when: Exit applies LoRA deltas
            then: global strength applies uniformly (backwards compatible)
        depends_on: []
        implements: []
        relates_to: []
        tests: []
        traits: []
        notes: []
        created: 2026-02-10T19:44:03.253Z
        status:
          maturity: draft
          implementation: implemented
      - _ulid: 01KHA77MCVSX367KN2Y8WMHXXK
        slugs:
          - layer-type-filter
        title: Layer-Type Filtering
        type: requirement
        tags: []
        description: Classify parameter keys by layer type (attention, feed_forward, norm) and apply
          multiplicative strength/t_factor overrides per layer type. Wires up the existing
          BlockConfig.layer_type_overrides field.
        depends_on: []
        implements: []
        relates_to: []
        tests: []
        traits: []
        notes: []
        created: 2026-02-13T00:43:22.395Z
        acceptance_criteria:
          - id: ac-1
            given: a parameter key from SDXL or Z-Image
            when: classify_layer_type is called
            then: "it returns one of: attention, feed_forward, norm, or None"
          - id: ac-2
            given: a block config with layer_type_overrides
            when: LoRA strength is applied per-block
            then: the effective strength is block_strength * layer_type_strength (multiplicative)
          - id: ac-3
            given: a block config with layer_type_overrides
            when: WIDEN t_factor is applied per-block
            then: the effective t_factor is block_t_factor * layer_type_multiplier (multiplicative)
          - id: ac-4
            given: a block config with empty layer_type_overrides (default)
            when: per-block processing runs
            then: behavior is identical to before (backwards compatible)
          - id: ac-5
            given: a block config node for SDXL or Z-Image
            when: rendered in ComfyUI
            then: it includes attention, feed_forward, and norm sliders (FLOAT 0.0-2.0, default 1.0) after the
              block sliders
          - id: ac-6
            given: a parameter key that matches no layer type pattern (e.g., time_embed, label_emb,
              adaLN_modulation, embedders)
            when: classify_layer_type is called
            then: it returns None, and effective strength/t_factor uses block-only value with no layer-type
              modification
          - id: ac-7
            given: a parameter key that could match multiple layer type patterns (e.g., q_norm matches both
              attention and norm)
            when: classify_layer_type is called
            then: "the first-match-wins rule applies with precedence order: attention > feed_forward > norm"
          - id: ac-8
            given: arch=None or an unsupported architecture
            when: classify_layer_type is called
            then: it returns None (no error)
    status:
      maturity: draft
      implementation: implemented
  - _ulid: 01KHCJ41CGXX0V1FTCF9J0H3YK
    slugs:
      - full-model-recipe
    title: Full Model Recipe Type
    type: feature
    tags: []
    description: |
      Frozen recipe dataclass representing a full model checkpoint to merge.
      Stores a file path and strength (like RecipeLoRA stores LoRA paths),
      not a ComfyUI MODEL reference. This enables deferred disk-based loading
      at Exit time via safetensors streaming. Follows all recipe conventions:
      frozen, tuples not lists, no GPU tensors.
    acceptance_criteria:
      - id: ac-1
        given: a RecipeModel instance
        when: a field is assigned after construction
        then: a FrozenInstanceError is raised
      - id: ac-2
        given: a RecipeModel
        when: inspecting its fields
        then: |
          it has path (str), strength (float, default 1.0), and
          block_config (BlockConfig or None, default None)
      - id: ac-3
        given: a RecipeModel instance
        when: passed to RecipeCompose.with_branch()
        then: a new RecipeCompose is returned containing it as a branch
      - id: ac-4
        given: a RecipeMerge constructed with target=RecipeModel
        when: the tree is inspected
        then: construction succeeds and target is the RecipeModel
      - id: ac-5
        given: the RecipeNode type alias
        when: inspected
        then: RecipeModel is included in the union
      - id: ac-6
        given: a RecipeModel
        when: inspected for GPU tensors
        then: no torch.Tensor objects are found (path and strength only)
    depends_on: []
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-13T22:32:07.824Z
  - _ulid: 01KHCJ41D55KYE8VE38DEBQD3K
    slugs:
      - model-input-node
    title: Model Input Node
    type: feature
    tags: []
    description: |
      ComfyUI node that produces a RecipeModel from a checkpoint file picker.
      Loads from disk out-of-band from ComfyUI -- the model is NOT loaded at
      node execution time. Like the LoRA node, this is pure recipe building
      with zero GPU work and zero file I/O. The file path is stored in the
      recipe and resolved at Exit time for deferred streaming access.
    acceptance_criteria:
      - id: ac-1
        given: the node's INPUT_TYPES
        when: inspected
        then: |
          it has model_name (checkpoint file combo via folder_paths) and
          strength (FLOAT, default 1.0, range 0.0-2.0)
      - id: ac-2
        given: the node executes with a valid checkpoint name
        when: output is inspected
        then: it returns a RecipeModel with the filename and strength stored
      - id: ac-3
        given: the node executes
        when: checking GPU memory and disk I/O
        then: no GPU memory is allocated and no file is opened (deferred to Exit)
      - id: ac-4
        given: the node class
        when: inspecting CATEGORY
        then: it is ecaj/merge
      - id: ac-5
        given: the node's RETURN_TYPES
        when: inspected
        then: it returns WIDEN type (compatible with Compose and Merge inputs)
      - id: ac-6
        given: an optional BLOCK_CONFIG input
        when: connected
        then: the BlockConfig is stored in RecipeModel.block_config
    depends_on:
      - "@full-model-recipe"
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-13T22:32:07.845Z
  - _ulid: 01KHCJ41DMYNDJH45JQQ0FAQ3K
    slugs:
      - full-model-loader
    title: Full Model Loader
    type: feature
    tags: []
    description: |
      Streaming model loader using safetensors.safe_open() for memory-efficient
      per-batch access to full checkpoint weights. Matches the LoRALoader
      interface pattern but provides full weight tensors instead of low-rank
      factors. Handles key normalization between checkpoint file format and
      base model state dict format. Architecture-specific key mapping for
      SDXL and Z-Image. Only supports safetensors format (non-safetensors
      checkpoints raise a clear error).
    acceptance_criteria:
      - id: ac-1
        given: a safetensors checkpoint path
        when: the loader opens it
        then: |
          it uses safe_open() for memory-mapped access without loading the
          full file into memory
      - id: ac-2
        given: a list of base model parameter keys
        when: get_weights(keys) is called
        then: |
          it returns the corresponding weight tensors from the checkpoint
          file, correctly mapped from file key format to base model key format
      - id: ac-3
        given: an SDXL checkpoint file with model.diffusion_model prefix
        when: key mapping runs
        then: |
          file keys are normalized to match base model state dict keys
          (e.g., model.diffusion_model.input_blocks.0 maps to input_blocks.0)
      - id: ac-4
        given: a Z-Image checkpoint file
        when: key mapping runs
        then: |
          file keys are normalized to match base model state dict keys,
          handling the diffusion_model or transformer prefix variants
      - id: ac-5
        given: the loader
        when: affected_keys is accessed
        then: |
          it returns the set of base model keys that have corresponding
          diffusion model weights in the checkpoint file, excluding
          VAE and text encoder keys
      - id: ac-6
        given: the loader is no longer needed
        when: cleanup() is called
        then: the safe_open file handle is closed and resources freed
      - id: ac-7
        given: a checkpoint file with keys that don't match the base model
        when: the mismatch is detected
        then: a clear error is raised listing unmatched keys
      - id: ac-8
        given: the loader
        when: detecting architecture from file keys
        then: |
          it can determine architecture without loading any tensor data
          by inspecting normalized keys against architecture patterns
      - id: ac-9
        given: a non-safetensors checkpoint file (e.g., .ckpt, .pt)
        when: the loader attempts to open it
        then: |
          a clear error is raised explaining that only safetensors
          format is supported for model merging
    depends_on: []
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-13T22:32:07.860Z
  - _ulid: 01KHCJ41E230H6BM4NNWYW0MJ5
    slugs:
      - full-model-execution
    title: Full Model Execution
    type: feature
    tags: []
    description: |
      Exit node extension for executing WIDEN merge on full model checkpoints.
      Adds OpApplyModel operation to the recipe evaluation engine. During
      recipe analysis, detects RecipeModel nodes and opens streaming loaders.
      During batched evaluation, OpApplyModel loads model weights per-batch
      into registers, then existing OpFilterDelta/OpMergeWeights apply WIDEN
      importance routing unchanged. Validates architecture consistency between
      base model and merge models. Adding RecipeModel as a 5th recipe type
      requires updating all isinstance dispatch points across the codebase.
    acceptance_criteria:
      - id: ac-1
        given: a recipe tree containing RecipeModel nodes
        when: recipe analysis runs (lib/analysis.py)
        then: |
          it detects RecipeModel nodes, opens FullModelLoader instances for
          each unique path, and builds affected-key maps per model
      - id: ac-2
        given: a RecipeModel in a recipe tree
        when: compile_plan processes it
        then: an OpApplyModel op is emitted referencing the model's loader ID
      - id: ac-3
        given: execute_plan encounters OpApplyModel
        when: executing a batch of keys
        then: |
          it loads the model weight tensors for those keys from the streaming
          loader into a register (the raw weights, no arithmetic)
      - id: ac-4
        given: OpApplyModel result is in a register
        when: OpFilterDelta or OpMergeWeights uses it with a backbone register
        then: |
          WIDEN computes delta (model_weights - backbone) internally and
          routes by importance -- existing algorithm, unchanged
      - id: ac-5
        given: a recipe that mixes RecipeModel and RecipeLoRA nodes
        when: the exit node processes it
        then: |
          both paths execute correctly -- LoRA via existing DeltaSpec path,
          models via OpApplyModel streaming path
      - id: ac-6
        given: a checkpoint file whose detected architecture
        when: it differs from the base model architecture
        then: a clear error is raised naming both architectures and both file paths
      - id: ac-7
        given: full model weights loaded per-batch via streaming
        when: GPU evaluation completes for a batch
        then: |
          loaded weights are freed after use, not held resident
          (streaming loader re-reads from disk as needed)
      - id: ac-8
        given: GPU runs out of memory during full model evaluation
        when: OOM is caught
        then: |
          existing chunked_evaluation backoff retries at batch_size=1
          (compatible with streaming loader -- just re-reads fewer keys)
      - id: ac-9
        given: a RecipeModel with block_config
        when: per-block control is applied during execution
        then: |
          block-level strength scaling is applied to full model deltas
          the same way it applies to LoRA deltas
      - id: ac-10
        given: the checkpoint file does not exist or is not a valid safetensors file
        when: the exit node attempts to open it
        then: |
          a clear error is raised naming the missing file and which
          Model Input node referenced it
      - id: ac-11
        given: IS_CHANGED is called
        when: the recipe tree contains RecipeModel nodes
        then: |
          checkpoint file (mtime, size) is included in the hash alongside
          any LoRA file hashes
      - id: ac-12
        given: a recipe with only RecipeModel targets (no LoRAs)
        when: affected keys are computed
        then: |
          all diffusion model keys present in both base and merge model
          are processed (not just LoRA-affected subset)
      - id: ac-13
        given: a recipe composing 3 full models for merge
        when: execution runs
        then: |
          only one batch of model weights per loader is on GPU at a time
          (streaming loaders are read sequentially, not all at once)
    depends_on:
      - "@full-model-recipe"
      - "@model-input-node"
      - "@full-model-loader"
    implements: []
    relates_to: []
    tests: []
    traits: []
    notes: []
    created: 2026-02-13T22:32:07.874Z
