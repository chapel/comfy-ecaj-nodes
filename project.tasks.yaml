- _ulid: 01KH4DWQ6BNTGB4PE6PXKS1K24
  slugs:
    - port-design-doc
  title: Port design doc into kspec specs and tasks
  type: task
  description: Translate the ComfyUI node pack design document
    (~/Projects/merge-router/docs/comfyui-node-pack-design.md) into kspec spec items (modules,
    features, requirements with acceptance criteria) and derive implementation tasks. This should be
    done as a design session with review checkpoints — not all at once.
  spec_ref: null
  meta_ref: null
  plan_ref: null
  status: completed
  blocked_by: []
  closed_reason: null
  depends_on: []
  context: []
  priority: 1
  tags:
    - setup
    - design
  vcs_refs: []
  created_at: 2026-02-10T18:44:18.251Z
  started_at: 2026-02-10T19:47:05.721Z
  completed_at: 2026-02-10T21:40:56.673Z
  notes:
    - _ulid: 01KH4DZDER1NSMEDTPRYEWBNDB
      created_at: 2026-02-10T18:45:46.584Z
      author: "@claude"
      content: "Source document: ~/Projects/merge-router/docs/comfyui-node-pack-design.md (627+ lines, 8
        sections). Sections 6.8 (Exit Node Algorithm), 6.9 (Memory Strategy), and 6.10 (Per-Block
        Control) were written this session — they are complete but newer than the rest of the doc."
      supersedes: null
    - _ulid: 01KH4DZTRTJSRZ7CT7QCZT534V
      created_at: 2026-02-10T18:46:00.218Z
      author: "@claude"
      content: "Key design decisions already resolved (in §6.7 and §6.10): (1) Auto-detect architecture at
        Entry time from state dict keys. (2) Explicit backbone input on Merge node, defaults to
        base. (3) LoRA file selection via folder_paths.get_filename_list standard dropdown. (4) Exit
        node produces MODEL, saved via built-in ComfyUI save nodes. (5) Per-block control uses
        Option B — explicit BLOCK_CONFIG type wired into Merge (block_t_factor) and LoRA
        (block_strength) inputs. (6) One generic Block Config node per architecture (not two),
        semantics carried by input names on consumers. (7) Deferred execution — recipe nodes do zero
        GPU work, Exit runs full batched pipeline."
      supersedes: null
    - _ulid: 01KH4E058E4DY6405MKGBJHW08
      created_at: 2026-02-10T18:46:10.959Z
      author: "@claude"
      content: "Suggested module structure for specs: (1) nodes module — Entry, LoRA, Compose, Merge, Exit
        node specs with ACs for each. (2) core-algorithm module — WIDEN, divergence, ranking,
        numerical config (ported from merge-router). (3) executor module — batched pipeline,
        OpSignature grouping, GPU LoRA apply, OOM backoff. (4) lora-loading module —
        architecture-specific loaders (SDXL, Z-Image, Qwen), key mapping, QKV fusing. (5)
        block-config module — per-block control, architecture-specific Block Config nodes,
        BLOCK_CONFIG type."
      supersedes: null
    - _ulid: 01KH4E0DSSVM3M9YBBYY5G1207
      created_at: 2026-02-10T18:46:19.705Z
      author: "@claude"
      content: "User preference: this should be done as a design session with review checkpoints — present
        spec decomposition incrementally for approval, not all at once. Use /spec-plan workflow.
        Stub node files and recipe dataclasses already exist in the project (nodes/*.py,
        lib/recipe.py) which can inform AC writing."
      supersedes: null
    - _ulid: 01KH4E0P5Q1HW5AYXSWE0EQPWH
      created_at: 2026-02-10T18:46:28.279Z
      author: "@claude"
      content: "What NOT to port as specs: merge.py JSON config runner, CLI argument parsing, safetensors
        save logic, training infrastructure, Nu scripts. These are replaced by the ComfyUI node
        graph. The design doc §5 has the full port/no-port list."
      supersedes: null
    - _ulid: 01KH4E5KFNP2RN8144NKCXXWTM
      created_at: 2026-02-10T18:49:09.366Z
      author: "@claude"
      content: Design doc moved to docs/design.md in this project (copied from
        ~/Projects/merge-router/docs/comfyui-node-pack-design.md). References to merge-router source
        files now use full paths. Project structure section updated to comfy-ecaj-nodes/.
      supersedes: null
  todos: []
- _ulid: 01KH4H1VQFJC261BXN6377A7XS
  slugs:
    - implement-comfyui-packaging
  title: Implement ComfyUI Packaging
  type: task
  spec_ref: "@comfyui-packaging"
  plan_ref: "@plan-foundation-project-infrastructure"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #1. Implemented ComfyUI packaging infrastructure: NODE_CLASS_MAPPINGS
    with 5 WIDEN nodes, pyproject.toml with [tool.comfy] metadata (PublisherId=ecaj,
    DisplayName=ECAJ Nodes), lowercase ecaj/merge CATEGORY on all nodes, empty runtime deps with dev
    deps in optional-dependencies. All 4 ACs covered by tests in test_packaging.py."
  depends_on: []
  context: []
  priority: 1
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:39:32.463Z
  started_at: 2026-02-11T01:44:24.977Z
  completed_at: 2026-02-11T02:25:41.782Z
  notes:
    - _ulid: 01KH4H1VQF30CM22CSZ4A997WD
      created_at: 2026-02-10T19:39:32.463Z
      author: "@claude"
      content: |
        Implementation notes:

        Skeleton exists in __init__.py (5 WIDEN mappings) and pyproject.toml.
        Validate NODE_CLASS_MAPPINGS pattern works with ComfyUI node discovery.
        CATEGORY must use lowercase ecaj/ prefix with sub-categories per capability
        area (e.g., ecaj/merge for WIDEN nodes). Existing stubs use ECAJ/merge --
        update all to lowercase ecaj/merge. For the task: review ComfyUI's
        nodes_core.py loading path, verify pyproject.toml [tool.comfy] fields match
        registry requirements (PublisherId, DisplayName, Icon), ensure requirements.txt
        lists only dependencies ComfyUI doesn't already provide (torch and safetensors
        are provided by ComfyUI -- check if we need them at all), update CATEGORY on
        all 5 existing node stubs from ECAJ/merge to ecaj/merge.
        Files: __init__.py, pyproject.toml, requirements.txt, nodes/entry.py,
        nodes/lora.py, nodes/compose.py, nodes/merge.py, nodes/exit.py.
    - _ulid: 01KH55Y8QK84TTHF6Y531RSKMR
      created_at: 2026-02-11T01:44:34.803Z
      author: "@claude"
      content: "Fixed CATEGORY casing ECAJ/merge -> ecaj/merge in all 5 node stubs. Updated
        pyproject.toml: empty runtime deps, dev deps (pytest/ruff/torch), pytest config with
        importlib mode, ruff config (line-length=99, py310, isort+pyflakes+pyupgrade). Made
        requirements.txt comment-only. Added setuptools package discovery for flat layout. Made
        __init__.py safe to import outside ComfyUI (try/except for relative imports). Added root
        conftest.py with collect_ignore."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4H1VQK0HMMA7NP2BTSFZEA
  slugs:
    - implement-testing-infrastructure
  title: Implement Testing Infrastructure
  type: task
  spec_ref: "@testing-infrastructure"
  plan_ref: "@plan-foundation-project-infrastructure"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #1. Implemented testing infrastructure with: MockModelPatcher (4x4
    float32 tensors, SDXL-like keys,
    model_state_dict/clone/add_patches/get_key_patches/patches_uuid), recipe fixtures (recipe_base,
    recipe_single_lora, recipe_multi_lora, recipe_compose, recipe_chain), autouse ComfyUI API mocks
    (folder_paths, comfy modules). All 37 tests pass without ComfyUI. All 4 ACs verified with test
    coverage."
  depends_on: []
  context: []
  priority: 1
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:39:32.467Z
  started_at: 2026-02-11T01:44:45.085Z
  completed_at: 2026-02-11T02:26:48.995Z
  notes:
    - _ulid: 01KH4H1VQK71QYFPXSF52PYAVJ
      created_at: 2026-02-10T19:39:32.467Z
      author: "@claude"
      content: |
        Implementation notes:

        Create tests/conftest.py with: (1) MockModelPatcher class -- needs
        model_state_dict(filter_prefix) returning a dict of fake tensors keyed like
        diffusion_model.input_blocks.0.0.weight, clone() returning a new
        MockModelPatcher, add_patches(patches, strength_patch, strength_model)
        storing patches, get_key_patches(filter_prefix) returning patch data,
        patches_uuid property. Use small tensors (e.g., 4x4 float32) for speed.
        (2) Recipe tree fixtures -- recipe_base() returning RecipeBase with
        MockModelPatcher and arch=sdxl, recipe_single_lora(), recipe_lora_set()
        (2 LoRAs chained), recipe_compose() (2 branches), recipe_chain() (2
        sequential merges), recipe_full() (compose + chain like hyphoria example
        from design doc section 6.5). (3) Mock folder_paths module --
        get_filename_list(loras) returning [test.safetensors]. (4) pytest config
        in pyproject.toml [tool.pytest.ini_options] with testpaths=[tests].
        Files: tests/conftest.py, pyproject.toml (pytest config section).
    - _ulid: 01KH54VWW1HZ2WMCP921EZKDSD
      created_at: 2026-02-11T01:25:48.545Z
      author: "@claude"
      content: "Dependencies cleared (was: @implement-comfyui-mocking-and-fixtures,
        @implement-node-graph-testing, @implement-ci-pipeline)"
      supersedes: null
    - _ulid: 01KH55YYAYCTEQ1X0ZX0RZQP3G
      created_at: 2026-02-11T01:44:56.926Z
      author: "@claude"
      content: "Created tests/conftest.py with: MockModelPatcher (4x4 float32 tensors, SDXL-like keys,
        model_state_dict/clone/add_patches/get_key_patches/patches_uuid), recipe fixtures
        (mock_model_patcher, recipe_base, recipe_single_lora, recipe_multi_lora, recipe_compose,
        recipe_chain), autouse ComfyUI API mocks (folder_paths, comfy, comfy.utils,
        comfy.model_management). Created 3 test files: test_packaging.py (5 smoke tests for node
        attributes/CATEGORY), test_recipe.py (13 tests for frozen/tuple/structure),
        test_mock_model_patcher.py (15 tests for mock fidelity). All 33 tests pass without ComfyUI,
        ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA460E5KKNBG2WJJK17GX
  slugs:
    - implement-recipe-type-system
  title: Implement Recipe Type System
  type: task
  spec_ref: "@recipe-system"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #3. Implemented recipe type system with frozen dataclasses, tuple
    fields, RecipeCompose.with_branch() for persistent tree semantics, RecipeNode type alias, and
    __all__ exports. All 5 ACs have test coverage: AC-1 (frozen), AC-2 (persistent semantics), AC-3
    (no GPU tensors), AC-4 (importable/constructible), AC-5 (WIDEN wire connections)."
  depends_on: []
  context: []
  priority: 1
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.264Z
  started_at: 2026-02-11T02:28:43.363Z
  completed_at: 2026-02-11T02:32:59.881Z
  notes:
    - _ulid: 01KH4HA46066MMF0GVC5BDX526
      created_at: 2026-02-10T19:44:03.264Z
      author: "@claude"
      content: |
        Implementation notes:

        Partially implemented in lib/recipe.py -- has all 4 dataclasses but
        missing BlockConfig (added later in per-block-control). The WIDEN custom
        type is registered implicitly by ComfyUI when a node declares
        RETURN_TYPES = ("WIDEN",) -- no explicit registration needed, but verify
        this works by checking that ComfyUI type system allows connections between
        nodes sharing the custom type name. AC-5 can be tested by constructing a
        mock workflow JSON with WIDEN connections and validating against ComfyUI
        graph validation, or by testing in a running ComfyUI instance. For the
        task: verify existing dataclasses match design doc section 6.6, ensure all
        fields use tuples (not lists), verify frozen=True on all, add __all__
        export list. Consider adding RecipeNode = Union[RecipeBase, RecipeLoRA,
        RecipeCompose, RecipeMerge] type alias for type checking.
        Files: lib/recipe.py.
    - _ulid: 01KH58K0JPCQZHDM7Z3ZB6P5M0
      created_at: 2026-02-11T02:30:51.734Z
      author: "@claude"
      content: "Completed Recipe Type System: added RecipeCompose.with_branch() for persistent tree
        semantics (AC-2), added RecipeNode type alias, added __all__ exports. Added 18 new tests: 4
        for AC-2 (with_branch returns new instance, original unchanged), 4 for AC-3 (no GPU tensors
        in recipe objects), 3 for AC-4 (all classes importable/constructible), 7 for AC-5 (WIDEN
        wire connections between nodes). All 58 tests pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA463BT1D6RF0ZD1DRW2P
  slugs:
    - implement-entry-node
  title: Implement Entry Node
  type: task
  spec_ref: "@entry-node"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #5. Implemented WIDEN Entry Node with architecture detection (SDXL,
    Z-Image supported; Flux/Qwen detected but unsupported). All 5 ACs covered by 17 tests. No GPU
    memory allocation or tensor copying."
  depends_on:
    - "@implement-testing-infrastructure"
    - "@implement-comfyui-packaging"
  context: []
  priority: 2
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.267Z
  started_at: 2026-02-11T02:44:49.090Z
  completed_at: 2026-02-11T02:49:00.440Z
  notes:
    - _ulid: 01KH4HA463N9QDME4T6H5MEV84
      created_at: 2026-02-10T19:44:03.267Z
      author: "@claude"
      content: |
        Implementation notes:

        Stub in nodes/entry.py. Architecture detection function should live in
        lib/arch_detect.py (or inline if small) -- inspect model.model_state_dict()
        keys for patterns: SDXL has input_blocks, middle_block, output_blocks;
        Z-Image has layers.N. plus noise_refiner; Flux has double_blocks; Qwen
        has transformer_blocks at depth 60. Check patterns in order of specificity
        (Z-Image before generic layers). For unsupported arch error, include first
        5 state dict key prefixes in the error message for debugging. ModelPatcher
        is stored as-is in RecipeBase (reference only, no clone, no tensor ops).
        Test by constructing MockModelPatcher instances with different key patterns
        and asserting detected arch.
        Files: nodes/entry.py, optionally lib/arch_detect.py.
    - _ulid: 01KH4J2TE18PEKWAPSGFSQG9N8
      created_at: 2026-02-10T19:57:32.482Z
      author: "@claude"
      content: "NF-1 scope clarification: v1 detects SDXL and Z-Image as supported architectures. Flux
        (double_blocks) and Qwen (transformer_blocks) should be detected and produce a clear
        UnsupportedArchitectureError with a message like 'Detected Flux architecture but no WIDEN
        loader is available yet. Supported: sdxl, zimage.' This way the detection code is
        future-proof but the error path is explicit."
      supersedes: null
    - _ulid: 01KH59FM8G8R54CKHG8BKP0BYQ
      created_at: 2026-02-11T02:46:29.393Z
      author: "@claude"
      content: "Implemented Entry Node with architecture detection. Created detect_architecture() function
        with pattern matching for SDXL (input_blocks/middle_block/output_blocks), Z-Image (layers +
        noise_refiner), Flux (double_blocks), and Qwen (60+ transformer_blocks). Flux/Qwen are
        detected but raise UnsupportedArchitectureError. Added 17 tests covering all 5 ACs: AC-1
        (returns RecipeBase), AC-2 (SDXL detection), AC-3 (Z-Image detection), AC-4 (no GPU/tensor
        copy), AC-5 (unsupported arch errors). All 107 tests pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA46729EXG3GT8QCW2F44
  slugs:
    - implement-lora-node
  title: Implement LoRA Node
  type: task
  spec_ref: "@lora-node"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #6. Implemented LoRA node with folder_paths dropdown, RecipeLoRA output
    with path/strength tuples, prev chaining for LoRA sets, and zero-strength preservation. All 5
    ACs have test coverage (12 tests total)."
  depends_on:
    - "@implement-testing-infrastructure"
    - "@implement-comfyui-packaging"
  context: []
  priority: 2
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.271Z
  started_at: 2026-02-11T02:49:57.336Z
  completed_at: 2026-02-11T02:54:03.844Z
  notes:
    - _ulid: 01KH4HA467XDAB7ACFJRB2MXSN
      created_at: 2026-02-10T19:44:03.271Z
      author: "@claude"
      content: |
        Implementation notes:

        Stub in nodes/lora.py. The lora_name input must change from STRING type
        to use folder_paths.get_filename_list(loras) -- the ComfyUI pattern is:
        import folder_paths then lora_name: (folder_paths.get_filename_list(loras),)
        as a combo input. Full file path resolved at Exit time via
        folder_paths.get_full_path(loras, lora_name). The prev input receives a
        RecipeLoRA from a previous LoRA node -- extract its .loras tuple and
        concatenate: new_loras = prev.loras + (dict(path=lora_name, strength=strength),).
        If prev is None, create single-element tuple. For the task: update
        INPUT_TYPES to use folder_paths combo, implement add_lora() method,
        handle prev chaining with tuple concatenation. Edge case: prev might
        be None (optional input).
        Files: nodes/lora.py.
    - _ulid: 01KH59SQ5H65P189F8PK584N2V
      created_at: 2026-02-11T02:52:00.050Z
      author: "@claude"
      content: "Implemented LoRA node with folder_paths dropdown (deferred import for testability).
        add_lora() returns RecipeLoRA with proper prev chaining for LoRA sets. Added 12 tests
        covering all 5 ACs: AC-1 (returns RecipeLoRA with path/strength), AC-2 (chaining via prev),
        AC-3 (folder_paths dropdown), AC-4 (no prev = single-element), AC-5 (zero strength
        preserved). Updated conftest.py mock for folder_paths.get_filename_list. All 119 tests pass,
        ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA46AAJBFFJ5JAVM9EM2H
  slugs:
    - implement-compose-node
  title: Implement Compose Node
  type: task
  spec_ref: "@compose-node"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #7. Implemented Compose node that accumulates branches for WIDEN
    merging. Uses RecipeCompose.with_branch() for persistent semantics. Validates branch types
    (accepts RecipeLoRA, RecipeCompose, RecipeMerge; rejects raw RecipeBase with helpful error). All
    4 ACs covered by 17 tests."
  depends_on:
    - "@implement-testing-infrastructure"
    - "@implement-comfyui-packaging"
  context: []
  priority: 2
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.274Z
  started_at: 2026-02-11T02:54:56.746Z
  completed_at: 2026-02-11T02:58:53.235Z
  notes:
    - _ulid: 01KH4HA46B8WQXTZ2DPN615WVW
      created_at: 2026-02-10T19:44:03.275Z
      author: "@claude"
      content: |
        Implementation notes:

        Stub in nodes/compose.py. Implementation: if compose is provided and is a
        RecipeCompose, extract its .branches tuple and append the new branch:
        RecipeCompose(branches=compose.branches + (branch,)). If compose is None,
        create RecipeCompose(branches=(branch,)). For AC-4 validation: check
        isinstance(branch, RecipeBase) and raise ValueError with message about
        needing to apply LoRAs first or use as a Merge base input. Valid branch
        types: RecipeLoRA, RecipeCompose, RecipeMerge. Also validate that compose
        input when provided is a RecipeCompose (not some other recipe type).
        Files: nodes/compose.py.
    - _ulid: 01KH5A2DYSRP2EWMDA3H13K7NC
      created_at: 2026-02-11T02:56:45.530Z
      author: "@claude"
      content: "Implemented Compose node with branch validation and chaining. compose() uses
        RecipeCompose.with_branch() for persistent semantics. Added 17 tests covering all 4 ACs:
        AC-1 (single branch returns single-element tuple), AC-2 (appends to existing compose), AC-3
        (three chained nodes in order), AC-4 (RecipeBase rejected with helpful error). Also
        validates compose input is RecipeCompose. All 136 tests pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA46DNS18CPRTTECH915J
  slugs:
    - implement-merge-node
  title: Implement Merge Node
  type: task
  spec_ref: "@merge-node"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #8. Implemented Merge node with base/target type validation. merge()
    validates base is RecipeBase or RecipeMerge (rejects RecipeLoRA/RecipeCompose with helpful error
    message suggesting Entry node or Merge output). All 6 ACs covered by 23 tests: AC-1 (RecipeMerge
    with fields), AC-2 (no backbone defaults None), AC-3 (backbone stored), AC-4 (merge chaining),
    AC-5 (invalid base rejection), AC-6 (t_factor -1.0 preserved). All tests pass, ruff clean."
  depends_on:
    - "@implement-testing-infrastructure"
    - "@implement-comfyui-packaging"
  context: []
  priority: 2
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.277Z
  started_at: 2026-02-11T02:59:47.138Z
  completed_at: 2026-02-11T03:04:40.091Z
  notes:
    - _ulid: 01KH4HA46D87PGPQBEVJMMND53
      created_at: 2026-02-10T19:44:03.277Z
      author: "@claude"
      content: |
        Implementation notes:

        Stub in nodes/merge.py. Implementation: validate base is RecipeBase or
        RecipeMerge (raise ValueError with specific message if not). Validate
        target is RecipeLoRA, RecipeCompose, or RecipeMerge (not RecipeBase).
        Then construct RecipeMerge(base=base, target=target, backbone=backbone,
        t_factor=t_factor). The t_factor slider range is -1.0 to 5.0, step 0.05
        (already correct in stub). -1.0 means passthrough -- interpreted by Exit
        node, not Merge node. Backbone defaults to None when not connected
        (optional input). For the task: add isinstance validation checks at top
        of merge(), construct and return RecipeMerge.
        Files: nodes/merge.py.
    - _ulid: 01KH5ABPDGFY83D0SQ5XDGRMG2
      created_at: 2026-02-11T03:01:49.105Z
      author: "@claude"
      content: "Implemented Merge node with base/target type validation. merge() validates base is
        RecipeBase or RecipeMerge (rejects RecipeLoRA/RecipeCompose with helpful error), accepts
        RecipeLoRA/RecipeCompose/RecipeMerge as target. All 6 ACs covered by 23 tests: AC-1 (returns
        RecipeMerge with fields), AC-2 (no backbone defaults None), AC-3 (backbone stored when
        provided), AC-4 (merge chaining via base), AC-5 (RecipeLoRA/RecipeCompose rejected as base),
        AC-6 (t_factor -1.0 preserved). All 159 tests pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA46HJ6V0VC67XRPWSGB5
  slugs:
    - implement-exit-node
  title: Implement Exit Node
  type: task
  spec_ref: "@exit-node"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #14. Implemented WIDENExitNode.execute() orchestrating the complete
    recipe tree evaluation with _validate_recipe_tree() for tree structure validation. All 8 ACs
    covered: returns MODEL with set patches (AC-1), validates tree with position-aware errors
    (AC-2), compose targets call merge_weights (AC-3), single LoRA targets call filter_delta (AC-4),
    chained merges evaluate inner first (AC-5), single-branch compose uses filter_delta (AC-6),
    downstream LoRA patches apply additively (AC-7), patch tensors match base model dtype (AC-8). 22
    tests added in test_exit_node.py, 291 total tests passing."
  depends_on:
    - "@implement-exit-recipe-analysis"
    - "@implement-exit-batched-evaluation"
    - "@implement-exit-patch-installation"
    - "@implement-comfyui-packaging"
  context: []
  priority: 2
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.281Z
  started_at: 2026-02-11T03:43:27.316Z
  completed_at: 2026-02-11T03:52:01.713Z
  notes:
    - _ulid: 01KH4HA46HK4H347XYD1D7VY9S
      created_at: 2026-02-10T19:44:03.281Z
      author: "@claude"
      content: |
        Implementation notes:

        Stub in nodes/exit.py. This is the most complex node -- it orchestrates
        everything. The execute() method: (1) Validate recipe tree structure by
        walking recursively and checking types at each node. (2) Call batched
        executor from lib/executor.py which handles phases 1-3. (3) Install
        results as set patches on a ModelPatcher clone. The IS_CHANGED classmethod
        must walk recipe tree to find all RecipeLoRA nodes, resolve file paths via
        folder_paths.get_full_path(loras, name), and return hash of (mtime, size)
        tuples. Use os.path.getmtime() and os.path.getsize(). If any file missing,
        return float(NaN) to force re-execution. Progress reporting via
        comfy.utils.ProgressBar(total_steps) -- get total from executor. Downstream
        LoRA compat: set patches work because ComfyUI calculate_weight() processes
        patches in list order -- set replaces first, then subsequent LoRA patches
        add on top. Depends on lib/executor.py, lib/recipe.py, all node implementations.
        Files: nodes/exit.py.
    - _ulid: 01KH4J2YGZ6J2K44BEG1QRPB5N
      created_at: 2026-02-10T19:57:36.671Z
      author: "@claude"
      content: "NF-3 scope clarification: This task implements the WIDENExitNode class shell in
        nodes/exit.py ONLY — INPUT_TYPES, RETURN_TYPES, the execute() method that validates the
        recipe tree then delegates to lib/executor.py, the IS_CHANGED classmethod, and progress bar
        setup. The actual executor internals (recipe analysis, batched evaluation, patch
        installation) are implemented in separate sub-requirement tasks that write to
        lib/executor.py. This task wires the node to the executor, it does not implement the
        executor."
      supersedes: null
    - _ulid: 01KH5D0YA4NYKD3JJKJ37R3DJG
      created_at: 2026-02-11T03:48:22.468Z
      author: "@claude"
      content: "Implemented WIDENExitNode.execute() in nodes/exit.py. Added _validate_recipe_tree() for
        AC-2 (validates tree structure, raises ValueError with position on type mismatches).
        execute() orchestrates: (1) analyze_recipe for LoRA loading and set_affected map (AC-1), (2)
        compile_batch_groups for OpSignature grouping, (3) chunked_evaluation with evaluate_recipe
        as eval_fn for batched GPU evaluation (AC-3,4,5), (4) install_merged_patches for set patch
        installation (AC-7,8). Updated lib/executor.py evaluate_recipe to handle single-branch
        RecipeCompose as filter_delta not merge_weights (AC-6). Added 22 tests in
        tests/test_exit_node.py covering all 8 ACs. All 291 tests pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA46PY9P8S7PZ0D32WZDD
  slugs:
    - implement-exit-recipe-analysis
  title: Implement Exit Recipe Analysis
  type: task
  spec_ref: "@exit-recipe-analysis"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #12. Implemented exit recipe analysis in lib/analysis.py with all 6 ACs
    covered: tree walk to RecipeBase (AC-1), object identity-based set ID assignment (AC-2),
    architecture loader selection (AC-3), affected-key map tracking (AC-4), key filtering for
    processing (AC-5), and FileNotFoundError with context (AC-6). 22 tests verify all acceptance
    criteria."
  depends_on:
    - "@implement-architecture-specific-lora-loaders"
  context: []
  priority: 2
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.286Z
  started_at: 2026-02-11T03:27:52.930Z
  completed_at: 2026-02-11T03:35:17.741Z
  notes:
    - _ulid: 01KH4HA46PWH5STPRD10BPGH8X
      created_at: 2026-02-10T19:44:03.286Z
      author: "@claude"
      content: |
        Implementation notes:

        This phase happens at start of execute() in nodes/exit.py or in
        lib/executor.py entry point. Tree walk: recursive function following
        RecipeMerge.base links until hitting RecipeBase. Collect all RecipeLoRA
        nodes by walking .target and .base recursively. Set ID assignment:
        identity-based -- with frozen dataclasses, chained LoRAs produce a single
        RecipeLoRA with a multi-element tuple, so each unique RecipeLoRA instance
        equals one set. LoRA loading: select loader from lib/lora/{arch}.py based
        on RecipeBase.arch. Resolve file paths with folder_paths.get_full_path(loras,
        name). Build affected-key map by calling loader.affected_keys for each set.
        The tree walk and set ID assignment is the most critical piece -- get the
        identity semantics right.
        Files: lib/executor.py, references lib/lora/base.py interface.
    - _ulid: 01KH5C3XXW78DMHNTE0VFWZ7TC
      created_at: 2026-02-11T03:32:31.805Z
      author: "@claude"
      content: Implemented exit recipe analysis in lib/analysis.py with pure torch/stdlib (no ComfyUI
        imports). Created AnalysisResult dataclass with model_patcher, arch, set_affected map,
        loader, and affected_keys. Implemented _walk_to_base() for AC-1, _collect_lora_sets() with
        object identity-based set IDs for AC-2, architecture loader selection via get_loader() for
        AC-3, affected-key map tracking during load for AC-4, get_keys_to_process() for AC-5, and
        FileNotFoundError with context for AC-6. Added 22 tests covering all 6 ACs. All 258 tests
        pass, ruff clean.
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA46X4V9RHXW26VP36FD9
  slugs:
    - implement-exit-batched-evaluation
  title: Implement Exit Batched Evaluation
  type: task
  spec_ref: "@exit-batched-eval"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #13. Implemented evaluate_recipe() tree walker in lib/executor.py with
    full AC coverage: RecipeCompose→merge_weights_batched (AC-1), RecipeLoRA→filter_delta_batched
    (AC-2), chained RecipeMerge→recursive evaluation (AC-3), results stay on GPU (AC-4), backbone
    override passed to WIDEN (AC-5). 13 tests added covering all 5 ACs."
  depends_on:
    - "@implement-batched-pipeline-executor"
    - "@implement-exit-recipe-analysis"
  context: []
  priority: 2
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.293Z
  started_at: 2026-02-11T03:36:17.464Z
  completed_at: 2026-02-11T03:42:23.167Z
  notes:
    - _ulid: 01KH4HA46X9JT6J2TFQA337BTM
      created_at: 2026-02-10T19:44:03.293Z
      author: "@claude"
      content: |
        Implementation notes:

        This is the inner evaluation loop in lib/executor.py. For each OpSignature
        group, for each chunk of B keys: stack base tensors to GPU, walk recipe
        tree recursively. Tree walker dispatches on recipe node type: RecipeMerge
        with RecipeCompose target -> evaluate each branch then call
        widen.merge_weights_batched() (or filter_delta_batched if single branch).
        RecipeMerge with RecipeLoRA target -> call _apply_lora_set_batched_gpu()
        to get delta then widen.filter_delta_batched(). Chain: if RecipeMerge.base
        is another RecipeMerge, recurse on inner merge first. Backbone: use
        RecipeMerge.backbone if not None, else use base tensor. Port recursive
        evaluation from merge-router evaluate_node_batched() (~lines 850-950 of
        scripts/lora_chain_merge.py), adapting from config dict traversal to
        recipe dataclass traversal.
        Files: lib/executor.py.
    - _ulid: 01KH4J3H45X4PGJCNH8W986KB6
      created_at: 2026-02-10T19:57:55.717Z
      author: "@claude"
      content: "NF-6 boundary clarification: This task implements the RECIPE TREE WALKER in
        lib/executor.py — the evaluate_node_batched() function (~line 850 in merge-router
        scripts/lora_chain_merge.py) adapted for recipe dataclasses. It dispatches to primitives
        from @implement-batched-pipeline-executor (OpSignature grouping,
        _apply_lora_set_batched_gpu, compute_batch_size) and WIDEN functions from
        @implement-widen-core-algorithm (filter_delta_batched, merge_weights_batched). This task is
        the glue between recipe traversal and the batched primitives. Also handles backbone
        override: when RecipeMerge.backbone is not None, pass it as the importance reference to
        WIDEN functions instead of the base tensor."
      supersedes: null
    - _ulid: 01KH5CGSAYSTMJKAFNCFXSR4DH
      created_at: 2026-02-11T03:39:33.086Z
      author: "@claude"
      content: "Implemented evaluate_recipe() tree walker in lib/executor.py. Function dispatches on
        recipe node type: RecipeCompose→merge_weights_batched (AC-1),
        RecipeLoRA→filter_delta_batched (AC-2), chained RecipeMerge→recursive evaluation (AC-3).
        Results stay on GPU (AC-4). Backbone override passed to WIDEN functions when
        RecipeMerge.backbone is not None (AC-5). Added 13 tests covering all 5 ACs with
        MockLoRALoader and MockWIDEN fixtures. All 269 tests pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA4733VECXNH5H4M4AA2B
  slugs:
    - implement-exit-patch-installation
  title: Implement Exit Patch Installation
  type: task
  spec_ref: "@exit-patch-install"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #9. Implemented install_merged_patches() helper with ModelPatcher
    cloning, diffusion_model. key prefixing, CPU transfer, and base dtype matching. Added IS_CHANGED
    classmethod using SHA-256 hash of LoRA file (path, mtime, size) tuples for cache invalidation.
    All 6 acceptance criteria covered by 26 tests."
  depends_on:
    - "@implement-testing-infrastructure"
  context: []
  priority: 2
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.299Z
  started_at: 2026-02-11T03:05:39.107Z
  completed_at: 2026-02-11T03:10:24.420Z
  notes:
    - _ulid: 01KH4HA473ZK9FSNE9X2012HTK
      created_at: 2026-02-10T19:44:03.299Z
      author: "@claude"
      content: |
        Implementation notes:

        After batched evaluation produces dict of {key: merged_tensor_on_gpu},
        transfer each to CPU with .cpu(), cast to base model storage dtype with
        .to(base_dtype). Get base dtype from first value in
        model_patcher.model_state_dict(). Clone model: merged = model_patcher.clone().
        Build patch dict: {f"diffusion_model.{k}": ("set", tensor) for k, tensor in
        merged_state.items()}. Install: merged.add_patches(patches, strength_patch=1.0).
        Note: the set patch format for add_patches is a tuple (strength, ("set", tensor),
        strength_model, None, None) -- check that add_patches handles the format or if
        raw tuple is needed. Verify against ComfyUI comfy/model_patcher.py add_patches
        and comfy/lora.py calculate_weight for exact format. IS_CHANGED: implement as
        @classmethod on WIDENExitNode -- receives same args as execute(). Walk recipe
        to collect all LoRA file paths, compute hashlib.sha256 of (path, mtime, size)
        tuples sorted by path. Return hex digest.
        Files: nodes/exit.py.
    - _ulid: 01KH5AQ1QPKBHNYK5HPDFVY9QX
      created_at: 2026-02-11T03:08:01.142Z
      author: "@claude"
      content: "Implemented install_merged_patches() helper and IS_CHANGED classmethod.
        install_merged_patches clones ModelPatcher, builds set patches with diffusion_model. prefix,
        transfers tensors to CPU and casts to base dtype. IS_CHANGED walks recipe tree to collect
        LoRA paths, computes SHA-256 hash from (path, mtime, size) tuples for cache key. All 6 ACs
        covered by 26 tests: AC-1 (clone and set patches), AC-2 (diffusion_model. prefix), AC-3 (CPU
        transfer), AC-4 (dtype matching), AC-5 (identical hash on no changes), AC-6 (different hash
        on modifications). All 185 tests pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA47BN0SVNJWSRGXBNJ2W
  slugs:
    - implement-widen-core-algorithm
  title: Implement WIDEN Core Algorithm
  type: task
  spec_ref: "@widen-core"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #4. Implemented WIDEN core algorithm port from merge-router with 5
    modules (numerical_config.py, sparsity.py, ranking.py, divergence.py, widen.py). All 9 ACs
    covered with 32 tests: filter_delta zeros low-importance (AC-1), merge_weights routes via
    softmax (AC-2), batched variants match per-key (AC-3), no ComfyUI imports (AC-4), deterministic
    behavior (AC-5), fp16/bf16 use fp32 (AC-6), default config values (AC-7), filter_delta_batched
    fallback (AC-8), merge_weights_batched fallback (AC-9)."
  depends_on: []
  context: []
  priority: 1
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.307Z
  started_at: 2026-02-11T02:33:53.360Z
  completed_at: 2026-02-11T02:43:54.707Z
  notes:
    - _ulid: 01KH4HA47BCY6NJ4SPY12G85HM
      created_at: 2026-02-10T19:44:03.307Z
      author: "@claude"
      content: |
        Implementation notes:

        Port from ~/Projects/merge-router/src/core/widen.py. Key classes/functions
        to port: WIDEN class with filter_delta(), merge_weights(),
        filter_delta_batched(), merge_weights_batched(), _disentangle(),
        _rank_importance(), _calibrate(). Also port WIDENConfig dataclass with
        fields: n_models, t_factor, s_calibration, ranking_strategy,
        sparsity_method, calibration_mode, dtype. Port supporting modules:
        lib/divergence.py from src/core/divergence.py (divergence metrics),
        lib/ranking.py from src/core/ranking.py (ranking mechanisms),
        lib/numerical_config.py from src/core/numerical_config.py (eps values
        per dtype). Strip: any CLI imports, config file parsing, logging setup
        (use stdlib logging). Keep: all torch operations, numerical stability
        handling (upcast to fp32 for computation, downcast result back), batched
        variants operating on [B, *shape] tensors. Fallback behavior (AC-8, AC-9)
        from merge-router lines ~905-928 in scripts/lora_chain_merge.py -- wrap
        batched WIDEN calls in try/except, on non-OOM error fall back to passthrough
        (filter_delta) or averaging (merge_weights), log warning via logging.warning().
        Test by creating small synthetic tensors (e.g., 8x8 float32) and verifying
        filter_delta zeros low-importance entries, merge_weights routes correctly.
        Compare against merge-router by running both on same input and checking allclose.
        Files: lib/widen.py, lib/divergence.py, lib/ranking.py, lib/numerical_config.py.
    - _ulid: 01KH4J2PAHPASF7QZ33EHV4RNX
      created_at: 2026-02-10T19:57:28.274Z
      author: "@claude"
      content: "NF-2 fix: Also port lib/sparsity.py from src/core/sparsity.py (sparsemax, entmax). This
        was listed in design doc S5 as required core algorithm but was missing from the files list.
        WIDENConfig.sparsity_method references it."
      supersedes: null
    - _ulid: 01KH594A77M5WRDPVMXZMD506W
      created_at: 2026-02-11T02:40:18.663Z
      author: "@claude"
      content: "Implemented WIDEN core algorithm port from merge-router. Created 5 modules:
        numerical_config.py (epsilon handling), sparsity.py (sparsemax/entmax), ranking.py
        (importance ranking), divergence.py (direction divergence), widen.py (main algorithm). Added
        32 tests covering all 9 ACs: filter_delta zeros low-importance (AC-1), merge_weights routes
        via softmax (AC-2), batched variants match per-key (AC-3), no ComfyUI imports (AC-4),
        deterministic behavior (AC-5), fp16/bf16 use fp32 (AC-6), default config values (AC-7),
        filter_delta_batched fallback (AC-8), merge_weights_batched fallback (AC-9). All 90 tests
        pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA47F493S53EWKW4EA9SF
  slugs:
    - implement-batched-pipeline-executor
  title: Implement Batched Pipeline Executor
  type: task
  spec_ref: "@batched-executor"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #10. Implemented batched pipeline executor primitives: OpSignature
    (parameter grouping), DeltaSpec (LoRA delta specs), compile_batch_groups (shape/affecting_sets
    grouping), compute_batch_size (70% VRAM targeting), apply_lora_batch_gpu (torch.bmm for standard
    LoRA, torch.kron for LoKr), and chunked_evaluation (OOM backoff wrapper). All 7 ACs covered by
    30 tests."
  depends_on:
    - "@implement-widen-core-algorithm"
    - "@implement-testing-infrastructure"
  context: []
  priority: 2
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.311Z
  started_at: 2026-02-11T03:11:31.166Z
  completed_at: 2026-02-11T03:18:03.688Z
  notes:
    - _ulid: 01KH4HA47F0N4G7WDBRY8CRMCT
      created_at: 2026-02-10T19:44:03.311Z
      author: "@claude"
      content: |
        Implementation notes:

        Port from ~/Projects/merge-router/scripts/lora_chain_merge.py. Key pieces:
        OpSignature frozen dataclass with affecting_sets (frozenset), shape (tuple),
        ndim (int). DeltaSpec dataclass with fields for LoRA factors (up, down,
        scale, alpha, kind, rank, key_index). compute_batch_size(shape, n_models,
        dtype, free_vram) formula: B = floor(free_vram * 0.7 / (numel(shape) *
        dtype_bytes * (3 + 3 * n_models))). _apply_lora_set_batched_gpu(base_batch,
        delta_specs, ...) -- partition specs by (kind, rank), stack up/down matrices,
        torch.bmm(down, up) for standard LoRA, torch.kron per-key for LoKr, scatter
        deltas back by key_index. OOM backoff: wrap chunk evaluation in try/except
        torch.cuda.OutOfMemoryError, on catch call torch.cuda.empty_cache() and retry
        with B=1. Define DeltaSpec in lib/types.py or lib/executor.py and import from
        lib/lora/ loaders. The executor is the main integration point -- it calls into
        lib/widen.py for WIDEN ops, lib/lora/*.py for LoRA loading, and walks the
        recipe tree from lib/recipe.py.
        Files: lib/executor.py, lib/types.py (for DeltaSpec/OpSignature if shared).
    - _ulid: 01KH4J3CTXKYTG9DN1GYM6W95A
      created_at: 2026-02-10T19:57:51.325Z
      author: "@claude"
      content: "NF-6 boundary clarification: This task implements the PRIMITIVES in lib/executor.py —
        OpSignature dataclass, DeltaSpec (in lib/types.py), compute_batch_size(),
        _apply_lora_set_batched_gpu() (LoRA apply via torch.bmm, LoKr via torch.kron), and OOM
        backoff wrapper. It does NOT implement the recipe tree walker — that is in
        @implement-exit-batched-evaluation. Key functions to port from scripts/lora_chain_merge.py:
        OpSignature (~line 50), DeltaSpec (~line 70), compute_batch_size() (~line 200),
        _apply_lora_set_batched_gpu() (~line 400). The evaluate_node_batched() tree walker (~line
        850) belongs to the exit-batched-eval task."
      supersedes: null
    - _ulid: 01KH5B4X0XS2GN6M3S54F4P0SM
      created_at: 2026-02-11T03:15:35.069Z
      author: "@claude"
      content: "Implemented batched pipeline executor primitives in lib/executor.py. Created OpSignature
        (frozen dataclass for parameter grouping by shape/affecting_sets), DeltaSpec (LoRA delta
        specification), compile_batch_groups (groups keys by OpSignature), compute_batch_size
        (targets 70% free VRAM), apply_lora_batch_gpu (torch.bmm for standard LoRA, torch.kron for
        LoKr), and chunked_evaluation (OOM backoff wrapper with CPU result transfer and dtype
        matching). All 7 ACs covered by 30 tests: AC-1 (grouping), AC-2 (bmm), AC-3 (70% VRAM), AC-4
        (OOM backoff), AC-5 (CPU results), AC-6 (dtype matching), AC-7 (LoKr kron). All 215 tests
        pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA47M05Q979790FR6MC9J
  slugs:
    - implement-architecture-specific-lora-loaders
  title: Implement Architecture-Specific LoRA Loaders
  type: task
  spec_ref: "@lora-loaders"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #11. Implemented architecture-specific LoRA loaders with pluggable
    registry design. Created LoRALoader ABC in lib/lora/base.py with load(), affected_keys,
    get_delta_specs(), cleanup() interface. Implemented SDXLLoader for kohya/A1111 format and
    ZImageLoader with QKV fusing support. All 4 ACs covered with 21 tests: AC-1 (architecture
    selection/key mapping), AC-2 (DeltaSpec production), AC-3 (pluggable design), AC-4 (interface
    contract)."
  depends_on:
    - "@implement-testing-infrastructure"
  context: []
  priority: 2
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.316Z
  started_at: 2026-02-11T03:18:49.921Z
  completed_at: 2026-02-11T03:26:45.215Z
  notes:
    - _ulid: 01KH4HA47M869P70TBAPP53GVA
      created_at: 2026-02-10T19:44:03.316Z
      author: "@claude"
      content: |
        Implementation notes:

        Define loader interface in lib/lora/base.py as an abstract base class or
        protocol: class LoRALoader(ABC) with @abstractmethod load(self, path, strength),
        @property affected_keys -> set[str], get_delta_specs(self, keys) -> list[DeltaSpec],
        cleanup(self). Each architecture implements in its own module. DeltaSpec
        dataclass (in lib/types.py or lib/executor.py) needs: key, key_index, kind
        (standard/lokr/qkv), rank, up (Tensor), down (Tensor), scale, alpha, offset
        (optional tuple for QKV). Loader selection: simple dict lookup in executor
        like {"sdxl": SDXLLoader, "zimage": ZImageLoader}. For AC-3 pluggable design:
        use registry pattern or just the dict -- adding new arch means adding one entry.
        Files: lib/lora/base.py, lib/lora/__init__.py (registry).
    - _ulid: 01KH5BKS82R40BARJP5B62RVM4
      created_at: 2026-02-11T03:23:42.722Z
      author: "@claude"
      content: "Implemented architecture-specific LoRA loaders with pluggable registry design. Created
        LoRALoader ABC in lib/lora/base.py with load(), affected_keys, get_delta_specs(), cleanup()
        interface. Implemented SDXLLoader for kohya/A1111 LoRA format with key mapping. Implemented
        ZImageLoader with QKV fusing support (to_q/to_k/to_v → fused qkv.weight). Created
        LOADER_REGISTRY in lib/lora/__init__.py for pluggable architecture selection. All 4 ACs
        covered by 21 tests: AC-1 (architecture selection/key mapping), AC-2 (DeltaSpec production),
        AC-3 (pluggable design), AC-4 (interface contract). All 236 tests pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA47ST02A5Y0G5ZABFP2D
  slugs:
    - implement-sdxl-lora-loader
  title: Implement SDXL LoRA Loader
  type: task
  spec_ref: "@sdxl-loader"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #15. Implemented SDXL LoRA Loader with greedy token matching for
    compound identifiers (input_blocks, proj_in, to_q, etc.). Added 21 tests covering all 3 ACs:
    block type mapping, DeltaSpec contents, and attention key mapping."
  depends_on:
    - "@implement-architecture-specific-lora-loaders"
  context: []
  priority: 2
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.321Z
  started_at: 2026-02-11T03:53:19.587Z
  completed_at: 2026-02-11T03:59:43.634Z
  notes:
    - _ulid: 01KH4HA47S0BBEWY8JBW0KCWSB
      created_at: 2026-02-10T19:44:03.321Z
      author: "@claude"
      content: |
        Implementation notes:

        Port from merge-router or implement fresh. SDXL LoRA key mapping: keys follow
        patterns like lora_unet_input_blocks_0_0_op.lora_down.weight -> base key
        input_blocks.0.0.weight. ComfyUI own comfy/lora.py has model_lora_keys_unet()
        that builds this mapping. Options: (1) Use ComfyUI key mapping function and
        wrap in our loader interface, (2) Implement standalone for consistency.
        Recommend option (1) for SDXL since ComfyUI handles all edge cases (attention,
        proj_in/out, time_embed). Load safetensors with safetensors.torch.load_file(),
        map keys, extract up/down/alpha per key, construct DeltaSpec objects. Standard
        LoRA: kind=standard, up=lora_up.weight, down=lora_down.weight, alpha from lora
        key or default to rank.
        Files: lib/lora/sdxl.py.
    - _ulid: 01KH5DHA80WYE26JZWQN4B3HVT
      created_at: 2026-02-11T03:57:18.976Z
      author: "@claude"
      content: "Fixed SDXL key parsing algorithm. The original implementation naively split all
        underscores into dots, breaking compound identifiers like input_blocks, transformer_blocks,
        proj_in, proj_out, to_q, to_k, to_v. Implemented _tokenize_lora_path() with greedy matching
        for 16 known compound tokens. Added 21 tests in test_sdxl_loader.py covering all 3 ACs: AC-1
        (block type mapping for input_blocks/middle_block/output_blocks), AC-2 (DeltaSpec contents:
        kind, up/down factors, rank, scale), AC-3 (attention key mapping: proj_in, proj_out,
        to_q/to_k/to_v/to_out). All 312 tests pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA47Z3DSGDJ07S2V7DNRD
  slugs:
    - implement-z-image-lora-loader
  title: Implement Z-Image LoRA Loader
  type: task
  spec_ref: "@zimage-loader"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #16. Implemented Z-Image LoRA Loader with QKV fusing - separate
    to_q/to_k/to_v keys fuse to attention.qkv.weight (AC-1), Diffusers key names map to S3-DiT
    parameters including LyCORIS format (AC-2), QKV DeltaSpecs have offset indexing q=(0,3840),
    k=(3840,3840), v=(7680,3840) (AC-3). 20 tests covering all 3 ACs."
  depends_on:
    - "@implement-architecture-specific-lora-loaders"
  context: []
  priority: 2
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.327Z
  started_at: 2026-02-11T04:01:28.519Z
  completed_at: 2026-02-11T04:07:39.216Z
  notes:
    - _ulid: 01KH4HA47ZYK7SM6X6WGWT9GZ8
      created_at: 2026-02-10T19:44:03.327Z
      author: "@claude"
      content: |
        Implementation notes:

        Port from ~/Projects/merge-router/scripts/zimage_lora_merge.py. Key function:
        _parse_lora_key(key) which maps Diffusers LoRA key names to S3-DiT base model
        keys and identifies QKV components. Z-Image base model uses fused
        attention.qkv.weight (11520x3840 = 3x3840) but LoRAs have separate
        to_q/to_k/to_v. The loader must: (1) Parse each LoRA key to identify
        target parameter and QKV component. (2) For QKV keys, create DeltaSpecs
        with kind=qkv and offset=(0, q_start, q_len) indicating which third of
        the fused weight this LoRA targets. The offset tuple is (dimension=0,
        start, length) where start is 0/3840/7680 for q/k/v respectively and
        length is 3840. (3) Handle non-QKV keys (FFN, norm, etc.) as standard
        LoRA. Also handle LoKr weights if present -- these have lokr_w1, lokr_w2
        instead of lora_up/lora_down, use kind=lokr. The Diffusers key mapping
        handles patterns like transformer_blocks.0.attn.to_q -> layers.0.attention.qkv
        (with offset for q portion).
        Files: lib/lora/zimage.py.
    - _ulid: 01KH5DZ7M8RZ57Y5JBMC9ZQPD4
      created_at: 2026-02-11T04:04:55.048Z
      author: "@claude"
      content: "Implemented Z-Image LoRA Loader with complete @zimage-loader AC coverage. Added
        DeltaSpec.offset field for QKV slice indexing (AC-3). Enhanced _parse_zimage_lora_key() to
        handle Diffusers key patterns including transformer./diffusion_model. prefixes, LyCORIS
        format with lycoris_ prefix, and to_out.0→out mapping (AC-2). QKV keys now fuse into single
        qkv.weight with proper offset tuples: q=(0,3840), k=(3840,3840), v=(7680,3840) (AC-1). Added
        _normalize_lycoris_key() for underscore→dot conversion preserving compound names. Created
        tests/test_zimage_loader.py with 20 tests covering all 3 ACs. All 332 tests pass, ruff
        clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA4865JG16KMN5YXENCYH
  slugs:
    - implement-memory-management
  title: Implement Memory Management
  type: task
  spec_ref: "@memory-management"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #17. Implemented GPU memory management with gc.collect() and
    torch.cuda.empty_cache() calls per-chunk and between OpSignature groups. Loader cleanup in
    finally block frees delta caches. All 5 ACs covered with 20 tests in test_memory_management.py."
  depends_on:
    - "@implement-exit-node"
  context: []
  priority: 2
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.334Z
  started_at: 2026-02-11T04:08:31.101Z
  completed_at: 2026-02-11T04:15:06.700Z
  notes:
    - _ulid: 01KH4HA486XCQMBVVXJ4R3TJHM
      created_at: 2026-02-10T19:44:03.334Z
      author: "@claude"
      content: |
        Implementation notes:

        Memory management is woven throughout lib/executor.py. Key patterns to port
        from merge-router scripts/lora_chain_merge.py: (1) Per-chunk cleanup: after
        transferring results to CPU, explicitly del base_batch and gpu intermediates
        then gc.collect() and torch.cuda.empty_cache(). (2) Per-group cleanup: between
        OpSignature groups, call gc.collect() + torch.cuda.empty_cache(). (3) Loader
        cleanup: after evaluation completes, call loader.cleanup() for each loader
        (which calls clear_delta_cache() and drops cached LoRA state). (4) Final
        cleanup: ensure returned merged_state dict contains only CPU tensors. The
        merge-router source has ~8 explicit gc.collect/empty_cache calls -- identify
        each and port the pattern. For AC-5 testing: use torch.cuda.max_memory_allocated()
        before/after chunk and compare to compute_batch_size prediction. This is a
        cross-cutting concern -- its ACs affect implementation in lib/executor.py
        primarily, also nodes/exit.py (loader cleanup after execute completes) and
        lib/lora/base.py (cleanup interface).
        Files: lib/executor.py (primary), nodes/exit.py (loader teardown),
        lib/lora/base.py (cleanup interface).
    - _ulid: 01KH4J4HP1J0TQVSKK15FKXETY
      created_at: 2026-02-10T19:58:29.057Z
      author: "@claude"
      content: "NF-7 restructured: This task is now a HARDENING PASS that runs AFTER exit-node integration
        works. It is no longer a prerequisite for exit-node. First get the pipeline working with
        basic cleanup, then this task adds comprehensive memory management:
        gc.collect()/torch.cuda.empty_cache() calls per-chunk and per-group, loader.cleanup()
        teardown, GPU tensor leak verification. This task reviews lib/executor.py and nodes/exit.py
        to add the ~8 cleanup points from merge-router's pattern."
      supersedes: null
    - _ulid: 01KH5EBDACFVETM2W1E5VQJE72
      created_at: 2026-02-11T04:11:34.092Z
      author: "@claude"
      content: Implemented GPU memory management with all 5 ACs covered. Added gc.collect() and
        torch.cuda.empty_cache() calls per-chunk in lib/executor.py:chunked_evaluation (AC-1) and
        between OpSignature groups in nodes/exit.py:execute (AC-2). Verified existing loader cleanup
        pattern frees delta caches (AC-3). Verified install_merged_patches ensures CPU-only patches
        with correct dtype (AC-4). compute_batch_size already provides conservative batch sizing
        within VRAM bounds (AC-5). Created tests/test_memory_management.py with 20 tests covering
        all ACs. 352 total tests pass, ruff clean.
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA48ERX1APFPSJS5E1Y7F
  slugs:
    - implement-per-block-control
  title: Implement Per-Block Control
  type: task
  spec_ref: "@per-block-control"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #19. Implemented per-block control with BLOCK_CONFIG custom ComfyUI
    type. Created WIDENBlockConfigSDXLNode (7 block group sliders: IN00-02, IN03-05, IN06-08, MID,
    OUT00-02, OUT03-05, OUT06-08) and WIDENBlockConfigZImageNode (8 block group sliders: L00-04
    through L25-29, noise_refiner, context_refiner). Each slider is FLOAT 0.0-2.0 with step 0.05.
    Added optional BLOCK_CONFIG input to WIDENLoRANode and WIDENMergeNode for fan-out support. All 3
    ACs verified: AC-1 (no block_config = pre-block-control behavior), AC-2 (architecture-specific
    sliders 0.0-2.0), AC-3 (fan-out to multiple consumers). 22 tests pass."
  depends_on:
    - "@implement-block-config-type"
    - "@implement-comfyui-packaging"
    - "@implement-testing-infrastructure"
  context: []
  priority: 3
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.342Z
  started_at: 2026-02-11T04:20:29.468Z
  completed_at: 2026-02-11T04:26:48.695Z
  notes:
    - _ulid: 01KH4HA48EJA9E7AFC4E38BA13
      created_at: 2026-02-10T19:44:03.342Z
      author: "@claude"
      content: |
        Implementation notes:

        Adds BLOCK_CONFIG custom ComfyUI type. Architecture-specific config nodes
        go in nodes/block_config_sdxl.py, nodes/block_config_zimage.py. Each
        exposes sliders for its architecture block groups: SDXL has input_blocks
        (groups of 3: IN00-02, IN03-05, IN06-08), middle_block, output_blocks
        (groups of 3). Z-Image has layers (groups of 5: L00-04, L05-09, ... L25-29),
        noise_refiner, context_refiner. Each slider FLOAT range 0.0-2.0, step 0.05.
        ComfyUI allows typing values outside slider range so -1.0 is accessible.
        The node produces a BlockConfig dataclass (in lib/recipe.py). Backwards
        compatibility: when block_config fields on RecipeMerge/RecipeLoRA are None,
        executor uses global t_factor/strength -- no special casing needed.
        Files: nodes/block_config_sdxl.py, nodes/block_config_zimage.py,
        lib/recipe.py (BlockConfig dataclass).
    - _ulid: 01KH5F1XF19NQV15D8DXRJG8V1
      created_at: 2026-02-11T04:23:51.522Z
      author: "@claude"
      content: "Implemented per-block control feature with BLOCK_CONFIG custom ComfyUI type. Created
        WIDENBlockConfigSDXLNode (7 block group sliders: IN00-02, IN03-05, IN06-08, MID, OUT00-02,
        OUT03-05, OUT06-08) and WIDENBlockConfigZImageNode (8 block group sliders: L00-04 through
        L25-29, noise_refiner, context_refiner). Each slider is FLOAT 0.0-2.0 step 0.05. Added
        BLOCK_CONFIG optional input to WIDENLoRANode and WIDENMergeNode for fan-out support. AC-1:
        when block_config is None, behavior identical to pre-block-control. AC-2:
        architecture-specific nodes expose sliders. AC-3: BLOCK_CONFIG fans out correctly (same
        instance shared). 22 tests in test_per_block_control.py covering all 3 ACs. 396 total tests
        pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA48PV4PF188MY0H239GA
  slugs:
    - implement-block-config-type
  title: Implement Block Config Type
  type: task
  spec_ref: "@block-config-type"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #18. Implemented BlockConfig frozen dataclass with arch,
    block_overrides (tuple of (pattern, float) pairs), and layer_type_overrides fields. Added
    block_config: object = None field to RecipeLoRA and RecipeMerge for backwards compatibility.
    AC-1 and AC-2 verified with 22 tests covering all acceptance criteria."
  depends_on:
    - "@implement-recipe-type-system"
  context: []
  priority: 3
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.350Z
  started_at: 2026-02-11T04:15:55.148Z
  completed_at: 2026-02-11T04:19:36.726Z
  notes:
    - _ulid: 01KH4HA48PDVTJC03SWE79A21X
      created_at: 2026-02-10T19:44:03.350Z
      author: "@claude"
      content: |
        Implementation notes:

        Add to lib/recipe.py: @dataclass(frozen=True) class BlockConfig with fields
        arch (str), block_overrides (tuple), layer_type_overrides (tuple). The
        block_overrides is a tuple of (block_pattern, value) pairs e.g.,
        (("IN00-02", 0.5), ("MID", 1.0), ...). The layer_type_overrides is a tuple
        of (layer_type, value) pairs for cross-cutting layer type control (attention,
        feed_forward, norm, etc.). Add block_config: object = None field to both
        RecipeLoRA and RecipeMerge -- since frozen, this means defining new versions
        with the additional field. Field defaults to None for backwards compat. The
        arch field must match RecipeBase.arch -- validated at Exit time.
        Files: lib/recipe.py.
    - _ulid: 01KH5EPMHV8C2DPANP9VNS60ND
      created_at: 2026-02-11T04:17:41.947Z
      author: "@claude"
      content: "Implemented BlockConfig frozen dataclass with arch, block_overrides (tuple of (pattern,
        float) pairs), and layer_type_overrides fields. Added block_config: object = None field to
        RecipeLoRA and RecipeMerge for backwards compatibility. AC-1: BlockConfig is frozen, stores
        arch and per-block float values as tuple of pairs. AC-2: RecipeLoRA and RecipeMerge accept
        BlockConfig or None. Created tests/test_block_config.py with 22 tests covering all ACs. 374
        total tests pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA48WYR6H4E0QV888VSD2
  slugs:
    - implement-merge-per-block-t-factor
  title: Implement Merge Per-Block T-Factor
  type: task
  spec_ref: "@merge-block-config"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #20. Implemented per-block t_factor support for merge operations with
    SDXL and Z-Image block classification. AC-1: BLOCK_CONFIG connected to Merge applies per-block
    t_factor overrides. AC-2: No BLOCK_CONFIG means global t_factor (backwards compatible). 35 tests
    in test_merge_block_config.py cover block classification and t_factor grouping."
  depends_on:
    - "@implement-per-block-control"
    - "@implement-merge-node"
    - "@implement-exit-node"
  context: []
  priority: 3
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.356Z
  started_at: 2026-02-11T04:27:40.314Z
  completed_at: 2026-02-11T04:38:33.183Z
  notes:
    - _ulid: 01KH4HA48WEV45JBSVZ5N8XDQ1
      created_at: 2026-02-10T19:44:03.356Z
      author: "@claude"
      content: |
        Implementation notes:

        Merge node gains optional block_t_factor input of type BLOCK_CONFIG in
        INPUT_TYPES. When present, stored in RecipeMerge.block_config. At Exit
        evaluation time, for each parameter key, executor: (1) classifies key into
        block group using architecture-specific patterns (e.g., for SDXL
        input_blocks.3. -> IN03-05), (2) looks up block group in
        BlockConfig.block_overrides, (3) uses override value as t_factor instead
        of global RecipeMerge.t_factor. If key block group not in overrides, falls
        back to global t_factor. Block classification function should live in
        lib/block_classify.py with one function per arch.
        Files: nodes/merge.py (add input), lib/executor.py (use block config
        during eval), lib/block_classify.py (key-to-block mapping).
    - _ulid: 01KH5FKM4X09R0C9SF93YTXJPZ
      created_at: 2026-02-11T04:33:31.805Z
      author: "@claude"
      content: "Implemented per-block t_factor support for merge operations. Created lib/block_classify.py
        with SDXL and Z-Image key classification (classify_key_sdxl, classify_key_zimage). Updated
        lib/executor.py with _get_block_t_factors helper that groups keys by their effective
        t_factor, and _apply_widen_filter_per_block/_apply_widen_merge_per_block functions that
        process each t_factor group with appropriate WIDEN instance. Updated nodes/exit.py to pass
        arch and widen_config to evaluate_recipe. AC-1: when block_config is present, per-block
        t_factor overrides are applied. AC-2: when block_config is None, global t_factor applies to
        all blocks. 35 tests in test_merge_block_config.py covering block classification and
        t_factor grouping. 431 total tests pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH4HA493AA51NFWN2NGQA6GA
  slugs:
    - implement-lora-per-block-strength
  title: Implement LoRA Per-Block Strength
  type: task
  spec_ref: "@lora-block-config"
  plan_ref: "@plan-widen-merge-feature-specs"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #21. Implemented LoRA per-block strength scaling:
    _apply_per_block_lora_strength helper in lib/executor.py scales LoRA deltas by BlockConfig
    overrides. AC-1: BLOCK_CONFIG with per-block strengths scales LoRA deltas per-key. AC-2: No
    block_config means global strength applies uniformly. 14 tests in test_lora_block_strength.py."
  depends_on:
    - "@implement-per-block-control"
    - "@implement-lora-node"
    - "@implement-exit-node"
  context: []
  priority: 3
  tags: []
  vcs_refs: []
  created_at: 2026-02-10T19:44:03.363Z
  started_at: 2026-02-11T04:39:41.971Z
  completed_at: 2026-02-11T04:45:26.430Z
  notes:
    - _ulid: 01KH4HA49389ZJ52761QNKPQ3X
      created_at: 2026-02-10T19:44:03.363Z
      author: "@claude"
      content: |
        Implementation notes:

        LoRA node gains optional block_strength input of type BLOCK_CONFIG in
        INPUT_TYPES. When present, stored in RecipeLoRA.block_config. At Exit time
        during batched LoRA apply phase (_apply_lora_set_batched_gpu), for each
        parameter key in batch: (1) classify key into block group, (2) look up
        override strength in BlockConfig, (3) multiply LoRA delta by per-block
        strength instead of global strength. This scales LoRA contribution before
        WIDEN sees it. During DeltaSpec processing, scale each spec effective
        strength by per-block override. Since batching groups keys by OpSignature
        (same shape + affecting sets), and per-block strength varies by key, need
        to apply scaling per-key within batch -- either as diagonal scaling matrix
        or by splitting batch by block group.
        Files: nodes/lora.py (add input), lib/executor.py (per-block scaling
        in LoRA apply).
    - _ulid: 01KH4J41ABBBKTGH8J7ZTBNFB0
      created_at: 2026-02-10T19:58:12.299Z
      author: "@claude"
      content: "NF-8 design decision resolved: Apply per-block strength as a per-key scaling vector within
        the batch. After computing the LoRA delta for the full batch via torch.bmm, multiply each
        key's delta by its per-block strength scalar. The scaling vector is constructed during
        recipe analysis by classifying each key to a block group via lib/block_classify.py and
        looking up the BlockConfig override. This avoids splitting the batch and preserves the bmm
        optimization. Implementation: after _apply_lora_set_batched_gpu() returns the delta batch
        [B, *shape], compute a strength vector [B, 1, 1] from per-key block classifications, then
        multiply: delta_batch *= strength_vector."
      supersedes: null
    - _ulid: 01KH5G40CSQ73M9YT2NH470Z17
      created_at: 2026-02-11T04:42:28.634Z
      author: "@claude"
      content: "Implemented per-block strength scaling for LoRA deltas. Created
        _apply_per_block_lora_strength helper in lib/executor.py that computes delta = lora_applied
        - base, scales each key's delta by its BlockConfig override, and returns base +
        scaled_delta. Modified _apply_lora_set in evaluate_recipe to call this helper when
        RecipeLoRA.block_config is present and arch is known. AC-1: BLOCK_CONFIG with per-block
        strengths scales LoRA deltas (strength 0.5 halves delta, strength 2.0 doubles it, strength
        0.0 removes the LoRA effect). AC-2: No block_config means global strength applies uniformly
        (helper not called). 14 tests in test_lora_block_strength.py covering strength scaling,
        different blocks, zero/amplified strengths, negative deltas, 4D conv2d shapes. 445 total
        tests pass, ruff clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH508VE1GEDSBP3VENYEF2R0
  slugs:
    - implement-comfyui-mocking-and-fixtures
  title: Implement ComfyUI Mocking and Fixtures
  type: task
  spec_ref: "@comfyui-mocking"
  plan_ref: "@plan-testing-strategy"
  status: pending_review
  blocked_by: []
  depends_on:
    - "@implement-testing-infrastructure"
    - "@implement-recipe-type-system"
  context: []
  priority: 3
  tags: []
  vcs_refs: []
  created_at: 2026-02-11T00:05:30.177Z
  started_at: 2026-02-11T04:46:59.375Z
  notes:
    - _ulid: 01KH508VE1T9JEBJV2R50E10ZD
      created_at: 2026-02-11T00:05:30.177Z
      author: "@claude"
      content: |
        Implementation notes:

        Use ComfyUI's own pattern from tests-unit/ and ComfyUI_Selectors:
        sys.modules patching in conftest.py before node imports. MockModelPatcher
        should use small tensors (4x4 float32) for speed. Recipe fixtures build
        on lib/recipe.py dataclasses. Arch fixtures provide representative state
        dict key sets for detection testing.
        Files: tests/conftest.py, tests/mocks/__init__.py, tests/mocks/mock_comfy.py
    - _ulid: 01KH5GFVRKF1TEASV99QCVHN05
      created_at: 2026-02-11T04:48:57.107Z
      author: "@claude"
      content: "Completed ComfyUI mocking and fixtures implementation. AC-1: MockModelPatcher fixture
        already fully implemented with model_state_dict, clone, add_patches, get_key_patches,
        patches_uuid. AC-2: Added recipe_full fixture for compose+chain pattern, single-LoRA,
        multi-LoRA, compose (2 branches), chain (2 sequential merges) already present. AC-3: ComfyUI
        sys.modules mocking already implemented with autouse fixture. AC-4: Added _ZIMAGE_KEYS
        constant with layers/noise_refiner/context_refiner keys, sdxl_state_dict_keys fixture,
        zimage_state_dict_keys fixture, and mock_model_patcher_zimage fixture. Created
        test_conftest_fixtures.py with 22 tests covering all 4 ACs. 467 total tests pass, ruff
        clean."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH508VEVHA4ZME2CDZ275XKS
  slugs:
    - implement-node-graph-testing
  title: Implement Node Graph Testing
  type: task
  spec_ref: "@node-graph-testing"
  plan_ref: "@plan-testing-strategy"
  status: pending
  blocked_by: []
  depends_on:
    - "@implement-comfyui-mocking-and-fixtures"
    - "@implement-entry-node"
    - "@implement-lora-node"
    - "@implement-compose-node"
    - "@implement-merge-node"
  context: []
  priority: 3
  tags: []
  vcs_refs: []
  created_at: 2026-02-11T00:05:30.203Z
  notes:
    - _ulid: 01KH508VEVQ633FW8E9Y3NX9TP
      created_at: 2026-02-11T00:05:30.203Z
      author: "@claude"
      content: |
        Implementation notes:

        Create tests/test_graph.py with helper functions that instantiate node
        classes and call their FUNCTION methods directly to build recipe trees.
        The mock executor is a lightweight tree walker (separate from the real
        executor) that returns an operation plan (list of {op: filter_delta|merge_weights,
        keys: ...}) without touching GPU. This validates the Exit node's recipe
        analysis logic independently.
        Files: tests/test_graph.py, tests/helpers/graph_builder.py
  todos: []
  automation: eligible
- _ulid: 01KH508VFBCS29YV4HX4J1QTST
  slugs:
    - implement-ci-pipeline
  title: Implement CI Pipeline
  type: task
  spec_ref: "@ci-pipeline"
  plan_ref: "@plan-testing-strategy"
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #2. Added GitHub Actions CI workflow with lint (ruff via
    astral-sh/ruff-action) and test (pytest with CPU-only PyTorch via uv) jobs. Triggers on push to
    main and all PRs. All ACs verified: ac-1 (pytest with CPU torch), ac-2 (ruff check), ac-3 (green
    checks on both jobs)."
  depends_on:
    - "@implement-testing-infrastructure"
  context: []
  priority: 3
  tags: []
  vcs_refs: []
  created_at: 2026-02-11T00:05:30.218Z
  started_at: 2026-02-11T02:20:48.141Z
  completed_at: 2026-02-11T02:27:43.054Z
  notes:
    - _ulid: 01KH508VFBT2H54M5F3RHDF08Z
      created_at: 2026-02-11T00:05:30.219Z
      author: "@claude"
      content: |
        Implementation notes:

        Follow ComfyUI's test-unit.yml pattern: install CPU-only torch via
        --index-url https://download.pytorch.org/whl/cpu, install project deps,
        run pytest. Add ruff for linting. Single ubuntu-latest runner to start
        (extend to matrix later). Add pyproject.toml [tool.ruff] config.
        Files: .github/workflows/test.yml, pyproject.toml (ruff + pytest config)
    - _ulid: 01KH58198P2XYEND6VHT87WAE3
      created_at: 2026-02-11T02:21:10.806Z
      author: "@claude"
      content: "Added .github/workflows/ci.yml with two jobs: lint (ruff via astral-sh/ruff-action) and
        test (pytest with CPU torch via uv). Triggers on push to main and all PRs."
      supersedes: null
  todos: []
  automation: eligible
- _ulid: 01KH5KGMYRRPD5W18RMR3WBHFB
  slugs:
    - fix-lora-set-scoping
  title: Fix LoRA set scoping bug in executor and loader API
  type: bug
  description: >-
    CORRECTNESS BUG: LoRA set IDs are looked up in evaluate_recipe (lib/executor.py:717) but never
    passed to the loader API. loader.get_delta_specs(keys, key_indices) has no set_id parameter, and
    analyze_recipe loads all LoRA sets cumulatively into one loader (lib/analysis.py:190-222). Each
    RecipeLoRA evaluation can apply deltas from ALL loaded sets affecting a key, not just that LoRA
    node's set. Compose branches with overlapping keys produce incorrect merged output.


    SCOPE:

    - lib/lora/base.py: Add set_id parameter to get_delta_specs() interface

    - lib/lora/sdxl.py: Segment _lora_data/_qkv_data by set, or maintain per-set loaders

    - lib/lora/zimage.py: Same segmentation

    - lib/analysis.py: Wire set scoping through analysis pipeline

    - lib/executor.py: Pass set_id through _apply_lora_set to loader calls

    - Also fix silent fallback: when set_id is None in _apply_lora_set (executor.py:717-720), raise
    RuntimeError instead of silently returning current weights


    VERIFICATION:

    - Test: two LoRA sets affecting same key produce distinct branch results

    - Test: missing set_id raises explicit error, not silent no-op

    - All 467 existing tests must pass
  spec_ref: null
  meta_ref: null
  plan_ref: null
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #24. Segmented LoRA loader storage by set_id, wired set scoping through
    analysis and executor, replaced silent fallback with RuntimeError. 13 new tests, all passing."
  depends_on: []
  context: []
  priority: 1
  tags:
    - correctness
    - review-finding
  vcs_refs: []
  created_at: 2026-02-11T05:41:48.628Z
  started_at: 2026-02-11T06:19:26.024Z
  completed_at: 2026-02-11T06:19:28.733Z
  notes: []
  todos: []
- _ulid: 01KH5KGYTSTC0BK7XM3GP5N2K7
  slugs:
    - fix-batched-parity
  title: Fix filter_delta_batched scalar parity bug for mixed flat/non-flat batches
  type: bug
  description: >-
    CORRECTNESS BUG: filter_delta_batched violates scalar parity for mixed flat/non-flat batches.
    Early-exit uses (var < eps).all() globally across entire batch, not per-sample. If one batch
    item is non-flat, flat items still go through ranking/masking and get attenuated, while scalar
    filter_delta would passthrough unchanged. Violates AC-3 (batched matches per-key behavior).


    REPRO: Batch item 0 = constant nonzero 1D delta (flat variance), Batch item 1 = non-flat delta.
    Batched output for item 0 is attenuated while scalar returns unchanged.


    FIX: Compute per-sample flat mask and blend passthrough vs filtered outputs per sample.
    Location: lib/widen.py:503-533.


    ALSO FIX WHILE HERE:

    - merge_weights vs merge_weights_batched inconsistency: empty weights_list raises IndexError in
    scalar but returns backbone fallback in batched

    - Unknown sparsity_method silently falls back to softmax instead of raising ValueError

    - Both paths should validate inputs identically


    VERIFICATION:

    - Test: mixed flat/non-flat batch produces identical results to per-item scalar calls

    - Test: empty weights_list behavior consistent (both raise or both handle gracefully)

    - Test: invalid sparsity_method raises ValueError
  spec_ref: null
  meta_ref: null
  plan_ref: null
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #23. Fixed filter_delta_batched per-sample flat mask, empty
    weights_list validation, sparsity_method validation. 10 new tests, 477 total passing."
  depends_on: []
  context: []
  priority: 1
  tags:
    - correctness
    - review-finding
  vcs_refs: []
  created_at: 2026-02-11T05:41:58.745Z
  started_at: 2026-02-11T06:16:19.722Z
  completed_at: 2026-02-11T06:16:28.992Z
  notes: []
  todos: []
- _ulid: 01KH5KHTVK03VYJGAPV28SYJB0
  slugs:
    - fix-test-quality
  title: "Fix test quality: placeholder tests, mock fidelity, and weak assertions"
  type: task
  description: >-
    TEST QUALITY issues creating false confidence in AC coverage.


    1. PLACEHOLDER PASS TESTS (3 tests verify nothing):

    - tests/test_exit_node.py:240 - TestComposeCallsMergeWeights (AC: @exit-node ac-3)

    - tests/test_exit_node.py:261 - TestLoRACallsFilterDelta (AC: @exit-node ac-4)

    - tests/test_exit_node.py:282 - TestChainedMergeOrder (AC: @exit-node ac-5)

    Replace with real implementations.


    2. MOCK FIDELITY BUG:

    MockModelPatcher.clone() sets new UUID (tests/conftest.py:90-97), but real ComfyUI copies
    patches_uuid from source. Tests enforce wrong behavior (test_mock_model_patcher.py:56-59). Fix
    mock to match real ComfyUI.


    3. WEAK ASSERTIONS:

    - test_executor.py:912-943 chained merge order: only asserts call count, not actual order

    - test_executor.py:1054-1085 backbone override: only checks backbone is not None

    - test_exit_recipe_analysis.py:471-493 cleanup: only asserts exception, not cleanup side-effects

    - test_lora_block_strength.py:309-325 backwards-compat: only checks block_config is None


    4. TEST ANTI-PATTERNS:

    - test_lora_loaders.py:249-267 substring check for module independence (fragile)

    - Duplicate _DIFFUSION_PREFIX in nodes/exit.py:23 and tests/conftest.py:37


    VERIFICATION: No pass-only test bodies, mock matches real ComfyUI, all assertions verify
    behavioral outcomes
  spec_ref: null
  meta_ref: null
  plan_ref: null
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #26. Replaced 3 placeholder pass tests, fixed MockModelPatcher.clone()
    fidelity, strengthened assertions, deduplicated _DIFFUSION_PREFIX."
  depends_on: []
  context: []
  priority: 2
  tags:
    - testing
    - review-finding
  vcs_refs: []
  created_at: 2026-02-11T05:42:27.443Z
  started_at: 2026-02-11T07:29:44.549Z
  completed_at: 2026-02-11T07:29:47.246Z
  notes: []
  todos: []
- _ulid: 01KH5KJ21MWX1JS1QCN873N9KZ
  slugs:
    - harden-validation
  title: Harden validation, error handling, and API contracts across codebase
  type: task
  description: >-
    Multiple small validation/error handling issues that collectively create a fragile system.


    NODES LAYER:

    1. nodes/lora.py:46 - Silently accepts invalid prev types, starts new set. Raise TypeError.

    2. nodes/lora.py:35, nodes/merge.py:34 - Accept BlockConfig without arch validation. Add check.

    3. __init__.py:3-14 - Broad ImportError catch hides real failures. Narrow to expected modules.


    LIBRARY LAYER:

    4. lib/analysis.py:102-122 - _collect_lora_sets no else guard for unknown node types. Add
    ValueError.

    5. lib/analysis.py:202-214 - Redundant FileNotFoundError catch/re-raise. Remove.

    6. lib/recipe.py:44 - RecipeLoRA.loras has mutable dicts in frozen dataclass. Use frozen
    entries.

    7. lib/numerical_config.py:154-175 - safe_norm shape inconsistency with keepdim=False. Fix.

    8. lib/lora/zimage.py:48-100 - Compound-name normalization order risk. Sort longest-first.

    9. lib/lora/sdxl.py:225, lib/lora/zimage.py:292 - affected_keys returns internal set. Return
    copy.


    SPARSITY:

    10. lib/sparsity.py:135-209 - EntmaxFunction backward incorrect. If inference-only, add no_grad
    guard. If gradients needed, implement correct VJP.


    VERIFICATION: Invalid prev raises TypeError, mismatched arch raises at boundary, unknown nodes
    raise ValueError, frozen recipes can't be mutated externally, all tests pass
  spec_ref: null
  meta_ref: null
  plan_ref: null
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #28. 10 validation/hardening fixes across nodes, lib, and sparsity. 491
    tests passing."
  depends_on: []
  context: []
  priority: 3
  tags:
    - hardening
    - review-finding
  vcs_refs: []
  created_at: 2026-02-11T05:42:34.804Z
  started_at: 2026-02-11T07:41:30.424Z
  completed_at: 2026-02-11T07:41:42.683Z
  notes: []
  todos: []
- _ulid: 01KH5KNGFJNFN8ZFD5FTA6NZG8
  slugs:
    - split-executor
  title: Split executor.py into focused modules
  type: task
  description: "STRUCTURAL: lib/executor.py is 883 lines mixing 4 distinct concerns. evaluate_recipe
    alone is ~230 lines with nested closures. Codex confirmed coupling hides correctness bugs. SPLIT
    INTO: 1) lib/batch_groups.py (OpSignature, compile_batch_groups), 2) lib/gpu_ops.py (DeltaSpec,
    apply_lora_batch_gpu, chunked_evaluation, compute_batch_size, chunked), 3) lib/per_block.py
    (per-block control functions), 4) lib/recipe_eval.py (evaluate_recipe with EvalContext dataclass
    and helpers), 5) lib/executor.py as facade/re-export. MIGRATION ORDER: data types first, pure
    helpers, gpu ops, per-block, recipe_eval last. ADDITIONAL FIXES: deduplicate
    apply_lora_batch_gpu (4 paths to 2), fix QKV to use DeltaSpec.offset, fix type contract for
    _eval_node, guard torch.cuda.empty_cache, fix backbone override. DEPENDS ON fix-lora-set-scoping
    completing first."
  spec_ref: null
  meta_ref: null
  plan_ref: null
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #25. Split executor.py into batch_groups.py, gpu_ops.py, per_block.py,
    recipe_eval.py with facade. Deduplicated matmul (4 paths to 2 via _compute_deltas), fixed QKV
    offset to use DeltaSpec.offset, decomposed evaluate_recipe from 230-line closure into 6
    module-level functions with EvalContext, guarded all torch.cuda.empty_cache() calls. 490 tests
    passing, ruff clean, CI green."
  depends_on: []
  context: []
  priority: 2
  tags:
    - refactor
    - review-finding
  vcs_refs: []
  created_at: 2026-02-11T05:44:27.890Z
  started_at: 2026-02-11T06:32:44.737Z
  completed_at: 2026-02-11T06:32:54.264Z
  notes: []
  todos: []
- _ulid: 01KH5KNKD2PFVCBKPN04G1PCXD
  slugs:
    - unify-scalar-batched
  title: Unify scalar/batched codepaths in widen.py and divergence.py
  type: task
  description: "STRUCTURAL: ~200 lines of duplicated scalar/batched code in widen.py plus dim dispatch
    duplication in both widen.py and divergence.py. Already caused the flat/non-flat parity bug.
    PAIRS TO UNIFY: filter_delta/batched, merge_weights/batched, _merge_1d_params/batched,
    _build_importance_masks/batched, WeightDisentangler 8 methods. APPROACH: Make batched canonical,
    scalar becomes unsqueeze(0)->batched->squeeze(0). DIMENSION DISPATCH: reshape-to-canonical,
    flatten non-output axes. DIVERGENCE.PY: same treatment, prefer reshape() over view(). ALSO FIX:
    remove unused WIDENConfig.n_models, add stable=True to exact_rank, normalize soft_rank to [0,1].
    DEPENDS ON fix-batched-parity completing first."
  spec_ref: null
  meta_ref: null
  plan_ref: null
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #27. Unified scalar/batched codepaths (-180 lines), generic dimension
    dispatch, removed n_models, fixed exact_rank stable sort, normalized soft_rank."
  depends_on: []
  context: []
  priority: 2
  tags:
    - refactor
    - review-finding
  vcs_refs: []
  created_at: 2026-02-11T05:44:30.882Z
  started_at: 2026-02-11T07:29:50.494Z
  completed_at: 2026-02-11T07:29:53.129Z
  notes: []
  todos: []
- _ulid: 01KH5XMHBMKSNB3YJ64XSM5H8R
  slugs: []
  title: "Per-block performance: cache classify_key and use indexed assignment"
  type: task
  description: "Bundle two perf items: (1) Cache classify_key results per (arch, key) within a batch
    to avoid repeated classification loops in per_block.py. (2) Replace per-index writeback loops in
    per-block merge/filter with indexed assignment (result[indices] = sub_result)."
  spec_ref: null
  status: pending
  blocked_by: []
  depends_on: []
  context: []
  priority: 4
  tags:
    - performance
    - review-finding
  vcs_refs: []
  created_at: 2026-02-11T08:38:41.780Z
  notes: []
  todos: []
- _ulid: 01KH5XMW1P2P2PKWR4NFHT7W1H
  slugs: []
  title: "DRY block config nodes: extract shared slider schema and packing helper"
  type: task
  description: block_config_sdxl.py and block_config_zimage.py repeat slider schema and packing logic.
    As new architectures are added (Flux, Qwen, etc.), this duplication multiplies. Extract shared
    helper/factory now.
  spec_ref: null
  status: pending
  blocked_by: []
  depends_on: []
  context: []
  priority: 4
  tags:
    - refactor
    - review-finding
  vcs_refs: []
  created_at: 2026-02-11T08:38:52.726Z
  notes: []
  todos: []
- _ulid: 01KH5XMY99SGNHVS97NFG78M6G
  slugs: []
  title: Add CI guard for empty/placeholder test bodies
  type: infra
  description: Add a lint check or CI step that fails if a test_ method body is only pass or if
    AC-tagged tests contain no behavioral assertion. Prevents future regressions.
  spec_ref: null
  status: pending
  blocked_by: []
  depends_on: []
  context: []
  priority: 4
  tags:
    - testing
    - review-finding
  vcs_refs: []
  created_at: 2026-02-11T08:38:55.017Z
  notes: []
  todos: []
- _ulid: 01KH5XN0ZP4N73QZ2YPVX0RJ2D
  slugs: []
  title: Harden block_classify.py key matching with anchored patterns
  type: task
  description: Refiner checks in block_classify.py (lines 122/124) use substring containment which can
    accidentally match unrelated keys. Prefer anchored/structured patterns.
  spec_ref: null
  status: pending
  blocked_by: []
  depends_on: []
  context: []
  priority: 4
  tags:
    - hardening
    - review-finding
  vcs_refs: []
  created_at: 2026-02-11T08:38:57.782Z
  notes: []
  todos: []
- _ulid: 01KH5XN34KRHPV070J3H177FTC
  slugs: []
  title: Add strict mode for batched catch-all fallbacks in widen.py
  type: task
  description: Non-OOM catch-all fallbacks in widen.py (around lines 556, 640) hide shape bugs and API
    misuse. Add optional strict mode that raises instead of falling back. Defer until integration
    testing surfaces real issues.
  spec_ref: null
  status: pending
  blocked_by: []
  depends_on: []
  context: []
  priority: 5
  tags:
    - hardening
    - review-finding
  vcs_refs: []
  created_at: 2026-02-11T08:38:59.987Z
  notes: []
  todos: []
- _ulid: 01KH5XN590K4EMDTCP1FJWFHM0
  slugs: []
  title: Fix _compute_deltas to preserve spec-to-delta mapping
  type: task
  description: gpu_ops.py _compute_deltas strips spec identity. Metadata resolved via next(s for s in
    group if s.key_index == key_index) is O(n^2) and couples to key_index uniqueness. Return (spec,
    delta) pairs instead.
  spec_ref: null
  status: pending
  blocked_by: []
  depends_on: []
  context: []
  priority: 4
  tags:
    - performance
    - review-finding
  vcs_refs: []
  created_at: 2026-02-11T08:39:02.176Z
  notes: []
  todos: []
- _ulid: 01KH60AR9NMW9K4DYQB51TTYQV
  slugs:
    - fix-lora-path-resolution
  title: Fix LoRA file path resolution for nested paths
  type: bug
  description: When ComfyUI passes a LoRA name like 'z-image/Mystic-XXX-ZIT-V5.safetensors' (nested
    subdirectory), analyze_recipe fails with FileNotFoundError. The lora_base_path resolution in
    lib/analysis.py likely doesn't handle ComfyUI's folder_paths.get_full_path() correctly for
    nested LoRA directories. Need to use ComfyUI's folder_paths to resolve the actual file location
    rather than simple path joining.
  spec_ref: null
  meta_ref: null
  plan_ref: null
  status: completed
  blocked_by: []
  closed_reason: "Merged in PR #30. Fixed 3 bugs: (1) lora_path_resolver callable replaces single-dir
    lora_base_path for multi-directory search, (2) fail-fast imports in __init__.py, (3) key prefix
    mismatch between loaders and exit node. Also hardened resolver-returns-None to fail immediately.
    Runtime corruption tracked separately."
  depends_on: []
  context: []
  priority: 2
  tags:
    - correctness
    - runtime
  vcs_refs: []
  created_at: 2026-02-11T09:25:46.933Z
  started_at: 2026-02-11T17:17:00.323Z
  completed_at: 2026-02-11T18:02:42.369Z
  notes:
    - _ulid: 01KH6VHP8AAVKHPDJ5A3BCSQM3
      created_at: 2026-02-11T17:21:25.771Z
      author: "@claude"
      content: "PR #30 created. Replaced lora_base_path with lora_path_resolver callable. Exit node now
        wraps folder_paths.get_full_path to search all registered LoRA directories. 2 new regression
        tests, 493 total passing."
      supersedes: null
    - _ulid: 01KH6X93F2H2Y1HTFGNZ8D4EC0
      created_at: 2026-02-11T17:51:41.538Z
      author: "@claude"
      content: "Found and fixed 3 bugs: (1) lora_path_resolver replaces single-dir lora_base_path, (2)
        __init__.py fail-fast instead of silent except ImportError, (3) key prefix mismatch - exit
        node used model.diffusion_model.state_dict() (unprefixed) but loaders produce prefixed keys,
        so intersection was always empty. Runtime testing revealed the merge pipeline now executes
        but produces corrupted output - filed inbox item 01KH6X7H for follow-up."
      supersedes: null
  todos: []
- _ulid: 01KH70GN0FT5GQ30E36REMRST3
  slugs: []
  title: Fix corrupted model output from WIDEN Exit node
  type: bug
  description: "WIDEN Exit node produces corrupted model output when executing the GPU merge pipeline.
    Nodes now execute (key prefix mismatch fixed) but the merged weights are wrong. Need to debug:
    evaluate_recipe data flow, WIDEN filter/merge math with real z-image tensors,
    install_merged_patches correctness, and possible dtype/device mismatches. Test with actual
    ComfyUI runtime."
  spec_ref: null
  status: completed
  blocked_by: []
  closed_reason: null
  depends_on: []
  context: []
  priority: 3
  tags:
    - correctness
    - runtime
  vcs_refs: []
  created_at: 2026-02-11T18:48:14.607Z
  started_at: 2026-02-11T18:48:22.108Z
  completed_at: 2026-02-11T19:08:40.042Z
  notes:
    - _ulid: 01KH70H3S74RKEMHR14H61DSTH
      created_at: 2026-02-11T18:48:29.736Z
      author: "@claude"
      content: "Bug 1 (b779ed5): Set patch tuple format was wrong — ('set', tensor) instead of ('set',
        (tensor,)). ComfyUI calculate_weight does weight.copy_(v[0]), so with raw tensor v[0]
        indexed the first row instead of unpacking a 1-tuple. Every output neuron got identical
        values = full corruption. Fixed in nodes/exit.py:123, updated 5 test files."
      supersedes: null
    - _ulid: 01KH70H6M68QE1AHER6PWQM3EZ
      created_at: 2026-02-11T18:48:32.646Z
      author: "@claude"
      content: "Bug 2 (e1bcdd1): Both SDXL and Z-Image loaders hardcoded alpha=rank, ignoring the .alpha
        tensor stored in LoRA safetensors files. Correct formula is scale=strength*alpha/rank. When
        alpha!=rank (common), delta magnitudes were wrong causing output divergence from
        merge-router reference. Fixed both loaders to read .alpha keys from file, defaulting to rank
        when absent."
      supersedes: null
    - _ulid: 01KH70HCEQ2VGABQ743GQ59WR0
      created_at: 2026-02-11T18:48:38.615Z
      author: "@claude"
      content: "Verified: output now matches merge-router reference. Minor precision differences remain
        (expected — we compute in float32 end-to-end vs reference's bf16 between chain steps). All
        493 tests pass, ruff clean."
      supersedes: null
  todos: []
- _ulid: 01KH70GQ9YAPG64C3JJVAA1WZ0
  slugs: []
  title: Optimize batched GPU pipeline performance
  type: task
  description: "Performance: batched GPU pipeline is ~2-3x slower than merge-router reference. Key
    bottlenecks: (1) Excessive gc.collect()+empty_cache() after every chunk AND every OpSig group —
    reference only cleans between stages (~60 vs ~30 cleanups). Files: gpu_ops.py:292-294,
    exit.py:400-404. (2) No block-level pre-grouping — we compile OpSig groups across ALL keys
    creating ~200+ tiny groups vs reference's ~50 via block pre-grouping. Files: exit.py:343-348,
    batch_groups.py:33-66. (3) Linear O(N) search in apply_lora_batch_gpu to find matching spec
    after delta computation. Files: gpu_ops.py:205,224. (4) No pin_memory for async CPU→GPU
    transfers. Priority order: remove per-chunk cleanup (30-40% win), pre-group by block (4x fewer
    groups), pre-build index dict, add pin_memory."
  spec_ref: null
  status: pending
  blocked_by: []
  depends_on: []
  context: []
  priority: 3
  tags:
    - performance
    - gpu
  vcs_refs: []
  created_at: 2026-02-11T18:48:16.958Z
  notes:
    - _ulid: 01KH70K6RM8SX3M6YT3Y8ECDPG
      created_at: 2026-02-11T18:49:38.324Z
      author: "@claude"
      content: Batched GPU pipeline is ~2-3x slower than merge-router reference. Root cause analysis from
        side-by-side comparison of lib/gpu_ops.py, lib/batch_groups.py, nodes/exit.py vs
        merge-router/scripts/lora_chain_merge.py.
      supersedes: null
    - _ulid: 01KH70KA4Y712WCV83YB4PFKF5
      created_at: 2026-02-11T18:49:41.790Z
      author: "@claude"
      content: "Bottleneck 1 (highest impact ~30-40%): Excessive gc.collect()+empty_cache() — called after
        every chunk (gpu_ops.py:292-294) AND every OpSig group (exit.py:400-404). Each call triggers
        GPU sync blocking kernel queuing. Reference only cleans between stages. Fix: remove
        per-chunk cleanup, keep only per-group."
      supersedes: null
    - _ulid: 01KH70KHGJSHM4CMHCAEPMM4J0
      created_at: 2026-02-11T18:49:49.330Z
      author: "@claude"
      content: "Bottleneck 2: No block-level pre-grouping — we compile OpSig groups across ALL keys at
        once (exit.py:343-348, batch_groups.py:33-66), creating ~200+ tiny groups. Reference
        pre-groups by model block first then by OpSig within each block, producing ~50 groups with
        better batch utilization. Fix: add _group_keys_by_block() pre-pass."
      supersedes: null
    - _ulid: 01KH70KM49RKYVNX0ANBKH07T2
      created_at: 2026-02-11T18:49:52.009Z
      author: "@claude"
      content: "Bottleneck 3: Linear O(N) search per key in apply_lora_batch_gpu (gpu_ops.py:205,224) —
        'next(s for s in group if s.key_index == key_index)' to find matching spec after delta
        computation. Fix: pre-build index dict from key_index → spec."
      supersedes: null
    - _ulid: 01KH70KP5RA3WHRCH442B03NTQ
      created_at: 2026-02-11T18:49:54.104Z
      author: "@claude"
      content: "Bottleneck 4 (minor): No pin_memory for CPU→GPU transfers. Reference uses pinned memory
        for async transfers on large batches. Fix: add pin_memory option in gpu_ops.py:273-274."
      supersedes: null
  todos: []
- _ulid: 01KH70WT78G1NNBJCNC9PH7JDC
  slugs: []
  title: Add unit tests for LoRA alpha reading from safetensors
  type: task
  spec_ref: null
  meta_ref: null
  plan_ref: null
  status: pending
  blocked_by: []
  depends_on: []
  context: []
  priority: 2
  tags:
    - testing
    - correctness
  vcs_refs: []
  created_at: 2026-02-11T18:54:53.160Z
  notes:
    - _ulid: 01KH70X1M1S20E8QECK2ARVN6N
      created_at: 2026-02-11T18:55:00.737Z
      author: "@claude"
      content: "Both SDXL and Z-Image loaders now read .alpha keys from safetensors files (commits
        e1bcdd1). Existing test fixtures (sdxl_lora_with_alpha etc.) don't include .alpha tensor
        keys, so only the default alpha=rank path is covered. Need tests with safetensors files
        containing .alpha scalars where alpha \\!= rank to verify correct scale computation (scale =
        strength * alpha / rank). Files: lib/lora/sdxl.py, lib/lora/zimage.py,
        tests/test_sdxl_loader.py, tests/test_zimage_loader.py."
      supersedes: null
  todos: []
- _ulid: 01KH731J1SG2XVAKCSJNK0QTPA
  slugs: []
  title: Fix device mismatch in chunked_evaluation torch.stack
  type: bug
  spec_ref: null
  meta_ref: null
  plan_ref: null
  status: in_progress
  blocked_by: []
  depends_on: []
  context: []
  priority: 1
  tags:
    - runtime
    - correctness
  vcs_refs: []
  created_at: 2026-02-11T19:32:25.782Z
  notes:
    - _ulid: 01KH731V18HTSHWJZQ7GT3KYVC
      created_at: 2026-02-11T19:32:34.984Z
      author: "@claude"
      content: "RuntimeError at gpu_ops.py:273 in chunked_evaluation: torch.stack fails because
        base_tensors dict contains a mix of CUDA and CPU tensors. The stack call is:
        torch.stack([base_tensors[k] for k in chunk_keys]). base_tensors comes from exit.py:386 as
        {k: base_state[k] for k in group_keys} where base_state = model_patcher.model_state_dict().
        Some keys in model_state_dict() are returning GPU-resident tensors (likely from prior set
        patches or model loading) while others are CPU. Fix: ensure all tensors are moved to CPU
        before stacking, e.g. base_tensors[k].cpu(), or move uniformly to device before stack."
      supersedes: null
  todos: []
  started_at: 2026-02-11T19:33:55.036Z
